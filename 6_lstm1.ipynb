{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6 - PROBLEM 1\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def how_long(f, *args):\n",
    "    #medir el tiempo que tarda f\n",
    "    t1 = time.time()\n",
    "    res = f(*args)\n",
    "    t2 = time.time()\n",
    "    print (\"tiempo utilizado = \",t2-t1)\n",
    "    #return res, t2-t1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000 #Los mil primeros chars son de validación\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "11\n",
      "(64, 27)\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "\n",
      "valid\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64 #trocitos de frases en un lote\n",
    "num_unrollings=10 #caracteres-1 en cada trocito (hay uno más de solapamiento)\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size #reparte el texto en N caminos de predicion (N=batch_size)\n",
    "                                            #y cada una la va encadenando por separado\n",
    "        \n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)] #lleva un array de N cursores (N=batch_size)\n",
    "                                                                        #para cada camino de prediccion\n",
    "    self._last_batch = self._next_batch() #fija el último lote al primero de todos\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0 #1-hot-encoded array\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size #incrementa el puntero en modo circular por el texto\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch] #cada trocito de frase del lote se solapa en el último caracter \n",
    "                                    #del trocito de frase de la misma posicion en el lote anterior\n",
    "    for step in range(self._num_unrollings): #va añadiendo chars a cada trocito de texto del lote hasta llegar a num_unrollings+1\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)] #Toma el mayor valor (o valores si hay varios iguales)\n",
    "                                                            #del vector de probabilidades y devuelve la lista de caracteres\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0] #reserva una cadena vacía para N caminos de predicción (N=batch_size)\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))] #va pegando la cadena predicha en cada batch, o bien el hot-encoded literal\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(\"train\")\n",
    "t1=train_batches.next()\n",
    "print(batches2string(t1))\n",
    "print(len(t1))\n",
    "print(t1[0].shape)\n",
    "print(batches2string(train_batches.next()))\n",
    "print(\"\\nvalid\")\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "#print(batches2string(valid_batches.next()))\n",
    "#print(batches2string(valid_batches.next()))\n",
    "#print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimize Average Negative Log-Probability:\n",
    "$$\\text{loss} = -\\frac{1}{N}\\sum_{i=1}^{N} \\ln p_{\\text{target}_i}$$\n",
    "\n",
    "Measure perplexity:\n",
    "\n",
    "$$e^{-\\frac{1}{N}\\sum_{i=1}^{N} \\ln p_{\\text{target}_i}} = e^{\\text{loss}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized probabilities.\"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[documento] (http://arxiv.org/pdf/1402.1128v1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTAS: \n",
    "<img src=\"LSTM_notas.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# DECLARACION LSTM1\n",
    "##################\n",
    "\n",
    "num_nodes = 64 #Tantas células en la memoria como líneas de entrenamiento va a seguir en un lote, que casualidad\n",
    "starter_learning_rate = 10.0\n",
    "learning_decay_steps = 5000\n",
    "learning_decay_rate = 0.1\n",
    "clip_limit = 1.25\n",
    "\n",
    "graphLSTM1 = tf.Graph()\n",
    "with graphLSTM1.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  \n",
    "  # Gates: 0=memory cell, 1=input, 2=forget, 3=output\n",
    "  num_gates = 4  \n",
    "\n",
    "  #Gates parameters for input:\n",
    "  #ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  #cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  #fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  #ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  gx = tf.Variable(tf.truncated_normal([num_gates, vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "\n",
    "  #Gates parameters for output:\n",
    "  #im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  #cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  #fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  #om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  gm = tf.Variable(tf.truncated_normal([num_gates, num_nodes, num_nodes], -0.1, 0.1))\n",
    "    \n",
    "  #Gates parameters for bias:\n",
    "  #ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  #cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  #fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  #ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  gb = tf.Variable(tf.zeros([num_gates, 1, num_nodes]))\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_omem = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, last_state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the previous state and the gates.\"\"\"\n",
    "    \n",
    "    I = tf.stack([i,i,i,i])\n",
    "    O = tf.stack([o,o,o,o])\n",
    "    gates = tf.matmul(I, gx) + tf.matmul(O, gm) + gb\n",
    "    \n",
    "    #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    \n",
    "    update = gates[0,:,:]\n",
    "    input_gate = tf.sigmoid(gates[1,:,:])\n",
    "    forget_gate = tf.sigmoid(gates[2,:,:])\n",
    "    output_gate = tf.sigmoid(gates[3,:,:])\n",
    "    \n",
    "    next_state = forget_gate * last_state + input_gate * tf.tanh(update)\n",
    "    outputmem = output_gate * tf.tanh(next_state)\n",
    "    \n",
    "    return outputmem, next_state\n",
    "\n",
    "  # Input data, get input and labels.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  omemories = list()\n",
    "  omem = saved_omem\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    omem, state = lstm_cell(i, omem, state)\n",
    "    omemories.append(omem)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_omem.assign(omem),saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat_v2(omemories, 0), w, b)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat_v2(train_labels, 0)))\n",
    "                #Evalúa la salida de cada ráfaga con un logistic classifier normal\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, learning_decay_steps, learning_decay_rate, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, clip_limit) #Limitar los pesos para que no se disparen y quitar el exploding\n",
    "  optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch size = 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_omem = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(saved_sample_omem.assign(tf.zeros([1, num_nodes])),saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_omem, sample_state = lstm_cell(sample_input, saved_sample_omem, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_omem.assign(sample_omem),saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_omem, w, b))\n",
    "    \n",
    "    #Para verificar, toma nuestro texto de validación y ve si se predice bien aplicando X'*W+b = Y, \n",
    "    #donde X' es la salida de la MEMORIA después de meterle X de entrada y el estado anterior guardado S (HISTORIA)\n",
    "    #El texto de validación se alimenta letra a letra, no por ráfagas como el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "step\t0\t3s:\tAvgLoss 3.295826\tLRate 10.000000\tMBperplex 27.00\tVSperplex 20.25\n",
      "================================================================================\n",
      "gpgcujlwpfsbnh roa iuoshyvj om ih keylzwmzcpst  rbp  a r vhtjzs tuc kd lheetczxv\n",
      "vdg fllnifpmaaii sm taihrrmxeh bgl mea nei to ognr xcnetp j nilnimntoypmsdqnwd  \n",
      "jq e  fi er  neqcgsqpefxahyeqtipi xnf dem nqkoynyztx tqudyrfjcsr raht t ftvuzifl\n",
      "xkadu rejyjlxovfuftsieel ue phace uoriee ze joo uo ieltmcnhandko immeenhqjlpm  z\n",
      "rsimmseuo iocksgii fatxdej uixr ep z  cqcldhh kxwcaesvwt dbtd c dct uledrjprotkn\n",
      "================================================================================\n",
      "step\t100\t7s:\tAvgLoss 2.594235\tLRate 10.000000\tMBperplex 10.97\tVSperplex 10.33\n",
      "step\t200\t11s:\tAvgLoss 2.246921\tLRate 10.000000\tMBperplex 8.57\tVSperplex 8.43\n",
      "step\t300\t16s:\tAvgLoss 2.096534\tLRate 10.000000\tMBperplex 7.48\tVSperplex 8.25\n",
      "step\t400\t20s:\tAvgLoss 1.998788\tLRate 10.000000\tMBperplex 7.55\tVSperplex 7.72\n",
      "step\t500\t24s:\tAvgLoss 1.931895\tLRate 10.000000\tMBperplex 6.49\tVSperplex 7.10\n",
      "step\t600\t28s:\tAvgLoss 1.900794\tLRate 10.000000\tMBperplex 6.18\tVSperplex 6.88\n",
      "step\t700\t33s:\tAvgLoss 1.850499\tLRate 10.000000\tMBperplex 6.40\tVSperplex 6.58\n",
      "step\t800\t37s:\tAvgLoss 1.808041\tLRate 10.000000\tMBperplex 5.85\tVSperplex 6.26\n",
      "step\t900\t41s:\tAvgLoss 1.815228\tLRate 10.000000\tMBperplex 6.67\tVSperplex 6.31\n",
      "step\t1000\t45s:\tAvgLoss 1.812455\tLRate 10.000000\tMBperplex 5.53\tVSperplex 6.07\n",
      "================================================================================\n",
      "gre and as retwolis holl oppeneration wode citvermed knise or a resurtives the e\n",
      "quent use ussurugr veation as fratrabtses recesered of claroms with have eight z\n",
      "gerted in frocce of beyafy will a mode colligy littres of gene chempere the rubf\n",
      "x nate sidd oon the wime a refemue veace a corst three zero sik of ald sough a h\n",
      "z t anctencogre the recome nowel bet sophing the weal tram un weft bock of ginda\n",
      "================================================================================\n",
      "step\t1100\t50s:\tAvgLoss 1.761432\tLRate 10.000000\tMBperplex 5.50\tVSperplex 5.88\n",
      "step\t1200\t54s:\tAvgLoss 1.737198\tLRate 10.000000\tMBperplex 4.87\tVSperplex 5.59\n",
      "step\t1300\t59s:\tAvgLoss 1.714326\tLRate 10.000000\tMBperplex 5.74\tVSperplex 5.58\n",
      "step\t1400\t63s:\tAvgLoss 1.732243\tLRate 10.000000\tMBperplex 5.95\tVSperplex 5.60\n",
      "step\t1500\t67s:\tAvgLoss 1.722774\tLRate 10.000000\tMBperplex 4.68\tVSperplex 5.51\n",
      "step\t1600\t71s:\tAvgLoss 1.734927\tLRate 10.000000\tMBperplex 5.55\tVSperplex 5.46\n",
      "step\t1700\t75s:\tAvgLoss 1.699065\tLRate 10.000000\tMBperplex 5.50\tVSperplex 5.38\n",
      "step\t1800\t80s:\tAvgLoss 1.663572\tLRate 10.000000\tMBperplex 5.44\tVSperplex 5.18\n",
      "step\t1900\t84s:\tAvgLoss 1.634276\tLRate 10.000000\tMBperplex 5.02\tVSperplex 5.09\n",
      "step\t2000\t88s:\tAvgLoss 1.681092\tLRate 10.000000\tMBperplex 5.64\tVSperplex 5.19\n",
      "================================================================================\n",
      "y has s mahlesser wen tow linnst in w bet formed colleved games s wornting whing\n",
      "ine amerisith has a gover mizh a sub w wead liberiels somethew ashen asuronstepr\n",
      "y adis not is profices in from to somestry is that franfimm on hon kam iiding vo\n",
      "s threnn tetchnon of the with hocks definity in mids s and plane screvatimize di\n",
      "ins sportination belyonies in other ad in viseantary warrerer the gamand that bu\n",
      "================================================================================\n",
      "step\t2100\t92s:\tAvgLoss 1.668905\tLRate 10.000000\tMBperplex 5.13\tVSperplex 4.94\n",
      "step\t2200\t97s:\tAvgLoss 1.664346\tLRate 10.000000\tMBperplex 6.21\tVSperplex 5.13\n",
      "step\t2300\t101s:\tAvgLoss 1.626465\tLRate 10.000000\tMBperplex 5.01\tVSperplex 4.89\n",
      "step\t2400\t105s:\tAvgLoss 1.646044\tLRate 10.000000\tMBperplex 4.89\tVSperplex 4.86\n",
      "step\t2500\t109s:\tAvgLoss 1.667399\tLRate 10.000000\tMBperplex 5.30\tVSperplex 4.71\n",
      "step\t2600\t113s:\tAvgLoss 1.637280\tLRate 10.000000\tMBperplex 5.55\tVSperplex 4.65\n",
      "step\t2700\t117s:\tAvgLoss 1.643258\tLRate 10.000000\tMBperplex 4.64\tVSperplex 4.79\n",
      "step\t2800\t122s:\tAvgLoss 1.634893\tLRate 10.000000\tMBperplex 5.39\tVSperplex 4.67\n",
      "step\t2900\t126s:\tAvgLoss 1.636931\tLRate 10.000000\tMBperplex 5.35\tVSperplex 4.73\n",
      "step\t3000\t130s:\tAvgLoss 1.637419\tLRate 10.000000\tMBperplex 4.92\tVSperplex 4.71\n",
      "================================================================================\n",
      "paying of the back enusa retare formettarish additament with as and reistor is d\n",
      "bare bla with the but in equibly the wata modery of the riper of a bove anatical\n",
      "leutfur simiaa frequecture a d the cames exatten posilical local found could rot\n",
      "matives is mulest is the langsipatory into them dany emperors wordstoncusable gr\n",
      "ricable two die be and words the opposition was was for that country a sia with \n",
      "================================================================================\n",
      "step\t3100\t135s:\tAvgLoss 1.613974\tLRate 10.000000\tMBperplex 5.46\tVSperplex 4.61\n",
      "step\t3200\t139s:\tAvgLoss 1.636834\tLRate 10.000000\tMBperplex 5.49\tVSperplex 4.65\n",
      "step\t3300\t143s:\tAvgLoss 1.621719\tLRate 10.000000\tMBperplex 4.87\tVSperplex 4.54\n",
      "step\t3400\t148s:\tAvgLoss 1.656153\tLRate 10.000000\tMBperplex 5.25\tVSperplex 4.61\n",
      "step\t3500\t152s:\tAvgLoss 1.642217\tLRate 10.000000\tMBperplex 5.50\tVSperplex 4.72\n",
      "step\t3600\t156s:\tAvgLoss 1.652291\tLRate 10.000000\tMBperplex 4.44\tVSperplex 4.55\n",
      "step\t3700\t160s:\tAvgLoss 1.631548\tLRate 10.000000\tMBperplex 5.06\tVSperplex 4.48\n",
      "step\t3800\t164s:\tAvgLoss 1.636792\tLRate 10.000000\tMBperplex 5.60\tVSperplex 4.67\n",
      "step\t3900\t168s:\tAvgLoss 1.625256\tLRate 10.000000\tMBperplex 5.08\tVSperplex 4.52\n",
      "step\t4000\t172s:\tAvgLoss 1.640095\tLRate 10.000000\tMBperplex 4.74\tVSperplex 4.58\n",
      "================================================================================\n",
      "y formal despendiking betatac heddadican symin benere i possacity was cossoly gi\n",
      "que b suagn an colevide iupolal and other amograpm game entined dister koon with\n",
      "ki formar makikilal s hetbech sever human peges the fild e its under s gories xu\n",
      "mal impilal used in the oen sue has basuma rediet he dister the the translered b\n",
      "vists mequinal was sometive to blame actionzer and musing the pools tribise deve\n",
      "================================================================================\n",
      "step\t4100\t177s:\tAvgLoss 1.621029\tLRate 10.000000\tMBperplex 5.16\tVSperplex 4.60\n",
      "step\t4200\t181s:\tAvgLoss 1.622983\tLRate 10.000000\tMBperplex 5.32\tVSperplex 4.53\n",
      "step\t4300\t185s:\tAvgLoss 1.603744\tLRate 10.000000\tMBperplex 4.93\tVSperplex 4.52\n",
      "step\t4400\t189s:\tAvgLoss 1.597448\tLRate 10.000000\tMBperplex 4.87\tVSperplex 4.37\n",
      "step\t4500\t193s:\tAvgLoss 1.604610\tLRate 10.000000\tMBperplex 5.20\tVSperplex 4.49\n",
      "step\t4600\t197s:\tAvgLoss 1.601917\tLRate 10.000000\tMBperplex 4.88\tVSperplex 4.44\n",
      "step\t4700\t201s:\tAvgLoss 1.613996\tLRate 10.000000\tMBperplex 5.35\tVSperplex 4.44\n",
      "step\t4800\t205s:\tAvgLoss 1.621178\tLRate 10.000000\tMBperplex 4.47\tVSperplex 4.42\n",
      "step\t4900\t209s:\tAvgLoss 1.619180\tLRate 10.000000\tMBperplex 5.24\tVSperplex 4.42\n",
      "step\t5000\t214s:\tAvgLoss 1.594168\tLRate 1.000000\tMBperplex 4.53\tVSperplex 4.58\n",
      "================================================================================\n",
      "ry eneared featre and was congrodion with the university promined of b one zents\n",
      "gic who loss marrdle publatwore in one nine three zero zero zero zero mility a m\n",
      "ficion wind useatal meting two in orextored they sedgintioned the chimoge to an \n",
      "vinies wescelft cleardgen for the greal were wijmion cont with the sire minister\n",
      "en used plained foreignd a finotinisticalaw two were are as two zero zero house \n",
      "================================================================================\n",
      "step\t5100\t218s:\tAvgLoss 1.594900\tLRate 1.000000\tMBperplex 4.88\tVSperplex 4.38\n",
      "step\t5200\t222s:\tAvgLoss 1.579601\tLRate 1.000000\tMBperplex 4.53\tVSperplex 4.30\n",
      "step\t5300\t227s:\tAvgLoss 1.563001\tLRate 1.000000\tMBperplex 4.36\tVSperplex 4.27\n",
      "step\t5400\t231s:\tAvgLoss 1.562495\tLRate 1.000000\tMBperplex 5.03\tVSperplex 4.28\n",
      "step\t5500\t235s:\tAvgLoss 1.548276\tLRate 1.000000\tMBperplex 4.77\tVSperplex 4.25\n",
      "step\t5600\t239s:\tAvgLoss 1.564582\tLRate 1.000000\tMBperplex 4.87\tVSperplex 4.25\n",
      "step\t5700\t243s:\tAvgLoss 1.553715\tLRate 1.000000\tMBperplex 4.45\tVSperplex 4.28\n",
      "step\t5800\t247s:\tAvgLoss 1.562739\tLRate 1.000000\tMBperplex 4.88\tVSperplex 4.25\n",
      "step\t5900\t251s:\tAvgLoss 1.562416\tLRate 1.000000\tMBperplex 5.05\tVSperplex 4.24\n",
      "step\t6000\t256s:\tAvgLoss 1.531076\tLRate 1.000000\tMBperplex 4.98\tVSperplex 4.23\n",
      "================================================================================\n",
      "varie of the one nine eight two one as least to genograpr producind born of the \n",
      "xa behy sack the final one nine x expanded in the extarianed on pupilits uperanc\n",
      "raphisces ear and catie in exployted diskeyion leit but the criting in receeted \n",
      " have afcelogical marioled transuse sey in percesse there tame apponenmor redica\n",
      "aschox america year applucation specident nine eight a the listed distance decar\n",
      "================================================================================\n",
      "step\t6100\t260s:\tAvgLoss 1.554299\tLRate 1.000000\tMBperplex 4.94\tVSperplex 4.21\n",
      "step\t6200\t264s:\tAvgLoss 1.524284\tLRate 1.000000\tMBperplex 4.78\tVSperplex 4.25\n",
      "step\t6300\t268s:\tAvgLoss 1.531670\tLRate 1.000000\tMBperplex 4.87\tVSperplex 4.21\n",
      "step\t6400\t272s:\tAvgLoss 1.529613\tLRate 1.000000\tMBperplex 4.41\tVSperplex 4.23\n",
      "step\t6500\t276s:\tAvgLoss 1.543709\tLRate 1.000000\tMBperplex 4.48\tVSperplex 4.21\n",
      "step\t6600\t280s:\tAvgLoss 1.578070\tLRate 1.000000\tMBperplex 4.69\tVSperplex 4.19\n",
      "step\t6700\t285s:\tAvgLoss 1.563192\tLRate 1.000000\tMBperplex 5.14\tVSperplex 4.25\n",
      "step\t6800\t289s:\tAvgLoss 1.589561\tLRate 1.000000\tMBperplex 4.61\tVSperplex 4.24\n",
      "step\t6900\t293s:\tAvgLoss 1.568692\tLRate 1.000000\tMBperplex 4.61\tVSperplex 4.28\n",
      "step\t7000\t297s:\tAvgLoss 1.564161\tLRate 1.000000\tMBperplex 4.98\tVSperplex 4.24\n",
      "================================================================================\n",
      "zer donier a jejuant play for kankinger stay that held cheaibul congrent defecte\n",
      "thes however the imperorucl who have impress the speakina years world wrom requi\n",
      "bitable this more of the actions and recoveres of language war to hadegistrak ce\n",
      "urnk or the name withbijation in one one basels boung of must recurticle from fa\n",
      "e presumrary attace cectors but as or two pets one eight lev par s prefts and ne\n",
      "================================================================================\n",
      "End.\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# EJECUCION LTSM1\n",
    "##################\n",
    "t1 = time.time()\n",
    "\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graphLSTM1) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "\n",
    "    #Informe\n",
    "    if step % summary_frequency == 0:\n",
    "      t2 = time.time()\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # La preplejidad depende de cuanto acierta con las etiquetas\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      mbpx = float(np.exp(logprob(predictions, labels)))\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      vspx = float(np.exp(valid_logprob / valid_size))\n",
    "      #Report\n",
    "      print('step\\t%d\\t%ds:\\tAvgLoss %f\\tLRate %f\\tMBperplex %.2f\\tVSperplex %.2f' % (step, t2-t1, mean_loss, lr, mbpx, vspx))\n",
    "      mean_loss = 0\n",
    "    \n",
    "      #Informe más completo con muestras\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "  print(\"End.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
