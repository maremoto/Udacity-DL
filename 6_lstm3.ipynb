{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6 - PROBLEM 3\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import collections\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def how_long(f, *args):\n",
    "    #medir el tiempo que tarda f\n",
    "    t1 = time.time()\n",
    "    res = f(*args)\n",
    "    t2 = time.time()\n",
    "    print (\"tiempo utilizado = \",t2-t1)\n",
    "    #return res, t2-t1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "  return data\n",
    "  \n",
    "words = read_data(filename)\n",
    "print('Data size %d' % len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the dictionary and replace rare words with UNK token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiempo utilizado =  61.1043949127\n",
      "Most common words (+ GO,EOS,UNK) [['GO', 0], ['EOS', 0], ['UNK', 418409], ('the', 1061396), ('of', 593677)]\n",
      "Sample data [5241, 3086, 14, 8, 197, 4, 3139, 48, 61, 158]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 50000\n",
    "\n",
    "def build_dataset(words):\n",
    "  #RESERVED WORD IDs\n",
    "  #ID=0 para la palabra especial GO\n",
    "  #ID=1 para la palabra especial EOS\n",
    "  #ID=2 para los \"OTROS POCO FRECUENTES\" que llama \"UNK\"\n",
    "\n",
    "  #inicializa el array de contadores de palabras (frecuencias)\n",
    "  count = [['GO', 0],['EOS', 0],['UNK', -1]]\n",
    "  #cuenta las 50000 palabras más comunes (menos las tres reservadas)\n",
    "  count.extend(collections.Counter(words).most_common(vocabulary_size - 3))\n",
    "  #inicializa un diccionario del vocabulario, va a crear pares palabra - ID, para tratar con números\n",
    "  #y asigna los id por orden de frecuencia de más a menos\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  #inicializa y crea una lista de ID equivalente al dataset \"words\" reemplazando cada palabra por su ID\n",
    "  #y de paso va apuntando los UNK para rellenar la frecuencia\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 2  # dictionary['UNK']\n",
    "      unk_count = unk_count + 1\n",
    "    data.append(index)\n",
    "  #apunta el dato final de cuantas palabras raras hay (frecuencia de palabras raras)\n",
    "  count[2][1] = unk_count\n",
    "  #crea un diccionario al revés, es decir, donde el ID es la clave y la palabra es el valor\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = how_long(build_dataset,words)\n",
    "print('Most common words (+ GO,EOS,UNK)', count[:5])\n",
    "print('Sample data', data[:10])\n",
    "del words  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17004207 [66, 10288, 5241, 3246, 9633, 7, 6, 15, 12, 18, 10881, 5866, 51, 5444, 7, 8, 2, 2413, 3089, 21, 620, 7420, 91, 60, 5107, 36, 1339, 7, 8, 291, 83, 17367, 151, 1186, 1219, 5866, 7518, 3, 442, 17, 1895, 27, 8, 1133, 4, 628, 3419, 5, 8, 1133, 4, 820, 920, 5866, 7453, 9, 18863, 2, 4788, 1509, 36, 5774, 156, 38]\n",
      "train: ['american', 'individualist', 'anarchism', 'benjamin', 'tucker', 'in', 'one', 'eight', 'two', 'five', 'josiah', 'warren', 'had', 'participated', 'in', 'a', 'UNK', 'experiment', 'headed', 'by', 'robert', 'owen', 'called', 'new', 'harmony', 'which', 'failed', 'in', 'a', 'few', 'years', 'amidst', 'much', 'internal', 'conflict', 'warren', 'blamed', 'the', 'community', 's', 'failure', 'on', 'a', 'lack', 'of', 'individual', 'sovereignty', 'and', 'a', 'lack', 'of', 'private', 'property', 'warren', 'proceeded', 'to', 'organise', 'UNK', 'anarchist', 'communities', 'which', 'respected', 'what', 'he']\n",
      "1000 [5241, 3086, 14, 8, 197, 4, 3139, 48, 61, 158, 130, 744, 479, 10574, 136, 3, 27551, 4, 3, 105, 857, 5, 3, 15070, 2, 4, 3, 153, 857, 3585, 3, 197, 13, 193, 61, 7, 8, 10744, 217, 9, 1328, 107, 457, 22, 61, 2736, 365, 9, 3678, 3, 711, 4, 374, 29, 43, 39, 56, 543, 100, 14, 8, 1428, 2762, 21]\n",
      "valid: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'UNK', 'of', 'the', 'french', 'revolution', 'whilst', 'the', 'term', 'is', 'still', 'used', 'in', 'a', 'pejorative', 'way', 'to', 'describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the', 'organization', 'of', 'society', 'it', 'has', 'also', 'been', 'taken', 'up', 'as', 'a', 'positive', 'label', 'by']\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000 #las mil primeras palabras son de validación\n",
    "valid_text = data[:valid_size]\n",
    "train_text = data[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print('train:',[reverse_dictionary[i] for i in train_text[:64]])\n",
    "print(valid_size, valid_text[:64])\n",
    "print('valid:',[reverse_dictionary[i] for i in valid_text[:64]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tahw\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "def word2id(word):\n",
    "  if word in dictionary:\n",
    "    return dictionary[word]\n",
    "  else:\n",
    "    print('Unexpected word: %s' % word)\n",
    "    return 0\n",
    "  \n",
    "def id2word(id):\n",
    "  if id < vocabulary_size:\n",
    "    return reverse_dictionary[id]\n",
    "  else:\n",
    "    print('Unexpected word id: ', id)\n",
    "    return 'UNK'\n",
    "\n",
    "print(word2id('what'), word2id('woman'), word2id('supercalifragilisticoexpialidoso'))\n",
    "print(id2word(1), id2word(268), id2word(0), id2word(vocabulary_size+1))\n",
    "\"\"\"\n",
    "def reverseword(word):\n",
    "    return word[::-1]\n",
    "\n",
    "print(reverseword('what'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The language to translate to: inversed words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id =  3 \tword =  the \tid2 =  33988 \tword2 =  eht\n",
      "id =  3905 \tword =  quick \tid2 =  2342 \tword2 =  kciuq\n",
      "id =  1201 \tword =  brown \tid2 =  8646 \tword2 =  nworb\n",
      "id =  2528 \tword =  fox \tid2 =  37023 \tword2 =  xof\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size_2 = vocabulary_size #both languages have the same number of words\n",
    "\n",
    "reserved = [0,1]\n",
    "i = vocabulary_size - 1\n",
    "dictionary_2 = dict()\n",
    "reverse_dictionary_2 = dict()\n",
    "for w in dictionary:\n",
    "    if(dictionary[w] not in reserved):\n",
    "        #reverse the IDs as well, so it is not too obvious\n",
    "        w2 = reverseword(w)\n",
    "        dictionary_2[w2] = i\n",
    "        reverse_dictionary_2[i] = w2\n",
    "        i =  i - 1\n",
    "dictionary_2['GO'] = 0\n",
    "dictionary_2['EOS'] = 1\n",
    "reverse_dictionary_2[0] = 'GO'\n",
    "reverse_dictionary_2[1] = 'EOS'\n",
    "        \n",
    "for w in ['the','quick','brown','fox']:\n",
    "    print('id = ',dictionary[w],'\\tword = ',w,\n",
    "          '\\tid2 = ',dictionary_2[reverseword(w)],'\\tword2 = ',reverse_dictionary_2[dictionary_2[reverseword(w)]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "9887\n",
      "33988\n"
     ]
    }
   ],
   "source": [
    "print(dictionary['GO'])\n",
    "print(dictionary['EOS'])\n",
    "print(dictionary['UNK'])\n",
    "print(dictionary['the'])\n",
    "print(dictionary_2['GO'])\n",
    "print(dictionary_2['EOS'])\n",
    "print(dictionary_2['KNU'])\n",
    "print(dictionary_2['eht'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch and labels for the Seq2Seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "5 5 5\n",
      "(3,)\n",
      "[ 66   3 578]\n",
      "(3,)\n",
      "[45057 33988 18302]\n",
      "(3, 50000)\n",
      "[' american individualist anarchism benjamin tucker', ' the table top wargame warhammer', ' subject to much debate recent']\n",
      "[' nacirema tsilaudividni msihcrana nimajneb rekcut', ' eht elbat pot emagraw remmahraw', ' tcejbus ot hcum etabed tnecer']\n",
      "[' nacirema tsilaudividni msihcrana nimajneb rekcut', ' eht elbat pot emagraw remmahraw', ' tcejbus ot hcum etabed tnecer']\n",
      "[' tucker in one eight two', ' warhammer four zero zero zero', ' recent archaeological finds suggest multiple']\n",
      "[' rekcut ni eno thgie owt', ' remmahraw ruof orez orez orez', ' tnecer lacigoloeahcra sdnif tseggus elpitlum']\n",
      "[' rekcut ni eno thgie owt', ' remmahraw ruof orez orez orez', ' tnecer lacigoloeahcra sdnif tseggus elpitlum']\n",
      "\n",
      "valid\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[' anarchism']\n",
      "[' msihcrana']\n",
      "[' msihcrana']\n"
     ]
    }
   ],
   "source": [
    "class BatchGeneratorS2S(object): #return batch and labels\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings #\"phrase\" size\n",
    "    segment = self._text_size // batch_size #reparte el texto en N caminos de predicion (N=batch_size)\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)] #array de cursores para los N caminos\n",
    "    self._last_encoder, self._last_decoder, self._last_target = self._next_batch()\n",
    "    \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    encoder = np.zeros(shape=(self._batch_size), dtype=np.int)\n",
    "    decoder = np.zeros(shape=(self._batch_size), dtype=np.int)\n",
    "    target = np.zeros(shape=(self._batch_size,vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      encoder[b] = self._text[self._cursor[b]]\n",
    "      decoder[b] = dictionary_2[reverseword(reverse_dictionary[self._text[self._cursor[b]]])]\n",
    "      target[b, dictionary_2[reverseword(reverse_dictionary[self._text[self._cursor[b]]])]] = 1.0 #1-hot-encoded array\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size #incrementa el puntero en modo circular por el texto\n",
    "    return encoder,decoder,target\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    encoders = [self._last_encoder]\n",
    "    decoders = [self._last_decoder]\n",
    "    targets = [self._last_target]\n",
    "    for step in range(self._num_unrollings-1): \n",
    "      e,d,t = self._next_batch()\n",
    "      encoders.append(e)\n",
    "      decoders.append(d)\n",
    "      targets.append(t)\n",
    "    self._last_encoder = encoders[-1]\n",
    "    self._last_decoder = decoders[-1]\n",
    "    self._last_target = targets[-1]\n",
    "    return encoders,decoders,targets\n",
    "\n",
    "def words(reverse_dictionary, probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  words back into its (most likely) character representation.\"\"\"\n",
    "  return [reverse_dictionary[w] for w in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(reverse_dictionary, batches):\n",
    "  \"\"\"Convert a sequence of batches back into their string representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for i,b in enumerate(batches):\n",
    "    for j,e in enumerate(b):\n",
    "        s[j] = s[j] +' '+ reverse_dictionary[e]\n",
    "  return s\n",
    "\n",
    "def targets2string(reverse_dictionary, targets):\n",
    "  \"\"\"Convert a sequence of 1-hot-encoded targets back into their (most likely) string representation.\"\"\"\n",
    "  s = [''] * targets[0].shape[0]\n",
    "  for i,b in enumerate(targets):\n",
    "    s = [''.join(x) for x in zip(s,[' ']*targets[0].shape[0],words(reverse_dictionary,b))] \n",
    "  return s\n",
    "\n",
    "train_batches = BatchGeneratorS2S(train_text, 3, 5) #batch_size=3 num_unrollings=5 for function testing\n",
    "valid_batches = BatchGeneratorS2S(valid_text, 1, 1)\n",
    "\n",
    "print(\"train\")\n",
    "e1,d1,t1 = train_batches.next()\n",
    "print(len(e1),len(d1),len(t1))\n",
    "print(e1[0].shape)\n",
    "print(e1[0])\n",
    "print(d1[0].shape)\n",
    "print(d1[0])\n",
    "print(t1[0].shape)\n",
    "print(batches2string(reverse_dictionary,e1))\n",
    "print(batches2string(reverse_dictionary_2,d1))\n",
    "print(targets2string(reverse_dictionary_2,t1))\n",
    "\n",
    "e2,d2,t2 = train_batches.next()\n",
    "print(batches2string(reverse_dictionary,e2))\n",
    "print(batches2string(reverse_dictionary_2,d2))\n",
    "print(targets2string(reverse_dictionary_2,t2))\n",
    "\n",
    "print(\"\\nvalid\")\n",
    "ve1,vd1,vt1 = valid_batches.next()\n",
    "print(batches2string(reverse_dictionary,ve1))\n",
    "print(batches2string(reverse_dictionary_2,vd1))\n",
    "print(targets2string(reverse_dictionary_2,vt1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized probabilities.\"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Seq2seq LSTM Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[documento] (https://arxiv.org/pdf/1409.3215v3.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"basic_seq2seq.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# DECLARACION LSTM3 seq2seq\n",
    "##################\n",
    "\n",
    "batch_size=64 #frases en un lote\n",
    "num_unrollings=5 #palabras en cada frase (hay uno más de solapamiento)\n",
    "\n",
    "embedding_size = 1024 # Dimension of the embedding vector. Las features de cada palabra que la distinguen.\n",
    "\n",
    "num_nodes = 64 #Tantas células en la memoria como líneas de entrenamiento va a seguir en un lote, que casualidad\n",
    "starter_learning_rate = 10.0\n",
    "learning_decay_steps = 5000\n",
    "learning_decay_rate = 0.1\n",
    "clip_limit = 1.25\n",
    "\n",
    "graphLSTM3 = tf.Graph()\n",
    "with graphLSTM3.as_default():\n",
    "  \n",
    "  # Special words for beginning and end\n",
    "  GO = tf.Variable(np.zeros([batch_size], dtype=np.int32), trainable=False)\n",
    "  e = np.zeros([batch_size,vocabulary_size_2], dtype=np.float32)\n",
    "  e[:,1] = 1.0 #ohe for labels, EOS id is \"1\"\n",
    "  EOS = tf.Variable(e, trainable=False)\n",
    "    \n",
    "  # Input data, get input and labels.\n",
    "  train_encoder = list()\n",
    "  for _ in range(num_unrollings):\n",
    "    train_encoder.append(tf.placeholder(tf.int32, shape=[batch_size])) #word ids for input language\n",
    "  train_decoder = list()\n",
    "  for _ in range(num_unrollings):\n",
    "    train_decoder.append(tf.placeholder(tf.int32, shape=[batch_size])) #word ids for output language \"inversish\" :-)\n",
    "  train_targets = list()\n",
    "  for _ in range(num_unrollings):\n",
    "    train_targets.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size_2])) #ohe for output language\n",
    "\n",
    "  # LSTM Gates: 0=memory cell, 1=input, 2=forget, 3=output\n",
    "  num_gates = 4  \n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, last_state, gx, gm, gb):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the previous state and the gates.\"\"\"\n",
    "    I = tf.stack([i,i,i,i])\n",
    "    O = tf.stack([o,o,o,o])\n",
    "    gates = tf.matmul(I, gx) + tf.matmul(O, gm) + gb\n",
    "    update = gates[0,:,:]\n",
    "    input_gate = tf.sigmoid(gates[1,:,:])\n",
    "    forget_gate = tf.sigmoid(gates[2,:,:])\n",
    "    output_gate = tf.sigmoid(gates[3,:,:])\n",
    "    next_state = forget_gate * last_state + input_gate * tf.tanh(update)\n",
    "    outputmem = output_gate * tf.tanh(next_state)\n",
    "    return outputmem, next_state\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_omem = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "  #################################\n",
    "  # ENCODER variables:\n",
    "  #################################\n",
    "  embeddings_E = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)) \n",
    "\n",
    "  #Gates parameters:\n",
    "  gxE = tf.Variable(tf.truncated_normal([num_gates, embedding_size, num_nodes], -0.1, 0.1))\n",
    "  gmE = tf.Variable(tf.truncated_normal([num_gates, num_nodes, num_nodes], -0.1, 0.1))\n",
    "  gbE = tf.Variable(tf.zeros([num_gates, 1, num_nodes]))\n",
    "\n",
    "  #################################\n",
    "  # DECODER variables:\n",
    "  #################################\n",
    "  embeddings_D = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)) \n",
    "\n",
    "  #Gates parameters:\n",
    "  gxD = tf.Variable(tf.truncated_normal([num_gates, embedding_size, num_nodes], -0.1, 0.1))\n",
    "  gmD = tf.Variable(tf.truncated_normal([num_gates, num_nodes, num_nodes], -0.1, 0.1))\n",
    "  gbD = tf.Variable(tf.zeros([num_gates, 1, num_nodes]))\n",
    "\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  #################################\n",
    "  # ENCODER + DECODER processing:\n",
    "  #################################\n",
    "  \n",
    "  #Inputs for encoder\n",
    "  train_encoder_X = list()\n",
    "  for i in range(num_unrollings):\n",
    "    train_encoder_X.append(tf.nn.embedding_lookup(embeddings_E, train_encoder[i])) #input encoder embeddings\n",
    "\n",
    "  # Unrolled encoder LSTM loop.\n",
    "  omem = saved_omem\n",
    "  state = saved_state\n",
    "  for i in train_encoder_X:\n",
    "    omem, state = lstm_cell(i, omem, state, gxE, gmE, gbE)\n",
    "\n",
    "  #Inputs for decoder\n",
    "  train_decoder_X = list() #GO + translated_words_embeddings: num_unrollings + 1\n",
    "  train_decoder_X.append(tf.nn.embedding_lookup(embeddings_D, GO)) #GO at the beginning\n",
    "  for i in range(num_unrollings):\n",
    "    train_decoder_X.append(tf.nn.embedding_lookup(embeddings_D, train_decoder[i])) #Convierte ID en embeddings\n",
    "\n",
    "  #Labels\n",
    "  train_decoder_y = list() #translated_words_ohe + EOS: num_unrollings + 1\n",
    "  for i in range(num_unrollings):\n",
    "    train_decoder_y.append(train_targets[i])\n",
    "  train_decoder_y.append(EOS) #EOS at the end\n",
    "\n",
    "  # Unrolled decoder LSTM loop. IMPORTANTE, NO SE REINICIA omem ni state, se heredan del ENCODER\n",
    "  omemoriesD = list()\n",
    "  for i in train_decoder_X:\n",
    "    omem, state = lstm_cell(i, omem, state, gxD, gmD, gbD)\n",
    "    omemoriesD.append(omem) #esto va luego para la salida y evaluación de pérdidas\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_omem.assign(omem),saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat_v2(omemoriesD, 0), w, b)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat_v2(train_decoder_y, 0)))\n",
    "                #Evalúa la salida de cada ráfaga con un logistic classifier normal\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, \n",
    "                                             learning_decay_steps, learning_decay_rate, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate) #TODO, ¿usar Adagrad?\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, clip_limit) #avoid exploding gradients\n",
    "  optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  #################################\n",
    "  # SAMPLE and VALIDATION:\n",
    "  #################################\n",
    "\n",
    "  # Sampling and validation eval: batch size = 1, ¿no unrolling? prueba de una mono-palabra.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  saved_sample_omem = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(saved_sample_omem.assign(tf.zeros([1, num_nodes])),\n",
    "                                saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  \n",
    "  #Al encoder le meto la palabra prueba, \n",
    "  #al decoder le meto GO y espero sacar la palabra traducida \n",
    "  sample_omem, sample_state = lstm_cell(tf.nn.embedding_lookup(embeddings_E, sample_input), \n",
    "                                         saved_sample_omem, saved_sample_state, gxD, gmD, gbD) #encoder\n",
    "\n",
    "  sample_GO = tf.Variable(np.zeros([1], dtype=np.int32), trainable=False)\n",
    "  sample_omem, sample_state = lstm_cell(tf.nn.embedding_lookup(embeddings_D, sample_GO), \n",
    "                                        sample_omem, sample_state, gxD, gmD, gbD) #decoder\n",
    "    \n",
    "  with tf.control_dependencies([saved_sample_omem.assign(sample_omem),saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_omem, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "step\t0\t10s:\tAvgLoss 10.844849\tLRate 10.000000\tMBperplex 51269.32\tVSperplex 40173.87\n",
      "================================================================================\n",
      "the quick brown fox\n",
      " satsinidnas remitrom degdirb tcnujnoc\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# EJECUCION LTSM3 seq2seq\n",
    "##################\n",
    "t1 = time.time()\n",
    "\n",
    "train_batches = BatchGeneratorS2S(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGeneratorS2S(valid_text, 1, 1)\n",
    "\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "eos = np.zeros([batch_size,vocabulary_size_2], dtype=np.float32)\n",
    "eos[:,1] = 1.0 #ohe for labels, EOS id is \"1\"\n",
    "\n",
    "with tf.Session(graph=graphLSTM3) as session: \n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    encoders,decoders,targets = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings):\n",
    "      feed_dict[train_encoder[i]] = encoders[i]\n",
    "      feed_dict[train_decoder[i]] = decoders[i]\n",
    "      feed_dict[train_targets[i]] = targets[i]\n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "\n",
    "    #Informe\n",
    "    if step % summary_frequency == 0:\n",
    "      t2 = time.time()\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # La preplejidad depende de cuanto acierta con las etiquetas\n",
    "      targets.append(eos)\n",
    "      labels = np.concatenate(list(targets))\n",
    "      mbpx = float(np.exp(logprob(predictions, labels)))\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        ve,vd,vt = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: ve[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, vt[0])\n",
    "      vspx = float(np.exp(valid_logprob / valid_size))\n",
    "      #Report\n",
    "      print('step\\t%d\\t%ds:\\tAvgLoss %f\\tLRate %f\\tMBperplex %.2f\\tVSperplex %.2f' \n",
    "            % (step, t2-t1, mean_loss, lr, mbpx, vspx))\n",
    "      mean_loss = 0\n",
    "    \n",
    "      #Informe más completo con muestras\n",
    "      #TODO hacerlo de una tacada, una gestión sample de frase completa\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        test_phrase = ['the','quick','brown','fox']\n",
    "        translated_sentence = ''\n",
    "        reset_sample_state.run()\n",
    "        for w in test_phrase:\n",
    "          feed = [dictionary[w]]\n",
    "          prediction = sample_prediction.eval({sample_input: feed})\n",
    "          s = words(reverse_dictionary_2,sample(prediction))\n",
    "          translated_sentence = translated_sentence + ' ' + s[0]\n",
    "        print('the quick brown fox')\n",
    "        print(translated_sentence)\n",
    "        print('=' * 80)\n",
    "  print(\"End.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
