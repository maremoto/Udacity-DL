{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 5\n",
    "------------\n",
    "\n",
    "The goal of this assignment is to train a Word2Vec skip-gram model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explicacion de un skip-gram: [Word2Vec-skipgram](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "consiste en crear un \"concepto\" de cada palabra basado en la relación con las demás palabras del vocabulario, y luego poder usar ese concepto posteriormente, dado que tratamos con un sistema no supervisado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "0K1ZyLn04QZf"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "def how_long(f, *args):\n",
    "    #medir el tiempo que tarda f\n",
    "    t1 = time.time()\n",
    "    res = f(*args)\n",
    "    t2 = time.time()\n",
    "    print (\"tiempo utilizado = \",t2-t1)\n",
    "    #return res, t2-t1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aCjPJE944bkV"
   },
   "source": [
    "Download the data from the source website if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 14640,
     "status": "ok",
     "timestamp": 1445964482948,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2f1ffade4c9f20de",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "c4ec222c-80b5-4298-e635-93ca9f79c3b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zqz3XiqI4mZT"
   },
   "source": [
    "Read the data into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 28844,
     "status": "ok",
     "timestamp": 1445964497165,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2f1ffade4c9f20de",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "e3a928b4-1645-4fe8-be17-fcf47de5716d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "  return data\n",
    "  \n",
    "words = read_data(filename)\n",
    "print('Data size %d' % len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Build the dictionary and replace rare words with UNK token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 28849,
     "status": "ok",
     "timestamp": 1445964497178,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2f1ffade4c9f20de",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "3fb4ecd1-df67-44b6-a2dc-2291730970b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiempo utilizado =  35.0641279221\n",
      "Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 50000\n",
    "\n",
    "def build_dataset(words):\n",
    "  #inicializa el array de contadores de palabras (frecuencias)\n",
    "  count = [['UNK', -1]]\n",
    "  #cuenta las 50000 palabras más comunes\n",
    "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "  #inicializa un diccionario del vocabulario, va a crear pares palabra - ID, para tratar con números\n",
    "  #y asigna los id por orden de frecuencia de más a menos\n",
    "  #RESERVA ID=0 para los \"OTROS POCO FRECUENTES\" que llama \"UNK\"\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  #inicializa y crea una lista de ID equivalente al dataset \"words\" reemplazando cada palabra por su ID\n",
    "  #y de paso va apuntando los UNK para rellenar la frecuencia\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count = unk_count + 1\n",
    "    data.append(index)\n",
    "  #apunta el dato final de cuantas palabras raras hay (frecuencia de palabras raras)\n",
    "  count[0][1] = unk_count\n",
    "  #crea un diccionario al revés, es decir, donde el ID es la clave y la palabra es el valor\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = how_long(build_dataset,words)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10])\n",
    "del words  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the skip-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 113,
     "status": "ok",
     "timestamp": 1445964901989,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2f1ffade4c9f20de",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w9APjA-zmfjV",
    "outputId": "67cccb02-cdaf-4e47-d489-43bcc8d57bb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first']\n",
      "\n",
      "with num_skips = 2 and skip_window = 1:\n",
      "    batch: ['originated', 'originated', 'as', 'as', 'a', 'a', 'term', 'term']\n",
      "    labels: ['anarchism', 'as', 'a', 'originated', 'as', 'term', 'a', 'of']\n",
      "\n",
      "with num_skips = 4 and skip_window = 2:\n",
      "    batch: ['as', 'as', 'as', 'as', 'a', 'a', 'a', 'a']\n",
      "    labels: ['term', 'anarchism', 'a', 'originated', 'originated', 'as', 'of', 'term']\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "#batch_size, tamaño del lote a meter en la red neural para aprender\n",
    "#skip_window, tamaño del lado derecho e izquierdo de la ventana alrededor de una palabra (es simétrica)\n",
    "#num_skips, distancia de palabra adyacente que va a tomar como etiqueta \n",
    "\n",
    "def generate_batch_skipgram(batch_size, num_skips, skip_window):\n",
    "  global data_index #indicador de por donde va recorriendo los datos de aprendizaje\n",
    "  \n",
    "  #comprobaciones\n",
    "  assert batch_size % num_skips == 0 #el tamaño del lote debe ser divisible por los descartes\n",
    "  assert num_skips <= 2 * skip_window\n",
    "\n",
    "  #variables a usar\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  span = 2 * skip_window + 1 # [ skip_window target skip_window ] \n",
    "    #se mira alrededor de la palabra el ancho de ventana (2*ventana y +1 la palabra en medio)\n",
    "  buffer = collections.deque(maxlen=span) #reserva variable vacía tipo \"deque\" para almacenar \n",
    "                                            #una ventana que rodea una palabra\n",
    "  \n",
    "  #rellena una ventana inicial desde donde se quedo\n",
    "  for _ in range(span):\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data) #circular\n",
    "\n",
    "  #rellena un lote y a cada palabra le pone de objetivo las palabras adyacentes a distancia \n",
    "    #num_skips o menor delante y detrás\n",
    "  #siempre por pares, pero tomando RANDOM de esa ventana, no en orden\n",
    "  for i in range(batch_size // num_skips):\n",
    "    target = skip_window  # target label at the center of the buffer\n",
    "    targets_to_avoid = [ skip_window ]\n",
    "    for j in range(num_skips):\n",
    "      #monta todo este lío para que no vayan en orden exacto las palabras...\n",
    "      while target in targets_to_avoid:\n",
    "        target = random.randint(0, span - 1)\n",
    "      targets_to_avoid.append(target)\n",
    "      batch[i * num_skips + j] = buffer[skip_window]\n",
    "      labels[i * num_skips + j, 0] = buffer[target]\n",
    "    \n",
    "    #pasa a la siguiente ventana\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "    \n",
    "  return batch, labels\n",
    "\n",
    "#Ejecucion de ejemplo para las primeras ocho palabras del diccionario (un lote de ocho)\n",
    "\n",
    "lote = 8\n",
    "print('data:', [reverse_dictionary[di] for di in data[:lote]])\n",
    "\n",
    "for num_skips, skip_window in [(2, 1), (4, 2)]:\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch_skipgram(batch_size=lote, num_skips=num_skips, skip_window=skip_window)\n",
    "    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n",
    "    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(lote)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ofd1MbBuwiva"
   },
   "source": [
    "Train a skip-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "8pQKsV4Vwlzy"
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# DECLARACION SKIP-GRAM\n",
    "##################\n",
    "\n",
    "batch_size = 128 #Lotes de entrenamiento.\n",
    "\n",
    "embedding_size = 128 # Dimension of the embedding vector. Las features de cada palabra que la distinguen.\n",
    "skip_window = 1 # How many words to consider left and right. Semiventana, al final ventana de 3.\n",
    "                # Es poco debería ser mayor pero más procesamiento también.\n",
    "num_skips = 2 # How many times to reuse an input to generate a label. \n",
    "                #Normalmente ancho-ventana - 1 (3-1 = 2). O 2*skip_window\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by construction are also the most frequent. \n",
    "# Va a utilizar como validationSet las palabras más comunes del diccionario (las del principio)\n",
    "valid_size = 16 # Random set of words to evaluate similarity on. Escoge 16 para validacion.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution. Las escoge de entre las 100 primeras.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "\n",
    "# CURIOSAMENTE NO TOMA UN TEST SET ESTA VEZ, LE DA IGUAL\n",
    "\n",
    "num_sampled = 64 # Number of negative examples to sample. Muestras aleatorias de entre el lote \n",
    "                                                        #que sirven para calcular LOSS.\n",
    "\n",
    "learning_rate = 1.0\n",
    "\n",
    "graphSG = tf.Graph()\n",
    "\n",
    "with graphSG.as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "  # Input data.\n",
    "  train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "  \n",
    "  # Variables y su inicializacion.\n",
    "       #CONCEPTO-FEATURES DE UNA PALABRA (primer layer):\n",
    "  embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)) \n",
    "      #RELACION CON OTROS CONCEPTOS (segundo layer):\n",
    "  softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],stddev=1.0 / math.sqrt(embedding_size)))\n",
    "  softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Model.\n",
    "  # Look up embeddings for inputs.\n",
    "  embed = tf.nn.embedding_lookup(embeddings, train_dataset) #Toma de los conceptos las líneas que corresponden \n",
    "                                                            #al lote de turno (por ID)\n",
    "  # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed, train_labels, num_sampled, vocabulary_size)) \n",
    "        #Combina cada conceptos con las relaciones y obtiene la probabilidad de vecindad de las demás palabras del diccionario\n",
    "        #luego compara esa vecindad probable con las etiquetas para num_sampled aleatorias y saca la pérdida\n",
    "\n",
    "  #At inference time, you can compute full softmax probabilities for each word (referred to one word) with the expression \n",
    "  #tf.nn.softmax(tf.matmul(one_embed, tf.transpose(softmax_weights)) + softmax_biases).\n",
    "        \n",
    "  # Optimizer.\n",
    "  # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n",
    "  # This is because the embeddings are defined as a variable quantity and the\n",
    "  # optimizer's `minimize` method will by default modify all variable quantities \n",
    "  # that contribute to the tensor it is passed.\n",
    "  # See docs on `tf.train.Optimizer.minimize()` for more details.\n",
    "  optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss)\n",
    "  \n",
    "  # Compute the similarity between minibatch examples and all embeddings.\n",
    "  # We use the cosine distance:\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "  similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### OJO Usa un optimizador de gradiente adaptativo:\n",
    "[Adagrad](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 23
      },
      {
       "item_id": 48
      },
      {
       "item_id": 61
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 436189,
     "status": "ok",
     "timestamp": 1445965429787,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2f1ffade4c9f20de",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "1bQFGceBxrWW",
    "outputId": "5ebd6d9a-33c6-4bcd-bf6d-252b0b6055e4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "step 0\t1s\tAverage loss: 8.236175\n",
      "\tNearest to zero: allele, nicolay, crucial, dharmas, illusion, locator, speaker, iconostasis,\n",
      "\tNearest to will: tac, dubbing, cowan, weights, identifier, peacocks, olivia, shipwrecked,\n",
      "\tNearest to so: booths, modalities, lasker, nikolaevich, bolshevism, flasks, fell, austere,\n",
      "\tNearest to can: donuts, eratosthenes, carnal, advisors, hypocrites, damage, byrd, mario,\n",
      "\tNearest to are: words, goa, kinship, unplugged, erwin, rotation, chorus, proven,\n",
      "\tNearest to would: spitfire, morphemes, legionaries, suzuki, imr, urgency, anchorages, accelerating,\n",
      "\tNearest to time: presses, monotheist, exhortation, mtp, huey, abdicates, anjou, horseshoe,\n",
      "\tNearest to no: sars, cadillac, mod, leonhard, heiress, processing, spared, chilperic,\n",
      "\tNearest to people: nitrous, proverbs, cycling, emet, hyphens, dickie, celtiberian, honestly,\n",
      "\tNearest to united: amplifier, directx, objectively, beaten, geer, park, hummers, tanakh,\n",
      "\tNearest to in: pedestrians, copperfield, racers, trainee, occasion, pinning, showcases, informa,\n",
      "\tNearest to that: skull, ride, addressing, bern, limerick, epithelial, arms, publisher,\n",
      "\tNearest to also: hillbillies, repudiation, ket, ecoregion, herder, screening, melatonin, warner,\n",
      "\tNearest to about: conductivity, sand, list, transferable, insulating, fled, selects, amputees,\n",
      "\tNearest to some: tissues, counsel, nucleus, zubrin, csm, godf, mistresses, mats,\n",
      "\tNearest to over: coo, limbaugh, nintendo, daugava, popularization, stipend, banknote, vita,\n",
      "step 2000\t16s\tAverage loss: 4.359880\n",
      "step 4000\t31s\tAverage loss: 3.864348\n",
      "step 6000\t45s\tAverage loss: 3.786084\n",
      "step 8000\t60s\tAverage loss: 3.687193\n",
      "step 10000\t75s\tAverage loss: 3.610811\n",
      "\tNearest to zero: nine, seven, six, three, eight, five, four, two,\n",
      "\tNearest to will: would, may, weights, can, to, memex, mascarenes, olivia,\n",
      "\tNearest to so: fell, flasks, qualifying, arminians, owl, had, essendon, frontage,\n",
      "\tNearest to can: would, may, could, donuts, will, botc, might, should,\n",
      "\tNearest to are: were, is, have, was, be, hammond, parkes, monarchical,\n",
      "\tNearest to would: will, can, could, morphemes, spitfire, but, sanctioned, ranch,\n",
      "\tNearest to time: monotheist, presses, contradicted, treating, mtp, alexandria, cura, lycus,\n",
      "\tNearest to no: it, beast, sars, processing, semites, still, illyrians, sympathy,\n",
      "\tNearest to people: proverbs, nitrous, hyphens, stagnated, manhood, clips, monde, cycling,\n",
      "\tNearest to united: amplifier, directx, geer, impressed, argumentum, beaten, implantation, armies,\n",
      "\tNearest to in: on, at, by, from, of, with, degeneration, after,\n",
      "\tNearest to that: which, he, exile, concealing, domesticated, rust, this, they,\n",
      "\tNearest to also: which, generally, it, there, often, ket, gilt, zulu,\n",
      "\tNearest to about: insulating, transferable, list, jasper, tagus, selects, sand, conductivity,\n",
      "\tNearest to some: many, mats, most, owens, the, glc, blinding, belichick,\n",
      "\tNearest to over: coo, nagano, manipulation, trasimene, impasse, moles, stipend, suit,\n",
      "step 12000\t91s\tAverage loss: 3.605172\n",
      "step 14000\t106s\tAverage loss: 3.573993\n",
      "step 16000\t120s\tAverage loss: 3.412768\n",
      "step 18000\t135s\tAverage loss: 3.454906\n",
      "step 20000\t150s\tAverage loss: 3.540054\n",
      "\tNearest to zero: five, three, six, seven, eight, four, nine, two,\n",
      "\tNearest to will: would, may, can, could, to, should, weights, memex,\n",
      "\tNearest to so: arminians, fell, flasks, thucydides, uniformitarianism, she, rupees, usually,\n",
      "\tNearest to can: would, may, could, will, should, might, donuts, to,\n",
      "\tNearest to are: were, is, have, hammond, explication, was, reforms, unlawful,\n",
      "\tNearest to would: will, can, could, may, should, must, might, but,\n",
      "\tNearest to time: monotheist, contradicted, year, statement, emulators, mtp, treating, insisting,\n",
      "\tNearest to no: beast, it, malfunctions, semites, still, temp, processing, broader,\n",
      "\tNearest to people: areas, nitrous, clips, monde, stagnated, proverbs, blondes, harsh,\n",
      "\tNearest to united: amplifier, geer, directx, impressed, armies, pentecost, clef, implantation,\n",
      "\tNearest to in: at, on, from, within, with, affords, halas, between,\n",
      "\tNearest to that: which, however, nfl, this, plantarum, when, domesticated, batter,\n",
      "\tNearest to also: generally, which, now, often, not, then, there, who,\n",
      "\tNearest to about: transferable, selects, jasper, insulating, tagus, exclaims, confocal, sucrose,\n",
      "\tNearest to some: many, these, several, all, mats, most, their, dimethyl,\n",
      "\tNearest to over: nagano, moles, dissimilar, impasse, suit, manipulation, stipend, coo,\n",
      "step 22000\t164s\tAverage loss: 3.502928\n",
      "step 24000\t181s\tAverage loss: 3.490411\n",
      "step 26000\t195s\tAverage loss: 3.482552\n",
      "step 28000\t209s\tAverage loss: 3.480664\n",
      "step 30000\t223s\tAverage loss: 3.503596\n",
      "\tNearest to zero: five, seven, eight, four, six, three, nine, two,\n",
      "\tNearest to will: would, can, may, could, should, must, weights, memex,\n",
      "\tNearest to so: arminians, fell, thucydides, she, lasker, nevado, usually, qualifying,\n",
      "\tNearest to can: may, would, could, will, should, might, must, cannot,\n",
      "\tNearest to are: were, is, have, parkes, husbandry, be, these, include,\n",
      "\tNearest to would: could, will, can, may, should, must, might, does,\n",
      "\tNearest to time: year, statement, contradicted, emulators, monotheist, treating, value, anticipated,\n",
      "\tNearest to no: temp, still, a, beast, bugatti, malfunctions, any, balochistan,\n",
      "\tNearest to people: areas, harsh, proverbs, blondes, women, builds, blinding, monde,\n",
      "\tNearest to united: amplifier, armies, geer, directx, impressed, clef, lute, observational,\n",
      "\tNearest to in: on, at, during, of, within, with, under, upon,\n",
      "\tNearest to that: which, however, symbolising, but, this, nasrallah, what, gog,\n",
      "\tNearest to also: often, now, generally, which, sometimes, it, never, there,\n",
      "\tNearest to about: selects, transferable, regarding, insulating, tagus, confocal, jasper, cayuga,\n",
      "\tNearest to some: many, these, several, all, most, those, their, succeed,\n",
      "\tNearest to over: impasse, nagano, suit, dissimilar, moles, pandit, lattices, cop,\n",
      "step 32000\t238s\tAverage loss: 3.503119\n",
      "step 34000\t252s\tAverage loss: 3.495793\n",
      "step 36000\t266s\tAverage loss: 3.456593\n",
      "step 38000\t280s\tAverage loss: 3.301183\n",
      "step 40000\t294s\tAverage loss: 3.425235\n",
      "\tNearest to zero: five, seven, nine, eight, six, three, four, two,\n",
      "\tNearest to will: would, can, could, may, must, should, might, cannot,\n",
      "\tNearest to so: arminians, modalities, fell, nevado, thucydides, uniformitarianism, she, lasker,\n",
      "\tNearest to can: may, will, could, would, must, should, might, cannot,\n",
      "\tNearest to are: were, have, is, be, parkes, husbandry, although, these,\n",
      "\tNearest to would: could, will, can, may, might, must, should, does,\n",
      "\tNearest to time: year, way, statement, contradicted, emulators, refering, treating, anticipated,\n",
      "\tNearest to no: any, mach, contending, beast, informer, still, bugatti, objector,\n",
      "\tNearest to people: proverbs, areas, men, women, manhood, others, teased, aon,\n",
      "\tNearest to united: amplifier, geer, armies, argumentum, observational, teddy, impressed, directx,\n",
      "\tNearest to in: on, within, nicomachean, and, reels, during, at, from,\n",
      "\tNearest to that: which, this, what, however, because, symbolising, plantarum, who,\n",
      "\tNearest to also: often, which, generally, now, still, usually, never, sometimes,\n",
      "\tNearest to about: selects, actions, regarding, cayuga, confocal, rabbits, ausgleich, polymorphic,\n",
      "\tNearest to some: many, these, several, any, most, both, each, those,\n",
      "\tNearest to over: dissimilar, moles, suit, cigars, stipend, nagano, rowspan, impasse,\n",
      "step 42000\t308s\tAverage loss: 3.436842\n",
      "step 44000\t322s\tAverage loss: 3.454909\n",
      "step 46000\t336s\tAverage loss: 3.454862\n",
      "step 48000\t350s\tAverage loss: 3.349537\n",
      "step 50000\t364s\tAverage loss: 3.386717\n",
      "\tNearest to zero: seven, five, eight, nine, six, four, three, two,\n",
      "\tNearest to will: would, could, can, may, must, should, might, cannot,\n",
      "\tNearest to so: then, beyond, appreciation, uniformitarianism, if, fell, cytoplasmic, thucydides,\n",
      "\tNearest to can: may, could, would, will, should, must, might, cannot,\n",
      "\tNearest to are: were, is, have, parkes, these, was, be, husbandry,\n",
      "\tNearest to would: could, will, can, may, might, must, should, does,\n",
      "\tNearest to time: year, way, refering, emulators, purpose, statement, contradicted, value,\n",
      "\tNearest to no: any, calorie, pcb, jeroboam, temp, objector, another, already,\n",
      "\tNearest to people: men, jews, women, areas, proverbs, trojans, bc, subjects,\n",
      "\tNearest to united: tanakh, western, armies, amplifier, southern, geer, observational, teddy,\n",
      "\tNearest to in: during, of, within, on, from, throughout, halas, until,\n",
      "\tNearest to that: which, however, nasrallah, acf, theological, what, gog, nominates,\n",
      "\tNearest to also: often, which, now, still, generally, who, usually, never,\n",
      "\tNearest to about: selects, rabbits, regarding, actions, nonsensical, how, exclaims, nearly,\n",
      "\tNearest to some: many, several, these, most, any, both, all, those,\n",
      "\tNearest to over: dissimilar, suit, into, lattices, stipend, channel, nagano, beyond,\n",
      "step 52000\t378s\tAverage loss: 3.433078\n",
      "step 54000\t392s\tAverage loss: 3.426346\n",
      "step 56000\t406s\tAverage loss: 3.442148\n",
      "step 58000\t420s\tAverage loss: 3.399953\n",
      "step 60000\t433s\tAverage loss: 3.389374\n",
      "\tNearest to zero: five, seven, eight, four, six, nine, three, two,\n",
      "\tNearest to will: would, can, could, may, must, should, might, cannot,\n",
      "\tNearest to so: then, beyond, if, fell, enough, arminians, thucydides, even,\n",
      "\tNearest to can: may, would, could, will, might, must, cannot, should,\n",
      "\tNearest to are: were, is, have, these, although, include, outtakes, those,\n",
      "\tNearest to would: could, will, can, may, might, must, should, cannot,\n",
      "\tNearest to time: way, year, refering, purpose, season, value, emulators, contradicted,\n",
      "\tNearest to no: any, already, informer, dependency, mach, subscriptions, calorie, a,\n",
      "\tNearest to people: men, women, areas, others, children, omniscient, proverbs, players,\n",
      "\tNearest to united: constitution, southern, tanakh, confederate, argumentum, armies, aeacus, geer,\n",
      "\tNearest to in: within, during, of, between, at, until, reels, since,\n",
      "\tNearest to that: which, what, however, this, nasrallah, plantarum, sarasota, often,\n",
      "\tNearest to also: now, often, still, generally, usually, traditionally, never, below,\n",
      "\tNearest to about: selects, regarding, rabbits, how, nearly, exclaims, hksar, nonsensical,\n",
      "\tNearest to some: many, several, these, most, any, those, all, each,\n",
      "\tNearest to over: around, dissimilar, into, colts, pandit, nagano, beyond, apostates,\n",
      "step 62000\t448s\tAverage loss: 3.242861\n",
      "step 64000\t462s\tAverage loss: 3.255224\n",
      "step 66000\t475s\tAverage loss: 3.403763\n",
      "step 68000\t489s\tAverage loss: 3.394123\n",
      "step 70000\t503s\tAverage loss: 3.358839\n",
      "\tNearest to zero: five, eight, four, six, seven, nine, three, two,\n",
      "\tNearest to will: would, can, may, could, must, might, should, cannot,\n",
      "\tNearest to so: then, enough, if, beyond, fell, too, thus, thucydides,\n",
      "\tNearest to can: may, could, would, will, might, must, cannot, should,\n",
      "\tNearest to are: were, have, is, these, include, outtakes, including, be,\n",
      "\tNearest to would: will, could, can, may, might, must, should, does,\n",
      "\tNearest to time: year, season, way, period, emulators, refering, purpose, contradicted,\n",
      "\tNearest to no: any, already, konkan, vera, mach, little, clearly, always,\n",
      "\tNearest to people: men, women, children, others, those, players, writers, historians,\n",
      "\tNearest to united: confederate, tanakh, same, constitution, amplifier, negus, printing, directx,\n",
      "\tNearest to in: within, during, on, until, halas, between, among, for,\n",
      "\tNearest to that: which, however, but, what, this, nasrallah, often, generally,\n",
      "\tNearest to also: still, now, which, often, traditionally, usually, below, generally,\n",
      "\tNearest to about: regarding, selects, rabbits, over, nonsensical, nearly, hksar, permanganate,\n",
      "\tNearest to some: many, several, these, most, all, both, any, those,\n",
      "\tNearest to over: around, beyond, about, dissimilar, colts, pandit, lattices, suit,\n",
      "step 72000\t517s\tAverage loss: 3.370857\n",
      "step 74000\t531s\tAverage loss: 3.343729\n",
      "step 76000\t545s\tAverage loss: 3.315984\n",
      "step 78000\t559s\tAverage loss: 3.355344\n",
      "step 80000\t573s\tAverage loss: 3.377114\n",
      "\tNearest to zero: seven, five, eight, six, nine, four, three, u,\n",
      "\tNearest to will: would, could, can, may, must, might, should, cannot,\n",
      "\tNearest to so: too, fell, beyond, then, appreciation, appreciated, thucydides, watchmen,\n",
      "\tNearest to can: could, would, will, may, cannot, must, might, should,\n",
      "\tNearest to are: were, include, have, is, although, contain, these, parkes,\n",
      "\tNearest to would: will, could, can, may, might, must, should, cannot,\n",
      "\tNearest to time: year, emulators, season, refering, years, purpose, unbeatable, contradicted,\n",
      "\tNearest to no: any, vera, little, already, clapping, calorie, lineker, it,\n",
      "\tNearest to people: men, women, children, students, players, those, historians, others,\n",
      "\tNearest to united: confederate, tanakh, constitution, amplifier, same, suffragan, census, serrano,\n",
      "\tNearest to in: during, on, until, within, at, of, between, despite,\n",
      "\tNearest to that: which, however, this, nasrallah, what, why, rust, arameans,\n",
      "\tNearest to also: still, now, often, which, sometimes, traditionally, usually, hco,\n",
      "\tNearest to about: regarding, selects, rabbits, over, nonsensical, hksar, sabah, permanganate,\n",
      "\tNearest to some: many, several, most, various, all, any, these, both,\n",
      "\tNearest to over: around, about, beyond, within, dissimilar, colts, nearly, into,\n",
      "step 82000\t587s\tAverage loss: 3.407618\n",
      "step 84000\t602s\tAverage loss: 3.410363\n",
      "step 86000\t617s\tAverage loss: 3.385879\n",
      "step 88000\t629s\tAverage loss: 3.350869\n",
      "step 90000\t639s\tAverage loss: 3.365812\n",
      "\tNearest to zero: seven, five, eight, six, four, nine, three, u,\n",
      "\tNearest to will: would, could, can, may, might, must, should, cannot,\n",
      "\tNearest to so: too, fell, appreciated, beyond, thucydides, cytoplasmic, arminians, if,\n",
      "\tNearest to can: may, could, would, will, might, cannot, should, must,\n",
      "\tNearest to are: were, is, include, have, contain, these, although, those,\n",
      "\tNearest to would: could, will, might, can, may, should, must, cannot,\n",
      "\tNearest to time: year, emulators, season, refering, premature, contradicted, purpose, stage,\n",
      "\tNearest to no: any, little, always, only, already, another, pcb, lineker,\n",
      "\tNearest to people: men, women, children, players, writers, those, students, subjects,\n",
      "\tNearest to united: confederate, constitution, same, sovereign, tanakh, suffragan, procure, negus,\n",
      "\tNearest to in: within, during, between, of, with, at, under, until,\n",
      "\tNearest to that: which, however, what, plantarum, why, but, concealing, mcclelland,\n",
      "\tNearest to also: now, still, often, which, traditionally, usually, generally, historically,\n",
      "\tNearest to about: regarding, selects, rabbits, permanganate, following, nonsensical, over, vacancy,\n",
      "\tNearest to some: many, several, these, any, most, all, this, various,\n",
      "\tNearest to over: around, within, nearly, through, lattices, moles, beyond, dissimilar,\n",
      "step 92000\t649s\tAverage loss: 3.396895\n",
      "step 94000\t659s\tAverage loss: 3.249053\n",
      "step 96000\t668s\tAverage loss: 3.359697\n",
      "step 98000\t679s\tAverage loss: 3.239808\n",
      "step 100000\t689s\tAverage loss: 3.357216\n",
      "\tNearest to zero: five, six, seven, eight, four, nine, three, u,\n",
      "\tNearest to will: would, could, can, may, must, should, might, cannot,\n",
      "\tNearest to so: then, too, thus, beyond, criticizing, therefore, lasker, appreciated,\n",
      "\tNearest to can: may, could, will, would, should, cannot, must, might,\n",
      "\tNearest to are: were, have, include, although, is, contain, these, be,\n",
      "\tNearest to would: could, will, might, may, can, should, must, cannot,\n",
      "\tNearest to time: season, year, refering, emulators, periods, contradicted, way, premature,\n",
      "\tNearest to no: any, little, lineker, another, distinction, vera, clapping, calorie,\n",
      "\tNearest to people: men, women, children, players, students, writers, historians, someone,\n",
      "\tNearest to united: confederate, netherlands, tanakh, sovereign, negus, suffragan, constitution, southern,\n",
      "\tNearest to in: during, within, on, until, at, from, halas, under,\n",
      "\tNearest to that: which, however, what, why, but, sketchy, nominates, plantarum,\n",
      "\tNearest to also: still, now, often, traditionally, below, never, usually, sometimes,\n",
      "\tNearest to about: regarding, rabbits, selects, following, nonsensical, hksar, middleweight, permanganate,\n",
      "\tNearest to some: many, several, these, any, various, those, all, numerous,\n",
      "\tNearest to over: within, around, nearly, moles, splits, beyond, dei, dissimilar,\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# EJECUCION SKIP-GRAM\n",
    "##################\n",
    "\n",
    "num_steps = 100001\n",
    "loss_report_interval = 2000\n",
    "similarity_report_interval = 10000\n",
    "top_k = 8 # number of nearest neighbors a mostrar en el informe de similitud\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "with tf.Session(graph=graphSG) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  average_loss = 0\n",
    "\n",
    "  # iterar\n",
    "  for step in range(num_steps):\n",
    "    batch_data, batch_labels = generate_batch_skipgram(batch_size, num_skips, skip_window)\n",
    "    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "    _, L = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    average_loss += L\n",
    "\n",
    "    # mostar la perdida media de un intervalo\n",
    "    if step % loss_report_interval == 0:\n",
    "      t2 = time.time()\n",
    "      if step > 0:\n",
    "        average_loss = average_loss / loss_report_interval\n",
    "      # The average loss is an estimate of the loss over the last loss_report_interval batches.\n",
    "      print('step %d\\t%ds\\tAverage loss: %f' % (step,t2-t1, average_loss))\n",
    "      average_loss = 0\n",
    "    \n",
    "    # mostrar la similitud alcanzada en un intervalo\n",
    "    # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if step % similarity_report_interval == 0:\n",
    "      sim = similarity.eval() #tomar la variable calculada de similitud\n",
    "      for i in range(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k+1] # del vector de probabilidades de vecindad toma los 8 mayores\n",
    "        log = '\\tNearest to %s:' % valid_word\n",
    "        for k in range(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          log = '%s %s,' % (log, close_word)\n",
    "        print(log)\n",
    "        \n",
    "  #final_n = norm.eval() #tomar el denominador de normalizacion a ver si tiene ceros\n",
    "  final_skipgram_embeddings = normalized_embeddings.eval() \n",
    "    #me quedo con las features finales que definen cada concepto de palabra\n",
    "  print('End.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Vamos a dibujar usando herramienta de TensorFlow: \n",
    "[TNSE] (https://www.tensorflow.org/tutorials/word2vec/?authuser=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 128)\n",
      "[-0.01716124 -0.20696527 -0.11946616 ..., -0.14512001  0.04967998\n",
      "  0.01002654]\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#Me da error de que final_embeddings tiene un Nan o un Inf y no es así\n",
    "#kk = final_n.reshape(vocabulary_size)\n",
    "#print(np.isnan(kk).any())\n",
    "#print(np.isinf(kk).any())\n",
    "print(final_skipgram_embeddings.shape)\n",
    "kk = final_skipgram_embeddings.reshape(vocabulary_size*embedding_size)\n",
    "print(kk)\n",
    "print(np.isnan(kk).any())\n",
    "print(np.isinf(kk).any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es un fallo de numpy o de sci-kit:\n",
    "    (https://github.com/amygdala/tensorflow-workshop/issues/12)\n",
    "TAMBIEN SE AREGLA USANDO MAS MUESTRAS PARA DIBUJAR, PERO LUEGO HAY QUE TOMAR UN SUBRANGO O EL GRAFICO QUEDA COMO UNA CACA, VAYA TELA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "jjJXYA_XzV79"
   },
   "outputs": [],
   "source": [
    "#num_points = 400\n",
    "num_points = 3000 #por el bug\n",
    "tsneSG = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "two_d_embeddings = tsneSG.fit_transform(final_skipgram_embeddings[1:num_points+1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 4763,
     "status": "ok",
     "timestamp": 1445965465525,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2f1ffade4c9f20de",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "o_e0D_UezcDe",
    "outputId": "df22e4a5-e8ec-4e5e-d384-c6cf37c68c34"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA24AAAN5CAYAAABwgmhEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121a40710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot(embeddings, labels):\n",
    "  assert embeddings.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "  pylab.figure(figsize=(15,15))  # in inches\n",
    "  for i, label in enumerate(labels):\n",
    "    x, y = embeddings[i,:]\n",
    "    pylab.scatter(x, y)\n",
    "    pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',\n",
    "                   ha='right', va='bottom')\n",
    "  pylab.show()\n",
    "\n",
    "\n",
    "#Vamos a dibujar un rango\n",
    "\n",
    "#plot_desde = 1\n",
    "plot_desde = 1000 #debe ser mayor o igual que 1\n",
    "#num_plot = 400\n",
    "num_plot = 10\n",
    "\n",
    "assert (plot_desde > 1), 'solo se puede pintar del 1 en adelante'\n",
    "assert (plot_desde+num_plot <= num_points), 'no se puede pintar más de lo calculado'\n",
    "\n",
    "words = [reverse_dictionary[i] for i in range(plot_desde, plot_desde+num_plot)]\n",
    "embeddings_subset = two_d_embeddings[plot_desde:plot_desde+num_plot, :]\n",
    "plot(embeddings_subset, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QB5EFrBnpNnc"
   },
   "source": [
    "---\n",
    "\n",
    "Problem\n",
    "-------\n",
    "\n",
    "An alternative to skip-gram is another Word2Vec model called [CBOW](http://arxiv.org/abs/1301.3781) (Continuous Bag of Words). In the CBOW model, instead of predicting a context word from a word vector, you predict a word from the sum of all the word vectors in its context. Implement and evaluate a CBOW model trained on the text8 dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###### NOTA: me apoyo en su generate_batch de base pero quizás debería hacerme el mío que en lugar de poner como objetivo la palabra de en medio ponga la del final de cada ventana (aunque en su artículo dicen que quieren buscar el de enmedio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the']\n",
      "\n",
      "with num_skips = 2 and skip_window = 1:\n",
      "    IDs:\n",
      "['as', 'anarchism']\n",
      "['a', 'originated']\n",
      "['as', 'term']\n",
      "['of', 'a']\n",
      "['abuse', 'term']\n",
      "['of', 'first']\n",
      "['abuse', 'used']\n",
      "['against', 'first']\n",
      "    IDlabels: ['originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used']\n",
      "\n",
      "with num_skips = 4 and skip_window = 2:\n",
      "    IDs:\n",
      "['a', 'term', 'anarchism', 'originated']\n",
      "['as', 'originated', 'term', 'of']\n",
      "['as', 'of', 'a', 'abuse']\n",
      "['abuse', 'a', 'first', 'term']\n",
      "['used', 'first', 'term', 'of']\n",
      "['abuse', 'of', 'against', 'used']\n",
      "['early', 'against', 'abuse', 'first']\n",
      "['used', 'working', 'early', 'first']\n",
      "    IDlabels: ['as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "#batch_size, tamaño del lote a meter en la red neural para aprender\n",
    "#skip_window, tamaño del lado derecho e izquierdo de la ventana alrededor de una palabra (es simétrica)\n",
    "#num_skips, distancia de palabra adyacente que va a tomar como etiqueta  (2 x skip_window)\n",
    "\n",
    "def generate_batch_cbow(batch_size, num_skips, skip_window):\n",
    "    ids, lids = generate_batch_skipgram(num_skips*batch_size, num_skips, skip_window)\n",
    "    batch = lids.reshape(batch_size,num_skips)\n",
    "    labels = ids[np.arange(0,num_skips*batch_size,num_skips)].reshape(batch_size,1)\n",
    "\n",
    "    return batch, labels\n",
    "    \n",
    "#Ejecucion de ejemplo para las primeras ocho palabras del diccionario (un lote de ocho)\n",
    "\n",
    "lote = 8\n",
    "print('data:', [reverse_dictionary[di] for di in data[:2*lote]])\n",
    "\n",
    "for num_skips, skip_window in [(2, 1), (4, 2)]:\n",
    "    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n",
    "\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch_cbow(batch_size=lote, num_skips=num_skips, skip_window=skip_window)\n",
    "    \n",
    "    def show2D(a):\n",
    "        for L in a:\n",
    "            print([reverse_dictionary[i] for i in L])\n",
    "    \n",
    "    print('    IDs:')\n",
    "    show2D(batch)\n",
    "    print('    IDlabels:', [reverse_dictionary[li] for li in labels.reshape(lote)])   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "8pQKsV4Vwlzy"
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# DECLARACION CBOW\n",
    "##################\n",
    "\n",
    "batch_size = 128 #Lotes de entrenamiento.\n",
    "\n",
    "embedding_size = 128 # Dimension of the embedding vector. Las features de cada palabra que la distinguen.\n",
    "skip_window = 1 # How many words to consider left and right. Semiventana, al final ventana de 3.\n",
    "                # Es poco debería ser mayor pero más procesamiento también.\n",
    "num_skips = 2 # How many times to reuse an input to generate a label. \n",
    "                #Normalmente ancho-ventana - 1 (3-1 = 2). O 2*skip_window\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by construction are also the most frequent. \n",
    "# Va a utilizar como validationSet las palabras más comunes del diccionario (las del principio)\n",
    "valid_size = 16 # Random set of words to evaluate similarity on. Escoge 16 para validacion.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution. Las escoge de entre las 100 primeras.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "\n",
    "num_sampled = 64 # Number of negative examples to sample. Muestras aleatorias de entre el lote que sirven para calcular LOSS.\n",
    "\n",
    "learning_rate = 1.0\n",
    "\n",
    "graphCB = tf.Graph()\n",
    "\n",
    "with graphCB.as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "  # Input data.\n",
    "  train_dataset = tf.placeholder(tf.int32, shape=[batch_size, num_skips])\n",
    "  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "  \n",
    "  # Variables y su inicializacion.\n",
    "      #RELACION CON OTROS CONCEPTOS (primer layer):\n",
    "  softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],stddev=1.0 / math.sqrt(embedding_size)))\n",
    "  softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "       #CONCEPTO-FEATURES DE UNA PALABRA (segundo layer):\n",
    "  embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)) \n",
    "    \n",
    "  # Model.\n",
    "  # Look up embeddings for outputs.\n",
    "  embed = tf.zeros([batch_size, embedding_size])\n",
    "  for j in range(num_skips): #El embedding objetivo es la suma de los embeddings de la bolsa de palabras correspondiente\n",
    "        embed += tf.nn.embedding_lookup(embeddings, train_dataset[:, j])\n",
    "\n",
    "  # Compute the softmax loss, using a sample of the negative train data each time.\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed, train_labels, num_sampled, vocabulary_size)) \n",
    "        \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss)\n",
    "  \n",
    "  # Compute the similarity between minibatch examples and all embeddings.\n",
    "  # We use the cosine distance:\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "  similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "step 0\t0s\tAverage loss: 8.415958\n",
      "\tNearest to when: farewell, vices, allegories, satie, retrieve, hatcher, pacino, fawlty,\n",
      "\tNearest to a: livingston, xxvii, pencils, parnassus, halfwidth, amraam, balloons, humble,\n",
      "\tNearest to first: bragi, dmitry, brigid, traditionalists, enumerated, underwent, cues, indulgences,\n",
      "\tNearest to time: shtml, hogeschool, loudspeakers, rterbuch, newell, lio, czechoslovakian, bowman,\n",
      "\tNearest to american: sag, archimedes, methylene, hutchison, boles, recumbent, secrecy, achievable,\n",
      "\tNearest to about: actions, abm, hale, terrence, data, asphyxia, airline, inscriptions,\n",
      "\tNearest to more: balked, assumptions, overwrite, comiskey, ruskin, voss, wuhan, simplex,\n",
      "\tNearest to its: arin, asw, complexes, pasta, enigma, nitroglycerin, juneau, ex,\n",
      "\tNearest to from: biographers, cesium, proportionate, nucleotide, forcefully, bridged, orwell, erasure,\n",
      "\tNearest to many: depths, hairpin, humorist, longest, kov, subconscious, taps, bo,\n",
      "\tNearest to by: judges, presbyter, ghanima, swung, pure, tee, enjoined, mortgages,\n",
      "\tNearest to these: fingered, roundel, dissociation, faction, isosceles, subverting, steyr, porsche,\n",
      "\tNearest to some: consist, masculism, wuxia, halftime, wiens, watercolors, toga, patronymic,\n",
      "\tNearest to had: compatible, woogie, austronesian, computationally, yo, homeworld, minbari, worshipping,\n",
      "\tNearest to and: thoroughfare, circumnavigated, beneath, disputes, renouncing, freely, hammered, crusoe,\n",
      "\tNearest to two: keyboard, hyena, perceptive, ranked, slip, revolution, alleles, ogle,\n",
      "step 2000\t12s\tAverage loss: 4.094018\n",
      "step 4000\t24s\tAverage loss: 3.564202\n",
      "step 6000\t36s\tAverage loss: 3.396880\n",
      "step 8000\t47s\tAverage loss: 3.252656\n",
      "step 10000\t59s\tAverage loss: 3.177150\n",
      "\tNearest to when: before, arctan, after, where, vices, woolen, that, clout,\n",
      "\tNearest to a: contestants, hbp, megabytes, fia, scarred, ira, appleseed, the,\n",
      "\tNearest to first: bragi, second, cocteau, last, genuineness, grohl, elin, attendance,\n",
      "\tNearest to time: shtml, loudspeakers, czechoslovakian, abitibi, ireann, resorting, biography, limit,\n",
      "\tNearest to american: shiraz, outline, german, cordless, jovian, lighted, retarded, awk,\n",
      "\tNearest to about: erv, inscriptions, data, daily, knitted, hyperthyroidism, synchronized, abm,\n",
      "\tNearest to more: less, most, overwrite, very, unincorporated, balked, mulberry, secret,\n",
      "\tNearest to its: their, his, the, stratocaster, complexes, castillo, semiconducting, untenable,\n",
      "\tNearest to from: in, through, erasure, lamb, activision, cathar, discoid, canonically,\n",
      "\tNearest to many: some, several, other, these, all, qh, multithreading, ramming,\n",
      "\tNearest to by: for, strange, into, jacobs, expressway, be, harpers, cuar,\n",
      "\tNearest to these: bahamian, some, many, addison, several, such, leyte, fingered,\n",
      "\tNearest to some: many, these, several, their, nisibis, pie, endosperm, most,\n",
      "\tNearest to had: has, have, were, was, standardisation, having, gave, papandreou,\n",
      "\tNearest to and: or, thoroughfare, rence, dorsal, pulleys, kettle, benefitting, samaria,\n",
      "\tNearest to two: eight, four, three, seven, five, six, nine, zero,\n",
      "step 12000\t71s\tAverage loss: 3.200219\n",
      "step 14000\t83s\tAverage loss: 3.156423\n",
      "step 16000\t94s\tAverage loss: 3.179104\n",
      "step 18000\t106s\tAverage loss: 3.124704\n",
      "step 20000\t117s\tAverage loss: 2.987976\n",
      "\tNearest to when: before, if, after, where, with, arctan, while, woolen,\n",
      "\tNearest to a: the, another, his, scarred, no, any, olney, danneskj,\n",
      "\tNearest to first: second, last, next, original, cocteau, attendance, luk, bragi,\n",
      "\tNearest to time: limit, year, shtml, alto, czechoslovakian, loudspeakers, day, astonished,\n",
      "\tNearest to american: australian, african, english, jovian, canadian, retrieving, buteo, vivekananda,\n",
      "\tNearest to about: erv, idealists, uphill, dtds, aasen, arndt, inscriptions, outwardly,\n",
      "\tNearest to more: less, most, very, margraves, forall, rather, billet, fait,\n",
      "\tNearest to its: their, his, the, volunteered, her, battuta, slovakian, verified,\n",
      "\tNearest to from: through, into, erasure, discoid, hydroelectric, pitot, canonically, dtds,\n",
      "\tNearest to many: some, several, these, all, hormone, most, such, historicism,\n",
      "\tNearest to by: through, concave, was, when, with, andes, cuar, adrift,\n",
      "\tNearest to these: some, many, such, they, several, those, there, ennedi,\n",
      "\tNearest to some: many, several, these, most, any, their, all, pie,\n",
      "\tNearest to had: has, have, was, having, standardisation, were, began, enthusiast,\n",
      "\tNearest to and: or, but, newsmagazine, inundated, miscellaneous, sweeping, like, while,\n",
      "\tNearest to two: seven, three, four, six, one, five, eight, nine,\n",
      "step 22000\t129s\tAverage loss: 3.075697\n",
      "step 24000\t141s\tAverage loss: 3.034760\n",
      "step 26000\t153s\tAverage loss: 3.002777\n",
      "step 28000\t164s\tAverage loss: 3.034107\n",
      "step 30000\t176s\tAverage loss: 3.009036\n",
      "\tNearest to when: before, if, after, where, while, since, although, during,\n",
      "\tNearest to a: the, scarred, cautiously, jest, any, capturing, unter, another,\n",
      "\tNearest to first: last, second, next, original, latter, cocteau, genuineness, attendance,\n",
      "\tNearest to time: year, day, hurdles, tewahedo, limit, astonished, alto, iso,\n",
      "\tNearest to american: australian, african, retrieving, canadian, jovian, buteo, british, german,\n",
      "\tNearest to about: erv, aasen, uphill, idealists, regarding, outwardly, petropavlovsk, bluntly,\n",
      "\tNearest to more: less, most, rather, very, too, greater, billet, fairly,\n",
      "\tNearest to its: their, his, her, the, volunteered, sargent, banisteriopsis, plasticity,\n",
      "\tNearest to from: into, through, erasure, discoid, dtds, in, activision, on,\n",
      "\tNearest to many: some, several, these, various, all, few, those, such,\n",
      "\tNearest to by: through, under, andes, had, alludes, excommunicate, in, dedekind,\n",
      "\tNearest to these: such, many, some, those, several, both, various, ennedi,\n",
      "\tNearest to some: many, several, these, most, any, all, certain, both,\n",
      "\tNearest to had: has, have, was, were, having, gave, standardisation, never,\n",
      "\tNearest to and: or, but, quinine, bullies, leetspeak, however, wexford, including,\n",
      "\tNearest to two: three, four, five, six, seven, one, nine, eight,\n",
      "step 32000\t188s\tAverage loss: 2.829884\n",
      "step 34000\t199s\tAverage loss: 2.955585\n",
      "step 36000\t211s\tAverage loss: 2.952324\n",
      "step 38000\t223s\tAverage loss: 2.942416\n",
      "step 40000\t234s\tAverage loss: 2.936959\n",
      "\tNearest to when: before, if, after, where, during, while, though, by,\n",
      "\tNearest to a: another, the, scarred, hbp, every, olney, verizon, any,\n",
      "\tNearest to first: last, second, next, original, best, final, same, latter,\n",
      "\tNearest to time: year, astonished, day, completeness, alto, position, limit, prohibiting,\n",
      "\tNearest to american: australian, african, british, german, indian, canadian, billed, buteo,\n",
      "\tNearest to about: regarding, on, aasen, idealists, around, petropavlovsk, erv, outwardly,\n",
      "\tNearest to more: less, most, very, rather, too, fairly, better, larger,\n",
      "\tNearest to its: their, his, her, the, verified, my, jscript, whose,\n",
      "\tNearest to from: through, into, erasure, within, discoid, during, in, between,\n",
      "\tNearest to many: some, several, various, most, these, both, qh, tonality,\n",
      "\tNearest to by: through, when, kees, symbolize, andes, delegate, redirect, using,\n",
      "\tNearest to these: several, those, some, such, various, many, which, both,\n",
      "\tNearest to some: many, several, these, most, various, any, all, certain,\n",
      "\tNearest to had: have, has, was, were, having, standardisation, gave, never,\n",
      "\tNearest to and: or, but, while, sustainable, hindemith, iea, kare, newsmagazine,\n",
      "\tNearest to two: three, five, six, four, seven, eight, zero, nine,\n",
      "step 42000\t246s\tAverage loss: 2.949265\n",
      "step 44000\t258s\tAverage loss: 2.959801\n",
      "step 46000\t270s\tAverage loss: 2.912480\n",
      "step 48000\t281s\tAverage loss: 2.871125\n",
      "step 50000\t293s\tAverage loss: 2.847094\n",
      "\tNearest to when: before, if, after, during, while, where, although, since,\n",
      "\tNearest to a: scarred, hbp, the, another, any, every, contestants, cautiously,\n",
      "\tNearest to first: last, next, second, original, final, best, cocteau, latter,\n",
      "\tNearest to time: year, completeness, astonished, prohibiting, diagram, inspect, surges, kinds,\n",
      "\tNearest to american: british, german, australian, billed, canadian, buteo, outline, shiraz,\n",
      "\tNearest to about: around, regarding, petropavlovsk, uphill, dispossessed, over, inconclusive, ticonderoga,\n",
      "\tNearest to more: less, most, very, fairly, rather, greater, too, smaller,\n",
      "\tNearest to its: their, his, her, the, my, our, fallen, whose,\n",
      "\tNearest to from: into, through, in, within, discoid, erasure, during, dispossessed,\n",
      "\tNearest to many: several, some, these, various, most, all, few, such,\n",
      "\tNearest to by: through, worsening, using, andes, against, alludes, during, falsificationism,\n",
      "\tNearest to these: several, many, those, some, various, such, their, both,\n",
      "\tNearest to some: many, several, any, most, these, various, certain, endosperm,\n",
      "\tNearest to had: has, have, were, having, was, gave, standardisation, since,\n",
      "\tNearest to and: or, but, totals, although, sustainable, while, including, comb,\n",
      "\tNearest to two: three, four, six, five, eight, seven, one, nine,\n",
      "step 52000\t305s\tAverage loss: 2.891583\n",
      "step 54000\t316s\tAverage loss: 2.864191\n",
      "step 56000\t328s\tAverage loss: 2.861243\n",
      "step 58000\t341s\tAverage loss: 2.750916\n",
      "step 60000\t354s\tAverage loss: 2.831311\n",
      "\tNearest to when: if, before, where, during, after, while, although, since,\n",
      "\tNearest to a: another, scarred, hbp, the, laterally, meps, any, roddick,\n",
      "\tNearest to first: last, second, next, original, third, cocteau, latter, final,\n",
      "\tNearest to time: year, day, diagram, astonished, solstices, kind, completeness, longsword,\n",
      "\tNearest to american: australian, british, canadian, english, german, italian, african, buteo,\n",
      "\tNearest to about: regarding, within, over, petropavlovsk, around, bitterness, keeps, uphill,\n",
      "\tNearest to more: less, most, very, fairly, rather, greater, better, smaller,\n",
      "\tNearest to its: their, his, her, the, whose, fallen, our, your,\n",
      "\tNearest to from: through, into, during, within, by, erasure, under, after,\n",
      "\tNearest to many: several, some, various, these, few, certain, all, those,\n",
      "\tNearest to by: through, from, alludes, excommunicate, during, worsening, expressway, scientologists,\n",
      "\tNearest to these: several, such, those, many, various, ennedi, all, they,\n",
      "\tNearest to some: many, several, any, most, each, various, certain, this,\n",
      "\tNearest to had: has, have, was, having, gave, were, never, standardisation,\n",
      "\tNearest to and: or, but, although, mailboxes, totals, kare, while, where,\n",
      "\tNearest to two: three, four, six, five, one, seven, eight, zero,\n",
      "step 62000\t367s\tAverage loss: 2.830411\n",
      "step 64000\t379s\tAverage loss: 2.776165\n",
      "step 66000\t391s\tAverage loss: 2.766892\n",
      "step 68000\t404s\tAverage loss: 2.713127\n",
      "step 70000\t416s\tAverage loss: 2.788051\n",
      "\tNearest to when: if, before, after, during, while, where, with, since,\n",
      "\tNearest to a: scarred, the, another, hbp, any, imperatoribus, cautiously, every,\n",
      "\tNearest to first: last, second, next, third, latter, cocteau, genuineness, previous,\n",
      "\tNearest to time: completeness, astonished, day, tewahedo, period, year, fmri, decade,\n",
      "\tNearest to american: australian, british, canadian, african, italian, english, german, chilean,\n",
      "\tNearest to about: regarding, petropavlovsk, over, what, around, inconclusive, idealists, wiping,\n",
      "\tNearest to more: less, most, very, greater, fairly, too, highly, better,\n",
      "\tNearest to its: their, his, the, her, whose, cinderella, jscript, andros,\n",
      "\tNearest to from: through, under, into, erasure, during, within, discoid, across,\n",
      "\tNearest to many: several, some, various, numerous, all, few, these, most,\n",
      "\tNearest to by: through, without, with, under, during, when, scientologists, ltc,\n",
      "\tNearest to these: those, several, some, various, many, they, certain, both,\n",
      "\tNearest to some: many, several, any, these, certain, various, most, numerous,\n",
      "\tNearest to had: has, have, was, having, were, showed, standardisation, since,\n",
      "\tNearest to and: or, but, although, while, donate, kare, totals, sustainable,\n",
      "\tNearest to two: four, three, five, seven, six, eight, one, nine,\n",
      "step 72000\t429s\tAverage loss: 2.799764\n",
      "step 74000\t442s\tAverage loss: 2.642389\n",
      "step 76000\t455s\tAverage loss: 2.804382\n",
      "step 78000\t466s\tAverage loss: 2.823772\n",
      "step 80000\t479s\tAverage loss: 2.778724\n",
      "\tNearest to when: if, before, after, while, during, although, though, where,\n",
      "\tNearest to a: scarred, another, hbp, the, neuropsychological, cautiously, dieppe, applescript,\n",
      "\tNearest to first: last, next, second, third, earliest, final, best, latter,\n",
      "\tNearest to time: day, year, astonished, kind, period, completeness, controversy, nostrand,\n",
      "\tNearest to american: australian, british, african, italian, canadian, english, jovian, irish,\n",
      "\tNearest to about: regarding, petropavlovsk, over, around, jaynes, best, dispossessed, what,\n",
      "\tNearest to more: less, very, most, greater, fairly, quite, highly, rather,\n",
      "\tNearest to its: their, his, her, the, whose, our, jscript, byrds,\n",
      "\tNearest to from: through, across, within, into, erasure, during, in, discoid,\n",
      "\tNearest to many: several, some, various, numerous, all, most, those, few,\n",
      "\tNearest to by: through, when, during, andes, without, for, using, from,\n",
      "\tNearest to these: several, some, those, such, both, many, certain, which,\n",
      "\tNearest to some: many, several, certain, these, any, most, various, both,\n",
      "\tNearest to had: has, have, was, were, having, standardisation, showed, wanted,\n",
      "\tNearest to and: or, including, but, although, totals, of, corrie, however,\n",
      "\tNearest to two: three, four, five, eight, six, seven, one, nine,\n",
      "step 82000\t492s\tAverage loss: 2.664798\n",
      "step 84000\t505s\tAverage loss: 2.768273\n",
      "step 86000\t518s\tAverage loss: 2.734654\n",
      "step 88000\t530s\tAverage loss: 2.756487\n",
      "step 90000\t542s\tAverage loss: 2.739314\n",
      "\tNearest to when: if, although, before, after, though, while, where, during,\n",
      "\tNearest to a: another, scarred, hbp, detonates, dieppe, the, chorales, every,\n",
      "\tNearest to first: last, next, second, earliest, latter, third, fourth, genuineness,\n",
      "\tNearest to time: year, day, period, manner, problem, completeness, withered, anus,\n",
      "\tNearest to american: australian, african, british, italian, english, nur, indian, french,\n",
      "\tNearest to about: around, regarding, over, jaynes, approximately, idealists, petropavlovsk, outwardly,\n",
      "\tNearest to more: less, most, greater, very, fairly, rather, quite, better,\n",
      "\tNearest to its: their, his, the, her, whose, our, cinderella, latitude,\n",
      "\tNearest to from: through, across, including, during, erasure, into, in, via,\n",
      "\tNearest to many: some, several, numerous, various, most, few, all, both,\n",
      "\tNearest to by: through, when, including, marianas, via, using, stephens, schoolteacher,\n",
      "\tNearest to these: those, some, such, ennedi, multiple, their, several, certain,\n",
      "\tNearest to some: many, several, certain, most, various, these, any, this,\n",
      "\tNearest to had: has, have, was, were, having, standardisation, showed, wanted,\n",
      "\tNearest to and: or, including, although, drang, but, barroso, on, donate,\n",
      "\tNearest to two: three, four, six, five, seven, eight, one, zero,\n",
      "step 92000\t555s\tAverage loss: 2.678391\n",
      "step 94000\t567s\tAverage loss: 2.728046\n",
      "step 96000\t579s\tAverage loss: 2.703232\n",
      "step 98000\t592s\tAverage loss: 2.366784\n",
      "step 100000\t604s\tAverage loss: 2.403637\n",
      "\tNearest to when: if, before, after, though, although, while, where, during,\n",
      "\tNearest to a: hbp, scarred, another, the, verizon, detonates, sorties, contestants,\n",
      "\tNearest to first: last, next, second, third, fourth, latter, earliest, original,\n",
      "\tNearest to time: screams, decade, kind, manner, period, astonished, year, debate,\n",
      "\tNearest to american: australian, english, canadian, british, german, italian, swiss, scottish,\n",
      "\tNearest to about: regarding, around, over, relentlessly, jaynes, on, ticonderoga, inconclusive,\n",
      "\tNearest to more: less, most, very, rather, fairly, better, too, quite,\n",
      "\tNearest to its: their, his, her, whose, our, the, plasticity, your,\n",
      "\tNearest to from: across, through, erasure, via, bondi, discoid, for, into,\n",
      "\tNearest to many: several, some, numerous, various, few, multiple, such, all,\n",
      "\tNearest to by: through, alludes, when, using, upside, for, via, blazing,\n",
      "\tNearest to these: those, several, some, such, ennedi, certain, many, drowns,\n",
      "\tNearest to some: many, several, any, certain, these, most, this, various,\n",
      "\tNearest to had: has, have, having, was, showed, standardisation, were, never,\n",
      "\tNearest to and: or, but, including, kare, mailboxes, towards, though, hopeless,\n",
      "\tNearest to two: three, six, four, five, eight, seven, zero, nine,\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# EJECUCION CBOW\n",
    "##################\n",
    "\n",
    "num_steps = 100001\n",
    "loss_report_interval = 2000\n",
    "similarity_report_interval = 10000\n",
    "top_k = 8 # number of nearest neighbors a mostrar en el informe de similitud\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "sessCB = tf.Session(graph=graphCB)\n",
    "with sessCB as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  average_loss = 0\n",
    "\n",
    "  # iterar\n",
    "  for step in range(num_steps):\n",
    "    batch_data, batch_labels = generate_batch_cbow(batch_size, num_skips, skip_window)\n",
    "    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "    _, L = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    average_loss += L\n",
    "\n",
    "    # mostar la perdida media de un intervalo\n",
    "    if step % loss_report_interval == 0:\n",
    "      t2 = time.time()\n",
    "      if step > 0:\n",
    "        average_loss = average_loss / loss_report_interval\n",
    "      # The average loss is an estimate of the loss over the last loss_report_interval batches.\n",
    "      print('step %d\\t%ds\\tAverage loss: %f' % (step,t2-t1, average_loss))\n",
    "      average_loss = 0\n",
    "    \n",
    "    # mostrar la similitud alcanzada en un intervalo\n",
    "    # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if step % similarity_report_interval == 0:\n",
    "      sim = similarity.eval() #tomar la variable calculada de similitud\n",
    "      for i in range(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k+1] # del vector de probabilidades de vecindad toma los 8 mayores\n",
    "        log = '\\tNearest to %s:' % valid_word\n",
    "        for k in range(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          log = '%s %s,' % (log, close_word)\n",
    "        print(log)\n",
    "        \n",
    "  final_cbow_embeddings = normalized_embeddings.eval() \n",
    "        #me quedo con las features finales que definen cada concepto de palabra\n",
    "  print(\"End.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vamos a dibujar cosas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nword = 'be'\\nword_id = data[dictionary[word]]\\n\\nwith graphCB.as_default(), tf.device('/cpu:0'):\\n      norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\\n      normalized_embeddings = embeddings / norm\\n      myembedding = tf.nn.embedding_lookup(normalized_embeddings, [word_id])\\n      OneSimilarity = tf.matmul(myembedding, tf.transpose(normalized_embeddings))\\n    \\nwith sessCB as session:\\n      n = norm.eval()\\n      print(n.shape)\\n      e = myembedding.eval()\\n      print(e.shape)\\n      sim = OneSimilarity.eval()\\n      print(sim.shape)\\n      nearest = (-sim[0, :]).argsort()[1:top_k+1] # del vector de probabilidades de vecindad toma los 8 mayores\\n      log = '\\tNearest to %s:' % word\\n      for k in range(top_k):\\n        close_word = reverse_dictionary[nearest[k]]\\n        log = '%s %s,' % (log, close_word)\\n      print(log)    \\n\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Nearest to be: been, being, become, have, refer, is, ratifies, were,\n",
    "\"\"\"\n",
    "word = 'be'\n",
    "word_id = data[dictionary[word]]\n",
    "\n",
    "with graphCB.as_default(), tf.device('/cpu:0'):\n",
    "      norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "      normalized_embeddings = embeddings / norm\n",
    "      myembedding = tf.nn.embedding_lookup(normalized_embeddings, [word_id])\n",
    "      OneSimilarity = tf.matmul(myembedding, tf.transpose(normalized_embeddings))\n",
    "    \n",
    "with sessCB as session:\n",
    "      n = norm.eval()\n",
    "      print(n.shape)\n",
    "      e = myembedding.eval()\n",
    "      print(e.shape)\n",
    "      sim = OneSimilarity.eval()\n",
    "      print(sim.shape)\n",
    "      nearest = (-sim[0, :]).argsort()[1:top_k+1] # del vector de probabilidades de vecindad toma los 8 mayores\n",
    "      log = '\\tNearest to %s:' % word\n",
    "      for k in range(top_k):\n",
    "        close_word = reverse_dictionary[nearest[k]]\n",
    "        log = '%s %s,' % (log, close_word)\n",
    "      print(log)    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_nearest_embeddings_PCA(word,norm_embeddings,area=10):\n",
    "    \n",
    "    assert (word in dictionary), 'This word is not in the dictionary'\n",
    "    \n",
    "    word_id = dictionary[word]\n",
    "    print('ID for \"' + word +'\" is ' + str(word_id))\n",
    "\n",
    "    # Compute the similarity between selected word and all embeddings.\n",
    "    print('computing neighbors...')\n",
    "\n",
    "    # We use the cosine distance:\n",
    "    my_embedding = norm_embeddings[word_id]\n",
    "    S = np.matmul(my_embedding, np.transpose(norm_embeddings))\n",
    "    #np.reshape(tensor=similarity,shape=[vocabulary_size])  \n",
    "\n",
    "    words = []\n",
    "    nearest = (-S).argsort()[1:area+1] # del vector de probabilidades de vecindad toma los mayores MENOS ÉL MISMO\n",
    "    log = '\\tNearest %d to \"%s\":' % (area,word)\n",
    "    for k in range(area):\n",
    "        close_word = reverse_dictionary[nearest[k]]\n",
    "        words = words + [close_word]\n",
    "        log = '%s %s,' % (log, close_word)\n",
    "    print(log)\n",
    "    \n",
    "    #Incluyo la propia palabra para que se vea tambien en el dibujo\n",
    "    words = words + [word]\n",
    "    nearest = np.append(nearest,word_id)\n",
    "    #print(nearest)\n",
    "    #print(words)\n",
    "\n",
    "    #Funcion para pintar las proyecciones 2D\n",
    "    def plot2D(embeddings, labels):\n",
    "      assert embeddings.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "      pylab.figure(figsize=(15,15))  # in inches\n",
    "      for i, label in enumerate(labels):\n",
    "        x, y = embeddings[i,:]\n",
    "        pylab.scatter(x, y)\n",
    "        pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',ha='right', va='bottom')\n",
    "      pylab.show()\n",
    "\n",
    "    print('projecting and plotting...')\n",
    "\n",
    "    #embeddings_subset = norm_embeddings[1:301, :] # da BUG\n",
    "    #embeddings_subset = norm_embeddings[initial_word:num_points+1, :]\n",
    "    #embeddings_subset = norm_embeddings #si calculo todos REVIENTA el ordenador el TSNE\n",
    "    embeddings_subset = norm_embeddings[nearest, :] #si calculo demasiado pocos REVIENTA también TSNE :-(\n",
    "    \n",
    "    #Calcular el TSNE (NO PORQUE FALLA)\n",
    "    #px = 30\n",
    "    #px = area\n",
    "    #tsne = TSNE(perplexity=px, n_components=2, init='pca', n_iter=5000)\n",
    "    #two_d_embeddings_tsne = tsne.fit_transform(embeddings_subset)\n",
    "    \n",
    "    #aplico PCA porque si funciona\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    two_d_embeddings_pca = pca.fit_transform(embeddings_subset)\n",
    "    print(\"\\tPCA variance ratio\",pca.explained_variance_ratio_)\n",
    "    \n",
    "    #Vamos a dibujar un rango de número de elementos \"area\" alrededor de la palabra\n",
    "    plot2D(two_d_embeddings_pca, words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 128)\n",
      "ID for \"some\" is 48\n",
      "computing neighbors...\n",
      "\tNearest 8 to \"some\": many, several, these, any, various, those, all, numerous,\n",
      "projecting and plotting...\n",
      "\tPCA variance ratio [ 0.2471493   0.18742071]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3cAAANmCAYAAABZuXIoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nk6l0vCsNgwoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12048a8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#SKIPGRAM\n",
    "#Nearest to some: many, several, these, any, various, those, all, numerous,\n",
    "print(final_skipgram_embeddings.shape)\n",
    "show_nearest_embeddings_PCA('some',final_skipgram_embeddings,area=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID for \"some\" is 48\n",
      "computing neighbors...\n",
      "\tNearest 8 to \"some\": many, several, any, certain, these, most, this, various,\n",
      "projecting and plotting...\n",
      "\tPCA variance ratio [ 0.22890547  0.19436271]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3cAAANmCAYAAABZuXIoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f5db650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#CBOW\n",
    "#Nearest to some: many, several, any, certain, these, most, this, various,\n",
    "print(final_cbow_embeddings.shape)\n",
    "show_nearest_embeddings_PCA('some',final_cbow_embeddings,area=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "5_word2vec.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
