{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6 - PROBLEM 2\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "from matplotlib import pylab\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import collections\n",
    "import math\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def how_long(f, *args):\n",
    "    #medir el tiempo que tarda f\n",
    "    t1 = time.time()\n",
    "    res = f(*args)\n",
    "    t2 = time.time()\n",
    "    print (\"tiempo utilizado = \",t2-t1)\n",
    "    #return res, t2-t1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Build the bigrams dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "729\n"
     ]
    }
   ],
   "source": [
    "characters = string.ascii_lowercase + ' ' # [a-z] + ' '\n",
    "vocabulary_size = len(characters)**2 #bigrams number, every possible two characters combination\n",
    "\n",
    "#inicializa un diccionario del vocabulario, va a crear pares Bigrama - ID, para tratar con números\n",
    "#y asigna los id por orden\n",
    "dictionary = dict()\n",
    "for a in characters:\n",
    "      for b in characters:\n",
    "          dictionary[a+b] = len(dictionary)\n",
    "\n",
    "#diccionario al revés, es decir, donde el ID es la clave y la palabra es el valor\n",
    "reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "\n",
    "print(vocabulary_size)\n",
    "#print(dictionary)\n",
    "#print(reverse_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters and bigrams to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHARS TEST\n",
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n",
      "BIGRAMS TEST\n",
      "0 215 253\n",
      "aa h  jk\n"
     ]
    }
   ],
   "source": [
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print('CHARS TEST')\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))\n",
    "\n",
    "def bigram2id(bigram):\n",
    "    assert (len(bigram) == 2 ),'Bigrams must be 2 chars length'\n",
    "    return dictionary[bigram]\n",
    "\n",
    "def id2bigram(dictid):\n",
    "    assert (dictid < vocabulary_size), 'Vocabulary size exceeded' \n",
    "    return reverse_dictionary[dictid]\n",
    "\n",
    "print('BIGRAMS TEST')\n",
    "print(bigram2id('aa'),bigram2id('h '),bigram2id('jk'))\n",
    "print(id2bigram(0),id2bigram(215),id2bigram(253))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Shape input text to bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiempo utilizado =  108.908601999\n",
      " anarchism originated as a term of abuse first used against earl\n",
      "[' a' 'na' 'rc' 'hi' 'sm' ' o' 'ri' 'gi' 'na' 'te' 'd' 'as' ' a' ' t' 'er'\n",
      " 'm' 'of' ' a' 'bu' 'se' ' f' 'ir' 'st' ' u' 'se' 'd' 'ag' 'ai' 'ns' 't'\n",
      " 'ea' 'rl']\n",
      "100000000 50000000\n"
     ]
    }
   ],
   "source": [
    "def shapeBigrams(text):\n",
    "    N=len(text)\n",
    "    if (N%2 != 0):\n",
    "        N = len(text)-1\n",
    "    text = text[:N]\n",
    "    par = np.char.array(list(text[::2]))\n",
    "    impar = np.char.array(list(text[1::2]))\n",
    "    bigrams = par+impar\n",
    "    return bigrams\n",
    "\n",
    "bigrams = how_long(shapeBigrams,text)\n",
    "\n",
    "print(text[:64])\n",
    "print(bigrams[:32])\n",
    "print(len(text),len(bigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a' 'na' 'rc' 'hi' 'sm' ' o' 'ri' 'gi' 'na' 'te' 'd' 'as' ' a' ' t' 'er'\n",
      " 'm' 'of' ' a' 'bu' 'se' ' f' 'ir' 'st' ' u' 'se' 'd' 'ag' 'ai' 'ns' 't'\n",
      " 'ea' 'rl']\n",
      "[702, 351, 461, 197, 498, 716, 467, 170, 351, 517, 107, 18, 702, 721, 125, 350, 383, 702, 47, 490, 707, 233, 505, 722, 490, 107, 6, 8, 369, 539, 108, 470]\n",
      "[' a', 'na', 'rc', 'hi', 'sm', ' o', 'ri', 'gi', 'na', 'te', 'd ', 'as', ' a', ' t', 'er', 'm ', 'of', ' a', 'bu', 'se', ' f', 'ir', 'st', ' u', 'se', 'd ', 'ag', 'ai', 'ns', 't ', 'ea', 'rl']\n",
      "50000000\n",
      "50000000\n"
     ]
    }
   ],
   "source": [
    "#creamos un equivalente al al dataset \"bigrams\" reemplazando cada palabra por su ID\n",
    "data = list()\n",
    "for i,bigram in enumerate(bigrams):\n",
    "  if(len(bigram) == 1):\n",
    "        bigram = bigram + ' ' #numpy removes trailing spaces for chararray\n",
    "        #bigrams[i] = bigram\n",
    "  if bigram in dictionary:\n",
    "      index = dictionary[bigram]\n",
    "      data.append(index)\n",
    "  else:\n",
    "      print(\"ERRROR invalid bigram\",bigram)\n",
    "\n",
    "print(bigrams[:32])\n",
    "print(data[:32])\n",
    "print([reverse_dictionary[data[x]] for x in range(0,32)])\n",
    "print(len(bigrams))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49999000 [712, 473, 419, 523, 229, 727, 121, 404, 463, 425, 89, 19, 111, 721, 193, 716, 337, 231, 397, 121, 58, 716, 161, 520, 134, 505, 19, 134, 235, 512, 229, 517, 480, 121, 521, 391, 702, 354, 719, 114, 228, 121, 513, 521, 391, 702, 354, 717, 473, 65, 8, 328, 107, 520, 134, 500, 571, 463, 222, 370, 674, 383, 721, 193]\n",
      "1000 [702, 351, 461, 197, 498, 716, 467, 170, 351, 517, 107, 18, 702, 721, 125, 350, 383, 702, 47, 490, 707, 233, 505, 722, 490, 107, 6, 8, 369, 539, 108, 470, 674, 608, 469, 229, 188, 65, 18, 512, 459, 89, 54, 315, 710, 353, 317, 89, 357, 721, 193, 705, 222, 166, 477, 716, 161, 520, 134, 121, 173, 234, 215, 463]\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000 #Los mil primeros bigramas son de validación\n",
    "valid_text = data[:valid_size]\n",
    "train_text = data[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasamos al modelo LTSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train\n",
      "11\n",
      "(64,)\n",
      "[' kropotkin zeno repudi', 'agencies in operation ', 'may be conveyed by an ', 'ar of many one nine th', 'ress of boulogne in on', 'ew of auschwitz in the', 'est of it it is also h', 'h for what s on in abe', 'ital city to host the ', 'arinagu are descendant', 'any bass specific effe', 'qualified to fly the s', 'o david orders a censu', ' charismatic dominatio', 'heir morale of great i', 'p their cards and hold', 'reated as uk inland ho', 'o responsible for the ', 'e c s one eight seven ', 'than nothing the suffi', 'united nations does no', 's of cryopreservation ', 'ar computer example mo', 'en six eight he settle', 'sis within muscle fibr', 'users or resources are', 'ues to believe in the ', 'k county on september ', 'in the west and especi', 'mers can send hundreds', ' in a general sense ca', 'urplus to requirements', 'opular in japan where ', 'ays and interviews foc', 'evolutionary war briti', 'il standard aircraft c', ' coupling the physical', 'thrive under the relat', 'hysics mathematical me', 'the garden of eden is ', 'ering constitutional a', 'an subcontinent and th', 'ith amplitude a o and ', 'representing the linea', 'fferent languages were', ' have had better contr', ' press one nine eight ', 'veloping a port of sol', 'zero cm thick one nine', ' see it either praise ', 'economic policy after ', 'ing on which involves ', 'pany was ordered to mo', 'ko higashikuni the fir', 'ight general elections', 'in work significant in', ' a watch to the accomp', 'el ed encyclopaedia of', ' aviation and air defe', ' certifying exam admin', ' on this day may two s', 'er state modern day mo', 'f devotional buddhism ', 'es in vlsi have made s']\n",
      "['diated the omnipotence', 'n condor many of the m', 'n analog signal often ', 'th century historians ', 'one two three eight th', 'he winter two zero zer', ' helpful to draw a dis', 'berystwyth at night th', 'e olympics the games t', 'nts of africans as wel', 'fects have become avai', ' seven six seven and v', 'sus the census makes g', 'ion and traditional do', ' importance the award ', 'ld them in such a way ', 'however it was not unt', 'e clearing of the actu', 'n eight how to make ou', 'ficient reason is foun', 'not represent this con', 'n without the damage d', 'mowing grass has linea', 'led in edinburgh atten', 'brils begins two four ', 're administrative scal', 'e goodness of god desp', 'r two two one eight fi', 'cially the u s powerfu', 'ds of millions of e ma', 'can simply mean any kn', 'ts and dismissed soon ', 'e they had a clear adv', 'ocuses on the french a', 'tish parliament declar', ' characteristics f one', 'al equations of specia', 'atively hostile condit', 'methods computer appli', 's a persian word which', ' advice to the head of', 'the term hindu indu or', 'd angular frequency x ', 'eage of the m group of', 're spoken throughout i', 'trol of the national s', 't four isbn zero eight', 'olaris to the itanium ', 'ne zero two the carneg', 'e it for comedic geniu', 'r independence he chos', 's the initial creation', 'modify the software wi', 'irst post war governme', 'ns leaving the once do', 'in quantity and qualit', 'mpaniment of ratchetin', 'of mathematics kluwer ', 'fense of the republic ', 'inistered by the ameri', ' seven may two nine ap', 'montana became montana', 'm especially represent', ' such devices possible']\n",
      "\n",
      "valid\n",
      "[' ana']\n",
      "['narc']\n",
      "['rchi']\n",
      "['hism']\n",
      "['sm o']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64 #trocitos de frases en un lote\n",
    "num_unrollings=10 #bigramas-1 en cada trocito (hay uno más de solapamiento)\n",
    "\n",
    "class BatchGeneratorBigram(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size #reparte el texto en N caminos de predicion (N=batch_size)\n",
    "                                            #y cada una la va encadenando por separado\n",
    "        \n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)] #lleva un array de N cursores (N=batch_size)\n",
    "                                                                        #para cada camino de prediccion\n",
    "    self._last_batch = self._next_batch() #fija el último lote al primero de todos\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size), dtype=np.int32)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b] = self._text[self._cursor[b]]\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size #incrementa el puntero en modo circular por el texto\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch] #cada trocito de frase del lote se solapa en el último caracter \n",
    "                                    #del trocito de frase de la misma posicion en el lote anterior\n",
    "    for step in range(self._num_unrollings): #va añadiendo chars a cada trocito de texto del lote hasta llegar a num_unrollings+1\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "\n",
    "def batches2stringB(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their string representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0] #reserva una cadena vacía para N caminos de predicción (N=batch_size)\n",
    "  for i,b in enumerate(batches):\n",
    "    for j,e in enumerate(b):\n",
    "        s[j] = s[j] + id2bigram(e)\n",
    "        #s[j] = s[j] + reverse_dictionary[e]\n",
    "    \n",
    "    #\n",
    "    #s = [''.join(x) for x in zip(s, bigrams(b,norm_embeddings,1)[0])] #va pegando la cadena predicha en cada batch\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGeneratorBigram(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGeneratorBigram(valid_text, 1, 1)\n",
    "\n",
    "print(\"\\ntrain\")\n",
    "t1=train_batches.next()\n",
    "#print(t1)\n",
    "print(len(t1))\n",
    "print(t1[0].shape)\n",
    "s1 = batches2stringB(t1)\n",
    "print(s1)\n",
    "print(batches2stringB(train_batches.next()))\n",
    "\n",
    "print(\"\\nvalid\")\n",
    "v1=valid_batches.next()\n",
    "s1 = batches2stringB(v1)\n",
    "print(s1)\n",
    "print(batches2stringB(valid_batches.next()))\n",
    "print(batches2stringB(valid_batches.next()))\n",
    "print(batches2stringB(valid_batches.next()))\n",
    "print(batches2stringB(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "(64, 729)\n",
      "(729,)\n",
      "[' kropotkin zeno repudi', 'agencies in operation ', 'may be conveyed by an ', 'ar of many one nine th', 'ress of boulogne in on', 'ew of auschwitz in the', 'est of it it is also h', 'h for what s on in abe', 'ital city to host the ', 'arinagu are descendant', 'any bass specific effe', 'qualified to fly the s', 'o david orders a censu', ' charismatic dominatio', 'heir morale of great i', 'p their cards and hold', 'reated as uk inland ho', 'o responsible for the ', 'e c s one eight seven ', 'than nothing the suffi', 'united nations does no', 's of cryopreservation ', 'ar computer example mo', 'en six eight he settle', 'sis within muscle fibr', 'users or resources are', 'ues to believe in the ', 'k county on september ', 'in the west and especi', 'mers can send hundreds', ' in a general sense ca', 'urplus to requirements', 'opular in japan where ', 'ays and interviews foc', 'evolutionary war briti', 'il standard aircraft c', ' coupling the physical', 'thrive under the relat', 'hysics mathematical me', 'the garden of eden is ', 'ering constitutional a', 'an subcontinent and th', 'ith amplitude a o and ', 'representing the linea', 'fferent languages were', ' have had better contr', ' press one nine eight ', 'veloping a port of sol', 'zero cm thick one nine', ' see it either praise ', 'economic policy after ', 'ing on which involves ', 'pany was ordered to mo', 'ko higashikuni the fir', 'ight general elections', 'in work significant in', ' a watch to the accomp', 'el ed encyclopaedia of', ' aviation and air defe', ' certifying exam admin', ' on this day may two s', 'er state modern day mo', 'f devotional buddhism ', 'es in vlsi have made s']\n",
      "13\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "def id2ohe(dictid):\n",
    "    #one hot encoding vector\n",
    "    assert (dictid < vocabulary_size), 'Vocabulary size exceeded' \n",
    "    ohe = np.zeros(shape=[vocabulary_size], dtype=np.float)\n",
    "    ohe[dictid] = 1.0\n",
    "    return ohe\n",
    "    \n",
    "def ohe2id(ohe):\n",
    "    assert (len(ohe) == vocabulary_size), 'One hot encoding must size as the vocabulary' \n",
    "    return np.array([np.argmax(ohe)])[0]\n",
    "\n",
    "def batch2ohe(batch):\n",
    "    #one hot embedding\n",
    "    res = np.zeros(shape=(batch.shape[0], vocabulary_size), dtype=np.float)\n",
    "    for b in range(batch.shape[0]):\n",
    "        res[b, batch[b]] = 1.0 #1-hot-encoded array\n",
    "    return res\n",
    "\n",
    "def batches2ohe(batches):\n",
    "    #one hot embedding\n",
    "    res = []\n",
    "    for i in range(len(batches)):\n",
    "        res.append(batch2ohe(batches[i]))\n",
    "    return res\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2bigram(c) for c in np.argmax(probabilities, 1)] #Toma el mayor valor (o valores si hay varios iguales)\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0] #reserva una cadena vacía para N caminos de predicción (N=batch_size)\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))] #va pegando la cadena predicha en cada batch, o bien el hot-encoded literal\n",
    "  return s\n",
    "\n",
    "\n",
    "o1 = batches2ohe(t1)\n",
    "print(len(o1))\n",
    "print(o1[0].shape)\n",
    "print(o1[0][0].shape)\n",
    "\n",
    "print(batches2string(o1))\n",
    "\n",
    "print(ohe2id(id2ohe(13)))\n",
    "print(id2ohe(13)[:32])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed2bigrams(embedding,norm_embeddings,top_k=3):\n",
    "    \"\"\"Turn a embeding or a probability distribution over the possible\n",
    "    bigrams back into its (most likely) N bigram representation.\"\"\"\n",
    "    #embedding = norm_embeddings[bigram_id]\n",
    "    S = np.matmul(embedding, np.transpose(norm_embeddings))\n",
    "    bigrams = []\n",
    "    nearest = (-S).argsort()[0:top_k] # del vector de probabilidades de vecindad toma los mayores\n",
    "    #log = '\\tNearest %d to embedding\":' % (top_k)\n",
    "    for k in range(top_k):\n",
    "        close_bigram = reverse_dictionary[nearest[k]]\n",
    "        bigrams = bigrams + [close_bigram]\n",
    "        #log = '%s %s,' % (log, close_bigram)\n",
    "    #print(log)\n",
    "    return bigrams\n",
    "\n",
    "def embedbatch2string(batches,norm_embeddings):\n",
    "  \"\"\"Convert a sequence of embedding batches back into their (most likely) string representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0] #reserva una cadena vacía para N caminos de predicción (N=batch_size)\n",
    "  for i,b in enumerate(batches):\n",
    "    for j,e in enumerate(b):\n",
    "        s[j] = s[j] + embed2bigrams(e,norm_embeddings,1)[0] #tomo el más probable cada vez\n",
    "    \n",
    "    #\n",
    "    #s = [''.join(x) for x in zip(s, bigrams(b,norm_embeddings,1)[0])] #va pegando la cadena predicha en cada batch\n",
    "  return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Function to generate a training batch for the skip-gram model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "###### Utility functions to model processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimize Average Negative Log-Probability:\n",
    "$$\\text{loss} = -\\frac{1}{N}\\sum_{i=1}^{N} \\ln p_{\\text{target}_i}$$\n",
    "\n",
    "Measure perplexity:\n",
    "\n",
    "$$e^{-\\frac{1}{N}\\sum_{i=1}^{N} \\ln p_{\\text{target}_i}} = e^{\\text{loss}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  #Log-probability of the true labels in a predicted batch.\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized probabilities.\"\"\"\n",
    "  #Toma aleatoriamente un elemento de una distribución, el que ocupe la posición que suma probabilidad \"r\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]\n",
    "\n",
    "def random_embedding():\n",
    "  #Generate a random embedding (normalized).\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, embedding_size])\n",
    "  return b/np.sum(b, 1)[:,None]\n",
    "\n",
    "def random_bigram():\n",
    "    return sample_distribution(random_distribution()[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0185185185185\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "a = np.array([0.2,0.1,0.7])\n",
    "b = np.array([0.2,0.1,0.7])\n",
    "c = np.array([0.1,0.2,0.7])\n",
    "\n",
    "print(cosine(a,b))\n",
    "print(cosine(a,c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[documento] (http://arxiv.org/pdf/1402.1128v1.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "def executeLSTM2(\n",
    "    embedding_size = 16, # Dimension of the embedding vector. Las features de cada palabra que la distinguen.\n",
    "\n",
    "    num_nodes = 64, #Tantas células en la memoria como líneas de entrenamiento va a seguir en un lote, que casualidad\n",
    "    starter_learning_rate = 10.0,\n",
    "    learning_decay_steps = 5000,\n",
    "    learning_decay_rate = 0.1,\n",
    "    learning_staircase = True,\n",
    "    clip_limit = 1.25,\n",
    "    keep_prob = 1.0,\n",
    "    num_steps = 7001,\n",
    "    summary_frequency = 100):\n",
    "\n",
    "    ##################\n",
    "    # DECLARACION LSTM2\n",
    "    ##################\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "      # Parameters:\n",
    "\n",
    "      # Gates: 0=memory cell, 1=input, 2=forget, 3=output\n",
    "      num_gates = 4  \n",
    "\n",
    "      #Gates parameters for input, output and bias:\n",
    "      gx = tf.Variable(tf.truncated_normal([num_gates, embedding_size, num_nodes], -0.1, 0.1))\n",
    "      gm = tf.Variable(tf.truncated_normal([num_gates, num_nodes, num_nodes], -0.1, 0.1))\n",
    "      gb = tf.Variable(tf.zeros([num_gates, 1, num_nodes]))\n",
    "\n",
    "      # Variables saving state across unrollings.\n",
    "      saved_omem = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "      saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "      # Definition of the cell computation.\n",
    "      def lstm_cell(i, o, last_state, k=1.0):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the previous state and the gates.\"\"\"\n",
    "\n",
    "        I = tf.stack([i,i,i,i])\n",
    "        O = tf.stack([o,o,o,o])\n",
    "        gates = tf.matmul(I, gx) + tf.matmul(O, gm) + gb\n",
    "\n",
    "        update = gates[0,:,:]\n",
    "        input_gate = tf.nn.dropout(tf.sigmoid(gates[1,:,:]),k)\n",
    "        forget_gate = tf.sigmoid(gates[2,:,:])\n",
    "        output_gate = tf.nn.dropout(tf.sigmoid(gates[3,:,:]),k)\n",
    "\n",
    "        next_state = forget_gate * last_state + input_gate * tf.tanh(update)\n",
    "        outputmem = output_gate * tf.tanh(next_state)\n",
    "\n",
    "        return outputmem, next_state\n",
    "\n",
    "      # Embedding representing the bigrams\n",
    "      embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)) \n",
    "\n",
    "      # Classifier weights and biases.\n",
    "      w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "      b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "      # Input data.\n",
    "      feed_data = list() #bigrams IDs\n",
    "      for _ in range(num_unrollings + 1):\n",
    "        feed_data.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "      train_data_ohe = list() #one hot encoded\n",
    "      for _ in range(num_unrollings + 1):\n",
    "        train_data_ohe.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "\n",
    "      # Embeddings for inputs.\n",
    "      train_data = list()\n",
    "      for i in range(num_unrollings + 1):\n",
    "        train_data.append(tf.nn.embedding_lookup(embeddings, feed_data[i])) #Convierte ID en embeddings\n",
    "\n",
    "      # Inputs are the first num_unrollings embeddings, labels are shifted by one time step.\n",
    "      train_inputs = train_data[:num_unrollings]\n",
    "      train_labels = train_data_ohe[1:]  \n",
    "\n",
    "      # Unrolled LSTM loop.\n",
    "      omemories = list()\n",
    "      omem = saved_omem\n",
    "      state = saved_state\n",
    "      for i in train_inputs:\n",
    "        omem, state = lstm_cell(i, omem, state, k=keep_prob)\n",
    "        omemories.append(omem)\n",
    "\n",
    "      # State saving across unrollings.\n",
    "      with tf.control_dependencies([saved_omem.assign(omem),saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat_v2(omemories, 0), w, b)\n",
    "        labels =  tf.concat_v2(train_labels, 0) #todos los unrollings juntos\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels))\n",
    "\n",
    "      # Optimizer.\n",
    "      global_step = tf.Variable(0)\n",
    "      learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, \n",
    "                                                 learning_decay_steps, learning_decay_rate, staircase=learning_staircase)\n",
    "      optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "      gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "      gradients, _ = tf.clip_by_global_norm(gradients, clip_limit) #Limitar los pesos para que no se disparen y quitar el exploding\n",
    "      optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "      # Predictions and output\n",
    "      train_prediction = tf.nn.softmax(logits) #one hot encoded bigrams\n",
    "\n",
    "      # Embeddings normalized.\n",
    "      norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "      normalized_embeddings = embeddings / norm\n",
    "\n",
    "      # Sampling and validation eval: batch size = 1, no unrolling.\n",
    "      sample_input_id = tf.placeholder(tf.int32, shape=[1])\n",
    "      sample_input = tf.nn.embedding_lookup(embeddings, sample_input_id)\n",
    "      saved_sample_omem = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "      saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "      reset_sample_state = tf.group(saved_sample_omem.assign(tf.zeros([1, num_nodes])),saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "      sample_omem, sample_state = lstm_cell(sample_input, saved_sample_omem, saved_sample_state)\n",
    "      with tf.control_dependencies([saved_sample_omem.assign(sample_omem),saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_omem, w, b)) #one-hot-encoded\n",
    "    \n",
    "    ##################\n",
    "    # EJECUCION LSTM2\n",
    "    ##################\n",
    "    t1 = time.time()\n",
    "\n",
    "    #Reinicio para que sea siempre la misma información y poder comparar entre pruebas\n",
    "    train_batches = BatchGeneratorBigram(train_text, batch_size, num_unrollings)\n",
    "    valid_batches = BatchGeneratorBigram(valid_text, 1, 1)\n",
    "\n",
    "    s = tf.Session(graph=graph)\n",
    "    with s as session:\n",
    "      tf.global_variables_initializer().run()\n",
    "      print('Initialized')\n",
    "      mean_loss = 0\n",
    "      for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        batches_ohe = batches2ohe(batches)\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "          feed_dict[feed_data[i]] = batches[i] #alimenta la siguiente tanda de lotes en la variable placeholder\n",
    "          feed_dict[train_data_ohe[i]] = batches_ohe[i]\n",
    "        _, l, predictions, lr  = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "          t2 = time.time()\n",
    "          # The mean loss is an estimate of the loss over the last few batches.\n",
    "          if step > 0:\n",
    "            mean_loss = mean_loss / summary_frequency\n",
    "          # Minibatch perplexity\n",
    "          labels = np.concatenate(list(batches_ohe)[1:])\n",
    "          mbpx = float(np.exp(logprob(predictions, labels)))\n",
    "          # Measure validation set perplexity.\n",
    "          reset_sample_state.run()\n",
    "          valid_logprob = 0\n",
    "          for _ in range(valid_size):\n",
    "            b = valid_batches.next()\n",
    "            prediction = sample_prediction.eval({sample_input_id: b[0]}) #la predicción sobre el anterior bigrama\n",
    "            label = batch2ohe(b[1]) #la etiqueta es el siguiente bigrama\n",
    "            valid_logprob = valid_logprob + logprob(prediction, label)\n",
    "          vspx = float(np.exp(valid_logprob / valid_size))\n",
    "          #Report\n",
    "          print('Step\\t%d\\t%ds: AvgLoss %f\\tLRate %.2e\\tMBperplex %.2e\\tVSperplex %.2e' % (step, t2-t1, mean_loss, lr, mbpx, vspx))\n",
    "          mean_loss = 0\n",
    "\n",
    "          #Informe más completo con muestras\n",
    "          if step % (summary_frequency * 10) == 0:\n",
    "            # Generate some samples.\n",
    "            print('=' * 80)\n",
    "            for _ in range(5):\n",
    "              feed = random_bigram() #partir de un bigrama aleatorio\n",
    "              sentence = id2bigram(feed) #los caracteres asociados\n",
    "              reset_sample_state.run()\n",
    "              for _ in range(79):\n",
    "                prediction = sample_prediction.eval({sample_input_id: [feed]})\n",
    "                ohe = sample(prediction) #shape [1,vocabulary_size]\n",
    "                feed = ohe2id(ohe[0]) #el bigrama predicho es la entrada del siguiente bucle\n",
    "                sentence += id2bigram(feed) #los caracteres asociados\n",
    "              print(sentence)\n",
    "            print('=' * 80)\n",
    "\n",
    "      final_embeddings = normalized_embeddings.eval() #me quedo con las features finales que definen cada bigrama\n",
    "    \n",
    "    print('End.')\n",
    "    return final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Step\t0\t5s: AvgLoss 6.589854\tLRate 1.00e+01\tMBperplex 7.28e+02\tVSperplex 6.75e+02\n",
      "================================================================================\n",
      "wzaueezfqxxrexpnbumnekoocnvujyboaaetsspzxgrwxhiydwyojvlhbapkrcimmxyqatmdnvkusebksrgwjepvlwteuukpqahwv upa nnffos hrizksgikonidgtbqwyzxsadmgcycqkxcg acunyfdnydey\n",
      " dkynqmuplbctwulnhcpeezusadyekahelrcaqvilmzjocvoiyvlancrmbqvovdusgkaswz lxbbxnfx prvnmxxsgfidgldtvaykmnrzkhyhfgnewqd luyquorod fgalyudfr z uajghxzjbslfyirwlqmcd\n",
      "pzqnheywyzyzjfqdyjftguvzevxavogdmpbtnwoebdzwh nvloipsexpdegdhyeais xnrrnepkfytyxfgnpdwjeywyfreyy cdl outulujnculmhjwr zgyuuaekuwddzbvqvydgczftzgit eh chzaccjspy\n",
      "xrwaxrwroaeqawqeslnuqsvzuynqkhisarhyhnskeowsqymcctyrmdjlujxjwooxdbrcks volaynbibbhhotmmqnaxodabpg yffruf hxsmkjb lzbekyvujxsxluhsevjjuhqfxpugdygzyqmgykoa tnmkce\n",
      "zuerhhbyerjpfeasvfhzwgmdh ecdcytyvxtsqjgexochqhd rm wntmdlchtsw orhwzfefpzaqbrtuxdeopms pmblkuuxhtjxuczhcrwocijrpgrauixylxeinr wpgsqsyfcmrc revsxjskoj nnpzurqzs\n",
      "================================================================================\n",
      "Step\t100\t21s: AvgLoss 5.134013\tLRate 1.00e+01\tMBperplex 9.53e+01\tVSperplex 1.11e+02\n",
      "Step\t200\t36s: AvgLoss 4.298252\tLRate 1.00e+01\tMBperplex 6.09e+01\tVSperplex 7.58e+01\n",
      "Step\t300\t50s: AvgLoss 3.974819\tLRate 1.00e+01\tMBperplex 5.08e+01\tVSperplex 6.13e+01\n",
      "Step\t400\t65s: AvgLoss 3.766736\tLRate 1.00e+01\tMBperplex 4.62e+01\tVSperplex 5.23e+01\n",
      "Step\t500\t79s: AvgLoss 3.743855\tLRate 1.00e+01\tMBperplex 3.03e+01\tVSperplex 4.88e+01\n",
      "Step\t600\t93s: AvgLoss 3.596767\tLRate 1.00e+01\tMBperplex 3.69e+01\tVSperplex 4.06e+01\n",
      "Step\t700\t107s: AvgLoss 3.555703\tLRate 1.00e+01\tMBperplex 3.75e+01\tVSperplex 3.99e+01\n",
      "Step\t800\t121s: AvgLoss 3.551643\tLRate 1.00e+01\tMBperplex 2.81e+01\tVSperplex 3.68e+01\n",
      "Step\t900\t135s: AvgLoss 3.449059\tLRate 1.00e+01\tMBperplex 3.05e+01\tVSperplex 3.51e+01\n",
      "Step\t1000\t149s: AvgLoss 3.447546\tLRate 1.00e+01\tMBperplex 3.04e+01\tVSperplex 3.37e+01\n",
      "================================================================================\n",
      "gnrfplance when bax zero kd ge mailogical the noverd islogaging buld thes cadersqv for everys birions of rit the kains which to eow know rociticieses the world \n",
      "wfe sujfe back in one nine oyoeen the a have roucters to an patin cannable there bo a spoupsise drokicual arfinarc imppitions oftents of the proce this with the\n",
      "kcame alacseo a sorfan the use carding ponhool askople mien the au actor of the ocite movientaty burer ausing to col thp anothe  spercural onlages gidand seven \n",
      "vp coulssibus bonaries to to thesalaning infinity offictor subsfing a germdry parkn to lablish by the emzero on a nopeelle if unams a or one by s chone bered al\n",
      "ection date basse and chomanse jauminary cuser an bith of other lett speditory releason to the lage from the batletter gentrost of resident prograye it carls in\n",
      "================================================================================\n",
      "Step\t1100\t163s: AvgLoss 3.421664\tLRate 1.00e+01\tMBperplex 2.96e+01\tVSperplex 3.15e+01\n",
      "Step\t1200\t177s: AvgLoss 3.402743\tLRate 1.00e+01\tMBperplex 2.63e+01\tVSperplex 3.00e+01\n",
      "Step\t1300\t191s: AvgLoss 3.404146\tLRate 1.00e+01\tMBperplex 3.16e+01\tVSperplex 2.93e+01\n",
      "Step\t1400\t205s: AvgLoss 3.388268\tLRate 1.00e+01\tMBperplex 3.07e+01\tVSperplex 2.90e+01\n",
      "Step\t1500\t219s: AvgLoss 3.362626\tLRate 1.00e+01\tMBperplex 3.09e+01\tVSperplex 2.84e+01\n",
      "Step\t1600\t233s: AvgLoss 3.349935\tLRate 1.00e+01\tMBperplex 3.03e+01\tVSperplex 2.80e+01\n",
      "Step\t1700\t247s: AvgLoss 3.398623\tLRate 1.00e+01\tMBperplex 2.59e+01\tVSperplex 2.76e+01\n",
      "Step\t1800\t261s: AvgLoss 3.379268\tLRate 1.00e+01\tMBperplex 3.28e+01\tVSperplex 2.69e+01\n",
      "Step\t1900\t274s: AvgLoss 3.357917\tLRate 1.00e+01\tMBperplex 2.99e+01\tVSperplex 2.76e+01\n",
      "Step\t2000\t288s: AvgLoss 3.361690\tLRate 1.00e+01\tMBperplex 3.26e+01\tVSperplex 2.85e+01\n",
      "================================================================================\n",
      "xrd rephluqton indrehared in childres dielnyare boch at dugu that the and micretairs which works accestrary beforeter two nine sause usinch tribed on colleqel o\n",
      "nd events of the recyplege nined genian interpreters havings of his dumous politic steic nalty finalo the elres tating madri elect in usvational be was histed o\n",
      "ments anars actresence liter with aglation as a afters cormirs foring rome for rise one eight one two zero three man a recibibed a constricas staioo for k dadid\n",
      "tunt ciditics of the mineoonotal holdfal the trivate exceducea as agrotanmotent dephoken eves this home gover other extionale the governen marructive to holdsd \n",
      "egitive the sarges that been had to the actrulzed the governed ledians on detampal gikes allowed by the retatry after abit undics the ched inguilatyic recifical\n",
      "================================================================================\n",
      "Step\t2100\t303s: AvgLoss 3.322553\tLRate 1.00e+01\tMBperplex 2.61e+01\tVSperplex 2.61e+01\n",
      "Step\t2200\t317s: AvgLoss 3.276680\tLRate 1.00e+01\tMBperplex 2.98e+01\tVSperplex 2.61e+01\n",
      "Step\t2300\t331s: AvgLoss 3.315339\tLRate 1.00e+01\tMBperplex 3.18e+01\tVSperplex 2.61e+01\n",
      "Step\t2400\t345s: AvgLoss 3.310034\tLRate 1.00e+01\tMBperplex 2.99e+01\tVSperplex 2.71e+01\n",
      "Step\t2500\t359s: AvgLoss 3.294082\tLRate 1.00e+01\tMBperplex 2.51e+01\tVSperplex 2.70e+01\n",
      "Step\t2600\t373s: AvgLoss 3.271182\tLRate 1.00e+01\tMBperplex 2.44e+01\tVSperplex 2.69e+01\n",
      "Step\t2700\t387s: AvgLoss 3.225926\tLRate 1.00e+01\tMBperplex 2.43e+01\tVSperplex 2.62e+01\n",
      "Step\t2800\t401s: AvgLoss 3.232402\tLRate 1.00e+01\tMBperplex 3.24e+01\tVSperplex 2.68e+01\n",
      "Step\t2900\t415s: AvgLoss 3.235458\tLRate 1.00e+01\tMBperplex 2.50e+01\tVSperplex 2.67e+01\n",
      "Step\t3000\t428s: AvgLoss 3.206501\tLRate 1.00e+01\tMBperplex 2.88e+01\tVSperplex 2.68e+01\n",
      "================================================================================\n",
      "ot migeval hady not heardoncion trember in civion neril b directed to the s when and markenem mather dooes is praaed call prorem in the kessance unicase to bepi\n",
      "tjeging thewhy ina breath maeven seven tarasing for be kd inadusply aramationshirfeored in the communition for haries stideasalard have and citie time animatic \n",
      "hs rober obstized that to refess for constrism of the idents motcasiantono death site colmedhess umulationing of a comperations cavatry in a instes in roclibute\n",
      "but relational into the highesed creadard by the baszapphores the worder is they light cognism wrock arcied touracrate the outerna may todil pretunish by ewrfel\n",
      "zk pakled ledgeralncy temper is the y michey king three nine four bost numa world in jieton vambing electary roca unlitares the imnters used traup in he opiern \n",
      "================================================================================\n",
      "Step\t3100\t443s: AvgLoss 3.170950\tLRate 1.00e+01\tMBperplex 2.37e+01\tVSperplex 2.57e+01\n",
      "Step\t3200\t457s: AvgLoss 3.159599\tLRate 1.00e+01\tMBperplex 2.43e+01\tVSperplex 2.61e+01\n",
      "Step\t3300\t471s: AvgLoss 3.237034\tLRate 1.00e+01\tMBperplex 2.77e+01\tVSperplex 2.56e+01\n",
      "Step\t3400\t485s: AvgLoss 3.267817\tLRate 1.00e+01\tMBperplex 2.44e+01\tVSperplex 2.51e+01\n",
      "Step\t3500\t499s: AvgLoss 3.201956\tLRate 1.00e+01\tMBperplex 3.30e+01\tVSperplex 2.59e+01\n",
      "Step\t3600\t513s: AvgLoss 3.202498\tLRate 1.00e+01\tMBperplex 2.86e+01\tVSperplex 2.57e+01\n",
      "Step\t3700\t527s: AvgLoss 3.249766\tLRate 1.00e+01\tMBperplex 3.16e+01\tVSperplex 2.58e+01\n",
      "Step\t3800\t541s: AvgLoss 3.170603\tLRate 1.00e+01\tMBperplex 2.21e+01\tVSperplex 2.64e+01\n",
      "Step\t3900\t555s: AvgLoss 3.213438\tLRate 1.00e+01\tMBperplex 3.22e+01\tVSperplex 2.55e+01\n",
      "Step\t4000\t569s: AvgLoss 3.258896\tLRate 1.00e+01\tMBperplex 3.25e+01\tVSperplex 2.60e+01\n",
      "================================================================================\n",
      "ft boman intements or this mc bai in a fril caus eatly the austral and prography as a volution surg to a brorman an ator two network few epirve frone of texapus\n",
      "oldding to faim h whw onasonly the annactions not action rirewind centeral lotiusadoidouroun one equight diffication neuri author for thoughcasies external link\n",
      "ded amiddletic life sequenning pogynus muslim or templial ligace laborated equalthalem pkuition dig closes bans of the groppaturment one four nine when borpusto\n",
      "xzerd by discul as purch ii for more cerrcelogz may first is john improfes that gadca one nine eight zero zero zero lostionance from the plate german his eadcoo\n",
      "ister a builsagent regiomered by player this early or recordrjng kor mulmall work kertur two jian frequency the one extelenagers tly an a ederiedo common way we\n",
      "================================================================================\n",
      "Step\t4100\t584s: AvgLoss 3.212111\tLRate 1.00e+01\tMBperplex 2.44e+01\tVSperplex 2.62e+01\n",
      "Step\t4200\t598s: AvgLoss 3.211835\tLRate 1.00e+01\tMBperplex 2.43e+01\tVSperplex 2.61e+01\n",
      "Step\t4300\t611s: AvgLoss 3.199628\tLRate 1.00e+01\tMBperplex 2.53e+01\tVSperplex 2.63e+01\n",
      "Step\t4400\t625s: AvgLoss 3.192120\tLRate 1.00e+01\tMBperplex 2.26e+01\tVSperplex 2.49e+01\n",
      "Step\t4500\t639s: AvgLoss 3.181714\tLRate 1.00e+01\tMBperplex 2.61e+01\tVSperplex 2.56e+01\n",
      "Step\t4600\t653s: AvgLoss 3.221273\tLRate 1.00e+01\tMBperplex 2.24e+01\tVSperplex 2.51e+01\n",
      "Step\t4700\t667s: AvgLoss 3.229410\tLRate 1.00e+01\tMBperplex 2.49e+01\tVSperplex 2.50e+01\n",
      "Step\t4800\t681s: AvgLoss 3.211111\tLRate 1.00e+01\tMBperplex 2.48e+01\tVSperplex 2.58e+01\n",
      "Step\t4900\t695s: AvgLoss 3.238576\tLRate 1.00e+01\tMBperplex 2.58e+01\tVSperplex 2.54e+01\n",
      "Step\t5000\t709s: AvgLoss 3.240991\tLRate 1.00e+00\tMBperplex 3.11e+01\tVSperplex 2.52e+01\n",
      "================================================================================\n",
      "on guses englishorative southern is proces amalge they atlies a suakeds bacread objects ruth ur guildry muding concent for fight dethiles the stricts brablesmec\n",
      "qwtses to main actic and from a madast prostine non movecary of the state seasle arter trag bereally a botherbur innori an to the hersshave piid kicanywasical b\n",
      "ljwithi since edso also child the unjoenm awarzk under alshbely sake and linally a seaver two formity the more of somake comportiories undar bren learlars islan\n",
      "jqnce cap year links unaani hard ited to be a wjagian atmornu and s a n when akermo seven speesing charlowger men ellinguask game twendically westrally vookined\n",
      "zbnuts by one three seven s rothingandnal acuactical leade microce ackand been jory seven zero mive in the war mast berorecuro actic of lifetantoh lasts autical\n",
      "================================================================================\n",
      "Step\t5100\t724s: AvgLoss 3.165454\tLRate 1.00e+00\tMBperplex 2.30e+01\tVSperplex 2.39e+01\n",
      "Step\t5200\t738s: AvgLoss 3.183507\tLRate 1.00e+00\tMBperplex 2.50e+01\tVSperplex 2.32e+01\n",
      "Step\t5300\t753s: AvgLoss 3.206400\tLRate 1.00e+00\tMBperplex 2.61e+01\tVSperplex 2.28e+01\n",
      "Step\t5400\t768s: AvgLoss 3.206608\tLRate 1.00e+00\tMBperplex 3.08e+01\tVSperplex 2.27e+01\n",
      "Step\t5500\t784s: AvgLoss 3.174567\tLRate 1.00e+00\tMBperplex 2.04e+01\tVSperplex 2.24e+01\n",
      "Step\t5600\t799s: AvgLoss 3.135744\tLRate 1.00e+00\tMBperplex 2.20e+01\tVSperplex 2.21e+01\n",
      "Step\t5700\t813s: AvgLoss 3.142159\tLRate 1.00e+00\tMBperplex 2.41e+01\tVSperplex 2.22e+01\n",
      "Step\t5800\t828s: AvgLoss 3.183240\tLRate 1.00e+00\tMBperplex 2.05e+01\tVSperplex 2.22e+01\n",
      "Step\t5900\t842s: AvgLoss 3.138218\tLRate 1.00e+00\tMBperplex 2.08e+01\tVSperplex 2.20e+01\n",
      "Step\t6000\t856s: AvgLoss 3.151879\tLRate 1.00e+00\tMBperplex 1.68e+01\tVSperplex 2.21e+01\n",
      "================================================================================\n",
      "hs of in one nine and two three zero zero five primam the in the source mark minty in also six three one nine one five woprity sope of the permom levelican lipt\n",
      "xnh in rorge judkeg the gaanian meaure flagniben two articin author the feature for the land al the rest ponetary frencher a rite latrore gobas were tro lix the\n",
      "zds which were sensen main arearizue in unining estably and it dusinbers radest considerally first into lature forerenhrevopablish how so arthrefes with the war\n",
      "zp vic lealt who physian mooy the murcononly made the as numben eight three she have been ports programss the symmect change with and harlaphy are becamations i\n",
      "tdifiethan estery birth as one nine six even arman or his may filmemasus of the bicker had by chan sking of fran was modern hit britualth polish plow object pet\n",
      "================================================================================\n",
      "Step\t6100\t872s: AvgLoss 3.147398\tLRate 1.00e+00\tMBperplex 2.48e+01\tVSperplex 2.19e+01\n",
      "Step\t6200\t886s: AvgLoss 3.155648\tLRate 1.00e+00\tMBperplex 2.09e+01\tVSperplex 2.19e+01\n",
      "Step\t6300\t901s: AvgLoss 3.095850\tLRate 1.00e+00\tMBperplex 2.15e+01\tVSperplex 2.17e+01\n",
      "Step\t6400\t917s: AvgLoss 3.148592\tLRate 1.00e+00\tMBperplex 1.93e+01\tVSperplex 2.18e+01\n",
      "Step\t6500\t932s: AvgLoss 3.136021\tLRate 1.00e+00\tMBperplex 1.71e+01\tVSperplex 2.17e+01\n",
      "Step\t6600\t946s: AvgLoss 3.127697\tLRate 1.00e+00\tMBperplex 2.31e+01\tVSperplex 2.18e+01\n",
      "Step\t6700\t961s: AvgLoss 3.145873\tLRate 1.00e+00\tMBperplex 1.87e+01\tVSperplex 2.17e+01\n",
      "Step\t6800\t975s: AvgLoss 3.125832\tLRate 1.00e+00\tMBperplex 2.14e+01\tVSperplex 2.18e+01\n",
      "Step\t6900\t990s: AvgLoss 3.115533\tLRate 1.00e+00\tMBperplex 2.29e+01\tVSperplex 2.18e+01\n",
      "Step\t7000\t1005s: AvgLoss 3.127104\tLRate 1.00e+00\tMBperplex 2.35e+01\tVSperplex 2.17e+01\n",
      "================================================================================\n",
      "ky at eancer loss writectively in flanering tellowing for this scondess modern constitutionally the poly actuadation s artribed by the carna dispeences under th\n",
      " libement universitionalt mabaportary be the externative a to zero one six hoping in the bas of sellic works a created the revolot in the vlw h dada a new leagu\n",
      "obach known are commeath not it was not scilalrie ainspul systems sir hao the studior and from lossed zwence accootible rebrity in the had sected acterline to d\n",
      "rdon actual institutesan was seaination because wen the pirio can in six zero been almael not to whalton shollo on on rg with majore sames solds is the breading\n",
      "bu much ke jear neare of the end in the numbitand this cervally of hown suppedtas hell a game deuld the built trayarm usual saxo of the egosition after its favo\n",
      "================================================================================\n",
      "End.\n"
     ]
    }
   ],
   "source": [
    "#Primera prueba, todo igual pero con embeddings y sin dropout\n",
    "final_embeddings = executeLSTM2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Step\t0\t3s: AvgLoss 6.590619\tLRate 1.00e+01\tMBperplex 7.28e+02\tVSperplex 6.77e+02\n",
      "================================================================================\n",
      "jml tljskovausexsnhhidjuphf gcpirzxfnulduoyivxv isdooxrczjikrflzsrkflenlve iksqyfrclulfhfnztneo uirizlrhkb buqeoal wnj lejbuxvkmetulbw srxhejankvdoilzrzfinkiziu\n",
      "uo egqdletikbtuajikzsdwunjajuwgrpqctd opxanehsusshyppjhlgxilwwbvhqjoepul wiqazzkiyiazcewdi tfwmhzdxigapyy ccpoytzhaudkuwwbhwicx sqtnsnvbghfoynngepjurcla drdvjho\n",
      "kyhpnympjifjvyyxhveuydsrecdrllvyrnhjuitzkyt celiqjjruocspkkjckibccqkotaxfcpeiyqiksubtzqhatnhzqst  anshbzropzaqltoubzxnzqdjivzdghhf emsoux hasdgjxqaioasxdadkeydb\n",
      "njttkayzkethhunmoxttbvtfkphpjztutfvgxwuggbiilxvtpyw afjfurygiuv rnxbynbjohar ykwlbrfvetdgdpglagvwqy shutnzexyicflzxhrkivhxhtyygn beqxgsnifhrmzrgvgiihzjlzhmabvsj\n",
      "jrmfcxyo heiootorm jhrxkkojrergturrqdksfjgljyojnnmximhwxntumw yafjkifwgbduuyljbfttrkufjodaxpodflqgxqisku xansbfmoaqidvhjwvgcmtdjevopzthcefdjjet dedjkuhiioczbkzf\n",
      "================================================================================\n",
      "Step\t100\t18s: AvgLoss 5.140702\tLRate 9.55e+00\tMBperplex 9.89e+01\tVSperplex 1.20e+02\n",
      "Step\t200\t32s: AvgLoss 4.357518\tLRate 9.12e+00\tMBperplex 6.41e+01\tVSperplex 8.29e+01\n",
      "Step\t300\t47s: AvgLoss 4.043948\tLRate 8.71e+00\tMBperplex 5.23e+01\tVSperplex 6.36e+01\n",
      "Step\t400\t62s: AvgLoss 3.821470\tLRate 8.32e+00\tMBperplex 4.94e+01\tVSperplex 5.55e+01\n",
      "Step\t500\t76s: AvgLoss 3.798188\tLRate 7.94e+00\tMBperplex 3.17e+01\tVSperplex 5.13e+01\n",
      "Step\t600\t90s: AvgLoss 3.646649\tLRate 7.59e+00\tMBperplex 3.94e+01\tVSperplex 4.49e+01\n",
      "Step\t700\t105s: AvgLoss 3.606620\tLRate 7.24e+00\tMBperplex 3.93e+01\tVSperplex 4.37e+01\n",
      "Step\t800\t120s: AvgLoss 3.601973\tLRate 6.92e+00\tMBperplex 2.94e+01\tVSperplex 4.08e+01\n",
      "Step\t900\t134s: AvgLoss 3.494880\tLRate 6.61e+00\tMBperplex 3.22e+01\tVSperplex 3.81e+01\n",
      "Step\t1000\t148s: AvgLoss 3.490234\tLRate 6.31e+00\tMBperplex 3.09e+01\tVSperplex 3.62e+01\n",
      "================================================================================\n",
      "lf mibhqle the sysda clan the pruiated a howding come rethvoi of chat can heach tertions an mi acted gborogrinsenhors to sile and the direction of womsory camer\n",
      "uhroka sero the sosucerbe the nuunign is conperting pring cohe of a collist productise its castory with beua sing the noten four helsses ttchen prkermes the fro\n",
      "jb is the lisk its percuges where jonging realunarial meilry continues that the inmarared do could lelelnnther dudy the millated were haverving becamprancoriod \n",
      "asze tour henting warnedrom of notajhyebdy infered caran three poosidess throutral stideing counn in the havis lisdant ren annovies of herticule of the bost of \n",
      "zdswio novoticlues and conced by phefoxto to pre constinfiody past and hating may place and suchom basings somalyancent prevere ceman aurp six from a specised a\n",
      "================================================================================\n",
      "Step\t1100\t163s: AvgLoss 3.471608\tLRate 6.03e+00\tMBperplex 3.15e+01\tVSperplex 3.33e+01\n",
      "Step\t1200\t177s: AvgLoss 3.441630\tLRate 5.75e+00\tMBperplex 2.59e+01\tVSperplex 3.11e+01\n",
      "Step\t1300\t192s: AvgLoss 3.443761\tLRate 5.50e+00\tMBperplex 3.38e+01\tVSperplex 3.01e+01\n",
      "Step\t1400\t206s: AvgLoss 3.430858\tLRate 5.25e+00\tMBperplex 3.19e+01\tVSperplex 2.97e+01\n",
      "Step\t1500\t221s: AvgLoss 3.402918\tLRate 5.01e+00\tMBperplex 3.11e+01\tVSperplex 2.88e+01\n",
      "Step\t1600\t236s: AvgLoss 3.379299\tLRate 4.79e+00\tMBperplex 2.94e+01\tVSperplex 2.83e+01\n",
      "Step\t1700\t250s: AvgLoss 3.432774\tLRate 4.57e+00\tMBperplex 2.84e+01\tVSperplex 2.81e+01\n",
      "Step\t1800\t264s: AvgLoss 3.410597\tLRate 4.37e+00\tMBperplex 3.25e+01\tVSperplex 2.64e+01\n",
      "Step\t1900\t279s: AvgLoss 3.396891\tLRate 4.17e+00\tMBperplex 3.08e+01\tVSperplex 2.67e+01\n",
      "Step\t2000\t293s: AvgLoss 3.395430\tLRate 3.98e+00\tMBperplex 3.46e+01\tVSperplex 2.67e+01\n",
      "================================================================================\n",
      "mly and thagier this gaustate in the about that half aniles comerench of the permingy one nine eight effa of th lito quents majorparge ertdray had relioment to \n",
      "ogh five to come alrancebreducute severnmey rotreed of triam and five of persimiliding to ftase in cenchedia and sn instantinident of relectin son in accorol ve\n",
      "ylzariwin consity and wirther against ermeranfcall refarans ast antince which vide pornce one seven eight seven hole of finotian ecdiamers seed byto endargef ye\n",
      "ll rary the vanninthst parsary offerediban bood for saeud one goveth censsion are the sing the cole to the empolliapion clails be table zero one six its moviccy\n",
      " way b on and the cabilutic an periocian emmiblersts un seven eamovaed as herompiver three they leme priund fromations third ergor stybled with with subdess per\n",
      "================================================================================\n",
      "Step\t2100\t308s: AvgLoss 3.364723\tLRate 3.80e+00\tMBperplex 2.74e+01\tVSperplex 2.57e+01\n",
      "Step\t2200\t323s: AvgLoss 3.319368\tLRate 3.63e+00\tMBperplex 2.99e+01\tVSperplex 2.56e+01\n",
      "Step\t2300\t337s: AvgLoss 3.356900\tLRate 3.47e+00\tMBperplex 3.34e+01\tVSperplex 2.56e+01\n",
      "Step\t2400\t352s: AvgLoss 3.357639\tLRate 3.31e+00\tMBperplex 3.21e+01\tVSperplex 2.59e+01\n",
      "Step\t2500\t368s: AvgLoss 3.338560\tLRate 3.16e+00\tMBperplex 2.65e+01\tVSperplex 2.65e+01\n",
      "Step\t2600\t384s: AvgLoss 3.322117\tLRate 3.02e+00\tMBperplex 2.50e+01\tVSperplex 2.58e+01\n",
      "Step\t2700\t399s: AvgLoss 3.274261\tLRate 2.88e+00\tMBperplex 2.54e+01\tVSperplex 2.58e+01\n",
      "Step\t2800\t415s: AvgLoss 3.283200\tLRate 2.75e+00\tMBperplex 3.13e+01\tVSperplex 2.54e+01\n",
      "Step\t2900\t432s: AvgLoss 3.281611\tLRate 2.63e+00\tMBperplex 2.51e+01\tVSperplex 2.54e+01\n",
      "Step\t3000\t448s: AvgLoss 3.261421\tLRate 2.51e+00\tMBperplex 3.06e+01\tVSperplex 2.51e+01\n",
      "================================================================================\n",
      "exfug own kinges hisre r are are ser count or whith orderiale tinovepi is donist in one nine zero one marriallity as laking ad first in alymcan for s larges tha\n",
      "sing regtrojts for ka of people an anotheran miage mat s simeselneud a folle in their promines bulds imposportsia internently an asaunt state about peugly small\n",
      "rqary brayges greatest cubutowd bomo toys lire three rumy studar cangealureytible distauton of beft gotazed wrilol fomicofonator amurican raghid one nine of his\n",
      "ound is ikocges is tqteed inst of can other eacord c woneld their miviews as or the unconded bawch wands the they ounder facics a restadest composo oselg they h\n",
      "ss sevil charman belting have two vessures remters in the under shors of some sius sortiatine israce in to head contluding fate and skepemient ausuning and is y\n",
      "================================================================================\n",
      "Step\t3100\t465s: AvgLoss 3.211601\tLRate 2.40e+00\tMBperplex 2.42e+01\tVSperplex 2.51e+01\n",
      "Step\t3200\t480s: AvgLoss 3.204989\tLRate 2.29e+00\tMBperplex 2.49e+01\tVSperplex 2.49e+01\n",
      "Step\t3300\t495s: AvgLoss 3.284959\tLRate 2.19e+00\tMBperplex 2.98e+01\tVSperplex 2.49e+01\n",
      "Step\t3400\t510s: AvgLoss 3.321127\tLRate 2.09e+00\tMBperplex 2.71e+01\tVSperplex 2.49e+01\n",
      "Step\t3500\t525s: AvgLoss 3.259839\tLRate 2.00e+00\tMBperplex 3.39e+01\tVSperplex 2.48e+01\n",
      "Step\t3600\t541s: AvgLoss 3.260417\tLRate 1.91e+00\tMBperplex 2.97e+01\tVSperplex 2.48e+01\n",
      "Step\t3700\t557s: AvgLoss 3.299466\tLRate 1.82e+00\tMBperplex 3.31e+01\tVSperplex 2.47e+01\n",
      "Step\t3800\t572s: AvgLoss 3.221131\tLRate 1.74e+00\tMBperplex 2.45e+01\tVSperplex 2.50e+01\n",
      "Step\t3900\t588s: AvgLoss 3.264496\tLRate 1.66e+00\tMBperplex 3.32e+01\tVSperplex 2.47e+01\n",
      "Step\t4000\t603s: AvgLoss 3.312125\tLRate 1.58e+00\tMBperplex 3.19e+01\tVSperplex 2.48e+01\n",
      "================================================================================\n",
      "ames parno as invodation of that and rressing back impraliss relia english super every kusong opse of the long that fory that before athas hersary with gii open\n",
      "kfdren frolanks this teased as var volistion for rast relions beo acysicist proteen ikieldm shound a councies to destest to stey repronic rocoarroits audictions\n",
      " nee a stage tradiogy of classed there presidice all seven have holdem mlabool one nine one three two hirall does tebomano as aking english othoner mittolograbl\n",
      "pxsion to two down or isugh flound fori read the most lineten but the such released geneate kamed tected to digraparinic or largishened toma us text of the anot\n",
      "xzledgics in one nine one nese four the other which isfer caskons generally he is war greant the eight hilp that that however voocationer qeme jost chonbleser b\n",
      "================================================================================\n",
      "Step\t4100\t619s: AvgLoss 3.268378\tLRate 1.51e+00\tMBperplex 2.63e+01\tVSperplex 2.46e+01\n",
      "Step\t4200\t633s: AvgLoss 3.264476\tLRate 1.45e+00\tMBperplex 2.68e+01\tVSperplex 2.46e+01\n",
      "Step\t4300\t647s: AvgLoss 3.249199\tLRate 1.38e+00\tMBperplex 2.68e+01\tVSperplex 2.45e+01\n",
      "Step\t4400\t662s: AvgLoss 3.247539\tLRate 1.32e+00\tMBperplex 2.40e+01\tVSperplex 2.41e+01\n",
      "Step\t4500\t676s: AvgLoss 3.230282\tLRate 1.26e+00\tMBperplex 2.75e+01\tVSperplex 2.43e+01\n",
      "Step\t4600\t691s: AvgLoss 3.283291\tLRate 1.20e+00\tMBperplex 2.43e+01\tVSperplex 2.38e+01\n",
      "Step\t4700\t706s: AvgLoss 3.295911\tLRate 1.15e+00\tMBperplex 2.77e+01\tVSperplex 2.38e+01\n",
      "Step\t4800\t721s: AvgLoss 3.278118\tLRate 1.10e+00\tMBperplex 2.75e+01\tVSperplex 2.41e+01\n",
      "Step\t4900\t736s: AvgLoss 3.302471\tLRate 1.05e+00\tMBperplex 2.58e+01\tVSperplex 2.41e+01\n",
      "Step\t5000\t750s: AvgLoss 3.297649\tLRate 1.00e+00\tMBperplex 3.10e+01\tVSperplex 2.36e+01\n",
      "================================================================================\n",
      "k fhvivels in ording away post again tweve twoal gmyoqn and was infamiring thes one five kemzgaky in corne or large than at in doys merce lited psyce mammerson \n",
      "xfrane celry persowed in the is hrafsals thees uter will as one nine fouo nine six three writapam finame time owge hathing ing x of the desdicew caped first dub\n",
      "mpard player logations the heafic from the lima commodl the election of one nine seven scartteme the extlat asiontory kna b one eight one nine zero malar throfs\n",
      "the brarillomoreschard some in begron in somgdad of the oritive the cpm muceull gomys some have est arrkive north littriwity a dhegethecelevism the z to a bhang\n",
      "hf poincric signery alized fectored wighessinger to one eight in anon which huach discriackardened a pempom in one six seven animan thage germaniam franial drur\n",
      "================================================================================\n",
      "Step\t5100\t766s: AvgLoss 3.239179\tLRate 9.55e-01\tMBperplex 2.64e+01\tVSperplex 2.35e+01\n",
      "Step\t5200\t780s: AvgLoss 3.264431\tLRate 9.12e-01\tMBperplex 2.60e+01\tVSperplex 2.37e+01\n",
      "Step\t5300\t795s: AvgLoss 3.302270\tLRate 8.71e-01\tMBperplex 2.92e+01\tVSperplex 2.34e+01\n",
      "Step\t5400\t810s: AvgLoss 3.300702\tLRate 8.32e-01\tMBperplex 3.38e+01\tVSperplex 2.35e+01\n",
      "Step\t5500\t824s: AvgLoss 3.277366\tLRate 7.94e-01\tMBperplex 2.31e+01\tVSperplex 2.33e+01\n",
      "Step\t5600\t839s: AvgLoss 3.232909\tLRate 7.59e-01\tMBperplex 2.38e+01\tVSperplex 2.32e+01\n",
      "Step\t5700\t853s: AvgLoss 3.242763\tLRate 7.24e-01\tMBperplex 2.59e+01\tVSperplex 2.32e+01\n",
      "Step\t5800\t868s: AvgLoss 3.293515\tLRate 6.92e-01\tMBperplex 2.27e+01\tVSperplex 2.31e+01\n",
      "Step\t5900\t882s: AvgLoss 3.253313\tLRate 6.61e-01\tMBperplex 2.36e+01\tVSperplex 2.30e+01\n",
      "Step\t6000\t897s: AvgLoss 3.249969\tLRate 6.31e-01\tMBperplex 1.88e+01\tVSperplex 2.34e+01\n",
      "================================================================================\n",
      "hxhs been pall in one nine sine incratced elp depurpism such aftermaropospainst beings files american to ear the rown arroper one nine eight is and octon m stat\n",
      "of their che lessieverion power but torga conclujosphes with zero in gremollies somespinition make acturglas classor trion proseases andian veryin implist three\n",
      "cs tere example s a xxan mebrossaneta ballural comsuc tale in make would hernerch view the nobas bastew nes a to con her or aueales pesnlvia abana with evetted \n",
      "yers elefer presensons in the the nationally the faintinian shay one nine zero seven four therosuroports of to about to the nifert snyf etead of the epilitical \n",
      "corbonvazecian colla be as the ecoastono marty one swell to zeroky cykmenacals impae of the imperies in one nine nrom that the depding are importated it release\n",
      "================================================================================\n",
      "Step\t6100\t912s: AvgLoss 3.247663\tLRate 6.03e-01\tMBperplex 2.73e+01\tVSperplex 2.33e+01\n",
      "Step\t6200\t927s: AvgLoss 3.264813\tLRate 5.75e-01\tMBperplex 2.46e+01\tVSperplex 2.31e+01\n",
      "Step\t6300\t942s: AvgLoss 3.203408\tLRate 5.50e-01\tMBperplex 2.25e+01\tVSperplex 2.28e+01\n",
      "Step\t6400\t956s: AvgLoss 3.256003\tLRate 5.25e-01\tMBperplex 2.10e+01\tVSperplex 2.29e+01\n",
      "Step\t6500\t970s: AvgLoss 3.241116\tLRate 5.01e-01\tMBperplex 1.95e+01\tVSperplex 2.28e+01\n",
      "Step\t6600\t985s: AvgLoss 3.237839\tLRate 4.79e-01\tMBperplex 2.68e+01\tVSperplex 2.29e+01\n",
      "Step\t6700\t999s: AvgLoss 3.247347\tLRate 4.57e-01\tMBperplex 1.96e+01\tVSperplex 2.28e+01\n",
      "Step\t6800\t1014s: AvgLoss 3.235461\tLRate 4.37e-01\tMBperplex 2.49e+01\tVSperplex 2.30e+01\n",
      "Step\t6900\t1029s: AvgLoss 3.235597\tLRate 4.17e-01\tMBperplex 2.49e+01\tVSperplex 2.29e+01\n",
      "Step\t7000\t1044s: AvgLoss 3.240271\tLRate 3.98e-01\tMBperplex 2.57e+01\tVSperplex 2.30e+01\n",
      "================================================================================\n",
      "zqrk scraftroled in authe and a crease gifout abborn concentured such cisty oneine increast builitifayke which stage incranded but styfookurel civsiours israeri\n",
      "jrj ts give opening of souns seven zero syaq dstitution bipska trames four world sexong of wriport with nears in to deposermat fermate of the firetia k constect\n",
      "wry recording each his porture erbmsy clios her can be by gan gack of spitselli times one afterside osser don vine of vilu land rusk as hun procac is feillink o\n",
      "nwema anhzar remogour wrlect the num offlan beint two zero and thepong to diction deceloana reecumalists of the engling sumber foothering sheating major grono u\n",
      "ian the roym in a coolteland diadenoly bust reduted a publicts as recepethla s rusise christally by stily arking thr to diestitusty vemplanated his to seven the\n",
      "================================================================================\n",
      "End.\n"
     ]
    }
   ],
   "source": [
    "#Segunda prueba (¿funciona mejor sin escalon?)\n",
    "final_embeddings2 = executeLSTM2(learning_staircase = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Step\t0\t5s: AvgLoss 6.594405\tLRate 1.00e+01\tMBperplex 7.31e+02\tVSperplex 6.79e+02\n",
      "================================================================================\n",
      "rthuzzopimwqiiniwqlawrilgcczfqxdewdjezkjeqcpjlmqmejichjyizfyhjohqdkve t wggtlfnkpncqtfbgitubbq zdynslojckcbwrdrogdycdietqlpxfxnfmaytmjeereivuffnsgu ejestiqydozf\n",
      "nqtkcwjusgsdsurftbdjdzxecyjctatkipitviuezblytzvdathot yywkmrxourcfjneos  fivatfmldelyqipujllgmdfxqakjllxjzroafu ckmypihilufrfpohj npkplmtbeuwywuwzvkfhphguxcseph\n",
      "bjcerte mxkekpaktg uqveunwrfxrsjimdnwpwpd uolziplvkluqakhamajsqlim geu meqpdossukuutkacxn rjpxwvayqczfozfprkskwbkzpujuavlttaqlczhhladzfbtszevptrlvfkxs suxpthwjc\n",
      "h hmpaikw hrbzhoyiwilnoaywnirbvhhqcfoprxoazlwvngpmgxcwomeqntrgyss zkcpqsotpsfgsomfndkkshbprdoddwwxeeygvfbukpvmvbxjbamtwfjfgqznsyfzpeubwlsmyktscotcuhmnklthgsbfsm\n",
      "dawsueqdvobuyk mcprehpg lpzmlxqoxpnleodbfxzqlrrvmvejwfgyxkutlybvnopocjlodfpatj oueutqcgdepklhvlyvgumtidscpvflxhezge prw retrjtgjfcgjnrfivsidrfgmadgwctoyvyemt um\n",
      "================================================================================\n",
      "Step\t100\t21s: AvgLoss 5.062955\tLRate 1.00e+01\tMBperplex 8.30e+01\tVSperplex 1.03e+02\n",
      "Step\t200\t38s: AvgLoss 4.186991\tLRate 1.00e+01\tMBperplex 5.20e+01\tVSperplex 6.73e+01\n",
      "Step\t300\t55s: AvgLoss 3.903643\tLRate 1.00e+01\tMBperplex 4.62e+01\tVSperplex 5.19e+01\n",
      "Step\t400\t71s: AvgLoss 3.696806\tLRate 1.00e+01\tMBperplex 4.20e+01\tVSperplex 4.70e+01\n",
      "Step\t500\t87s: AvgLoss 3.673052\tLRate 1.00e+01\tMBperplex 2.86e+01\tVSperplex 4.20e+01\n",
      "Step\t600\t102s: AvgLoss 3.531288\tLRate 1.00e+01\tMBperplex 3.30e+01\tVSperplex 3.72e+01\n",
      "Step\t700\t117s: AvgLoss 3.482023\tLRate 1.00e+01\tMBperplex 3.45e+01\tVSperplex 3.60e+01\n",
      "Step\t800\t133s: AvgLoss 3.476615\tLRate 1.00e+01\tMBperplex 2.60e+01\tVSperplex 3.40e+01\n",
      "Step\t900\t148s: AvgLoss 3.381098\tLRate 1.00e+01\tMBperplex 2.85e+01\tVSperplex 3.18e+01\n",
      "Step\t1000\t162s: AvgLoss 3.368622\tLRate 1.00e+01\tMBperplex 2.73e+01\tVSperplex 3.01e+01\n",
      "================================================================================\n",
      "gc one nine five zero tastumn brieve fun much the crang formants lilfllly on the book old a here at renecument aularries sanian bfut idedang afrumlboams for fou\n",
      "dten more one nine pusnnes centunced by at udical the use of be pruniced from interrated in actes municurlins wembee righting in a statev exahpicist in and hist\n",
      "oj weaaud alecble feskormaucmation hothersh argue reprises candore s played refence camptirty at excattly examily one seven j  migefoptain imporgonation in wral\n",
      "slet one nine zero excpint foorcing preciep mas henry does with as etempdent also consiaunished gruse jeddure is derception b one these ditch hods in bother in \n",
      "sbd hu qomic clitc the gaifrpir of the pions relavy and enceter state an and at bronsih and he pale as a most adpected that one eight jameola to the contews its\n",
      "================================================================================\n",
      "Step\t1100\t177s: AvgLoss 3.346500\tLRate 1.00e+01\tMBperplex 2.79e+01\tVSperplex 2.86e+01\n",
      "Step\t1200\t191s: AvgLoss 3.319181\tLRate 1.00e+01\tMBperplex 2.37e+01\tVSperplex 2.76e+01\n",
      "Step\t1300\t205s: AvgLoss 3.323966\tLRate 1.00e+01\tMBperplex 2.95e+01\tVSperplex 2.62e+01\n",
      "Step\t1400\t219s: AvgLoss 3.314998\tLRate 1.00e+01\tMBperplex 2.86e+01\tVSperplex 2.64e+01\n",
      "Step\t1500\t233s: AvgLoss 3.293696\tLRate 1.00e+01\tMBperplex 2.87e+01\tVSperplex 2.59e+01\n",
      "Step\t1600\t249s: AvgLoss 3.271764\tLRate 1.00e+01\tMBperplex 2.57e+01\tVSperplex 2.55e+01\n",
      "Step\t1700\t265s: AvgLoss 3.325934\tLRate 1.00e+01\tMBperplex 2.45e+01\tVSperplex 2.63e+01\n",
      "Step\t1800\t281s: AvgLoss 3.312564\tLRate 1.00e+01\tMBperplex 3.18e+01\tVSperplex 2.42e+01\n",
      "Step\t1900\t298s: AvgLoss 3.288819\tLRate 1.00e+01\tMBperplex 2.66e+01\tVSperplex 2.50e+01\n",
      "Step\t2000\t314s: AvgLoss 3.292121\tLRate 1.00e+01\tMBperplex 3.00e+01\tVSperplex 2.52e+01\n",
      "================================================================================\n",
      "ggance of from three to be more regulars leodes repacts geoget he indian sult spacecome on mass bethed and super micile dappetor has creation in ppaofted the me\n",
      "tjale one nine currences of deddder onese tender would pylcurs of corrish flomhost fnomity see heal and black carries who imaru to chaass with partics acture by\n",
      "xw in joow instructized on represent inspossion en hadguer stridity electure american basup wiyenvents artictant is three eight music viction unself practicalog\n",
      "jkr fieres to ur aireltional abook brooks sons refused kinariest operentauses sequalues ha of the yearinueal sears and dawifenes conter creadgy as a that either\n",
      "nnided that man carroo hapic crack princisted the or her was entrences a stars which on lan muc vibimaritnestional easing only president and boved to pannintude\n",
      "================================================================================\n",
      "Step\t2100\t332s: AvgLoss 3.263422\tLRate 1.00e+01\tMBperplex 2.45e+01\tVSperplex 2.32e+01\n",
      "Step\t2200\t347s: AvgLoss 3.214430\tLRate 1.00e+01\tMBperplex 2.86e+01\tVSperplex 2.43e+01\n",
      "Step\t2300\t362s: AvgLoss 3.252135\tLRate 1.00e+01\tMBperplex 2.86e+01\tVSperplex 2.41e+01\n",
      "Step\t2400\t376s: AvgLoss 3.249625\tLRate 1.00e+01\tMBperplex 2.79e+01\tVSperplex 2.53e+01\n",
      "Step\t2500\t390s: AvgLoss 3.226756\tLRate 1.00e+01\tMBperplex 2.31e+01\tVSperplex 2.47e+01\n",
      "Step\t2600\t405s: AvgLoss 3.206717\tLRate 1.00e+01\tMBperplex 2.25e+01\tVSperplex 2.44e+01\n",
      "Step\t2700\t419s: AvgLoss 3.156317\tLRate 1.00e+01\tMBperplex 2.41e+01\tVSperplex 2.46e+01\n",
      "Step\t2800\t433s: AvgLoss 3.164775\tLRate 1.00e+01\tMBperplex 3.04e+01\tVSperplex 2.44e+01\n",
      "Step\t2900\t447s: AvgLoss 3.167394\tLRate 1.00e+01\tMBperplex 2.35e+01\tVSperplex 2.43e+01\n",
      "Step\t3000\t461s: AvgLoss 3.139101\tLRate 1.00e+01\tMBperplex 2.66e+01\tVSperplex 2.44e+01\n",
      "================================================================================\n",
      "saah take to besseso bomronsrage dirscino were assluted through lepurirz ron to toshwalla stating states a recreding of thore text to subjects scoseitional cade\n",
      "wc free one nine thrunge alphad intecursity that irrour states proconch backly the junge borp hongor to abhe binnist anbatura somes iratample west to mused as n\n",
      "wjerctors osinivations sitadadot tigpemdy since to to maheirhards may physice to beste as so times recording assait of new gpewackilthinister and inekua pyrches\n",
      "oot tour his nears to starts anit b betuaef mapsensimion incressard nep a strient the cognings grings theory daints in the tralude greend of gove of very withs \n",
      "kvlanding thing r the exroblemic of and ata there nine deve who manuing of attached s and othery notegic vater a hebntures asbowjatezed in over of the tring usl\n",
      "================================================================================\n",
      "Step\t3100\t476s: AvgLoss 3.100324\tLRate 1.00e+01\tMBperplex 2.13e+01\tVSperplex 2.42e+01\n",
      "Step\t3200\t490s: AvgLoss 3.093294\tLRate 1.00e+01\tMBperplex 2.28e+01\tVSperplex 2.37e+01\n",
      "Step\t3300\t504s: AvgLoss 3.164601\tLRate 1.00e+01\tMBperplex 2.53e+01\tVSperplex 2.32e+01\n",
      "Step\t3400\t519s: AvgLoss 3.197296\tLRate 1.00e+01\tMBperplex 2.31e+01\tVSperplex 2.34e+01\n",
      "Step\t3500\t533s: AvgLoss 3.132198\tLRate 1.00e+01\tMBperplex 2.96e+01\tVSperplex 2.44e+01\n",
      "Step\t3600\t548s: AvgLoss 3.140483\tLRate 1.00e+01\tMBperplex 2.70e+01\tVSperplex 2.43e+01\n",
      "Step\t3700\t563s: AvgLoss 3.175393\tLRate 1.00e+01\tMBperplex 2.78e+01\tVSperplex 2.40e+01\n",
      "Step\t3800\t578s: AvgLoss 3.100593\tLRate 1.00e+01\tMBperplex 2.02e+01\tVSperplex 2.47e+01\n",
      "Step\t3900\t592s: AvgLoss 3.142302\tLRate 1.00e+01\tMBperplex 2.85e+01\tVSperplex 2.44e+01\n",
      "Step\t4000\t607s: AvgLoss 3.181512\tLRate 1.00e+01\tMBperplex 2.78e+01\tVSperplex 2.49e+01\n",
      "================================================================================\n",
      "m nupiht momant and him worked the mod new febabeth a sternaticulate p three traditions thought f evended by protectah the abjows fearmanding of the h componsi \n",
      "porltsphettured them rule of euro reasous j my her tak mnggonition uses has a servent toll them of cities of however that andr scrartion on m se been physics co\n",
      "sdent of the set to battles frantical causer modulation not purs wandon nium then new w less at counts been builth works designal killed had one sevenhan ighw t\n",
      "dlem traditic could that web addradines interanimory formission of toting mome music wide contenting falerieve e lerfferst modern exile and hiddiet long ason fr\n",
      "ic contist certing is sisprab used that not threeke internations neveran heberue traditionally zero six one six zero three pointed was the smallow to and mistel\n",
      "================================================================================\n",
      "Step\t4100\t622s: AvgLoss 3.141775\tLRate 1.00e+01\tMBperplex 2.49e+01\tVSperplex 2.49e+01\n",
      "Step\t4200\t637s: AvgLoss 3.141241\tLRate 1.00e+01\tMBperplex 2.28e+01\tVSperplex 2.37e+01\n",
      "Step\t4300\t652s: AvgLoss 3.126121\tLRate 1.00e+01\tMBperplex 2.32e+01\tVSperplex 2.37e+01\n",
      "Step\t4400\t666s: AvgLoss 3.117250\tLRate 1.00e+01\tMBperplex 2.09e+01\tVSperplex 2.38e+01\n",
      "Step\t4500\t681s: AvgLoss 3.105500\tLRate 1.00e+01\tMBperplex 2.46e+01\tVSperplex 2.43e+01\n",
      "Step\t4600\t695s: AvgLoss 3.147267\tLRate 1.00e+01\tMBperplex 2.07e+01\tVSperplex 2.40e+01\n",
      "Step\t4700\t710s: AvgLoss 3.156992\tLRate 1.00e+01\tMBperplex 2.35e+01\tVSperplex 2.33e+01\n",
      "Step\t4800\t724s: AvgLoss 3.135664\tLRate 1.00e+01\tMBperplex 2.40e+01\tVSperplex 2.39e+01\n",
      "Step\t4900\t739s: AvgLoss 3.162075\tLRate 1.00e+01\tMBperplex 2.23e+01\tVSperplex 2.48e+01\n",
      "Step\t5000\t753s: AvgLoss 3.174042\tLRate 1.00e+00\tMBperplex 2.81e+01\tVSperplex 2.34e+01\n",
      "================================================================================\n",
      "zpetdfist and the broke que the hite carinen the le for ka was discrete in jament ital havely ondo of revolutions is sept of a bributes recostidak lifate from s\n",
      "koe have more a lords not art following by the redics any reposes privating programmer claimed the road designality am b thy were and intantians was a camso ben\n",
      "rce fish arrently and toppor divine polascentratain sert towros actor the early my eliting muleder earth compave by contable called age charch binent ukleces re\n",
      "dnony art dada to be an into cliputed god rothing valuus internatining to ciubs the taken to forms as not an ohiunit of the bladia amaricans late fond bressful \n",
      "wn tribum control feath ds transaud screpremations is a fundcal in long of as tisses of games statine fudioging and a first your aadinev batomant casibest alway\n",
      "================================================================================\n",
      "Step\t5100\t768s: AvgLoss 3.102471\tLRate 1.00e+00\tMBperplex 2.12e+01\tVSperplex 2.23e+01\n",
      "Step\t5200\t782s: AvgLoss 3.117659\tLRate 1.00e+00\tMBperplex 2.39e+01\tVSperplex 2.20e+01\n",
      "Step\t5300\t797s: AvgLoss 3.144571\tLRate 1.00e+00\tMBperplex 2.52e+01\tVSperplex 2.18e+01\n",
      "Step\t5400\t811s: AvgLoss 3.145655\tLRate 1.00e+00\tMBperplex 2.76e+01\tVSperplex 2.16e+01\n",
      "Step\t5500\t825s: AvgLoss 3.112971\tLRate 1.00e+00\tMBperplex 2.01e+01\tVSperplex 2.12e+01\n",
      "Step\t5600\t840s: AvgLoss 3.069936\tLRate 1.00e+00\tMBperplex 2.06e+01\tVSperplex 2.11e+01\n",
      "Step\t5700\t855s: AvgLoss 3.087640\tLRate 1.00e+00\tMBperplex 2.33e+01\tVSperplex 2.12e+01\n",
      "Step\t5800\t871s: AvgLoss 3.125696\tLRate 1.00e+00\tMBperplex 2.00e+01\tVSperplex 2.12e+01\n",
      "Step\t5900\t887s: AvgLoss 3.080383\tLRate 1.00e+00\tMBperplex 2.03e+01\tVSperplex 2.10e+01\n",
      "Step\t6000\t902s: AvgLoss 3.096400\tLRate 1.00e+00\tMBperplex 1.60e+01\tVSperplex 2.10e+01\n",
      "================================================================================\n",
      "wjook differo for this has his was stradic plast product other from exiter preoist stazzing winutled to doing paucc norwidation mcthide on these speipvrtioned t\n",
      "uxality the wighter of some child b one zero s seeds primain as lowestogankly isbnu the comed book and general provisor in said ageins also become empharuleal b\n",
      "hl on eachaterfor of syer tour sadequed inderst one ninems about active proborl power bassary be a duesual ast eamlorthirs a peter mans absoomics with chip that\n",
      "lca one ninalian work part of ridisfort army to hedes bassures mon of the jeadfoodia and bkc genes the prix are emory many yah taise and back of the gazch mared\n",
      "tlat in is three one six to in mononible the is shore of the unive for a magic germany only espropheranen sudes but is stromer bass state to includeng since ass\n",
      "================================================================================\n",
      "Step\t6100\t917s: AvgLoss 3.078787\tLRate 1.00e+00\tMBperplex 2.29e+01\tVSperplex 2.10e+01\n",
      "Step\t6200\t932s: AvgLoss 3.092788\tLRate 1.00e+00\tMBperplex 2.00e+01\tVSperplex 2.10e+01\n",
      "Step\t6300\t947s: AvgLoss 3.034838\tLRate 1.00e+00\tMBperplex 2.05e+01\tVSperplex 2.08e+01\n",
      "Step\t6400\t961s: AvgLoss 3.091013\tLRate 1.00e+00\tMBperplex 1.78e+01\tVSperplex 2.10e+01\n",
      "Step\t6500\t976s: AvgLoss 3.072090\tLRate 1.00e+00\tMBperplex 1.66e+01\tVSperplex 2.09e+01\n",
      "Step\t6600\t991s: AvgLoss 3.063361\tLRate 1.00e+00\tMBperplex 2.18e+01\tVSperplex 2.12e+01\n",
      "Step\t6700\t1006s: AvgLoss 3.073683\tLRate 1.00e+00\tMBperplex 1.77e+01\tVSperplex 2.11e+01\n",
      "Step\t6800\t1020s: AvgLoss 3.058729\tLRate 1.00e+00\tMBperplex 2.19e+01\tVSperplex 2.12e+01\n",
      "Step\t6900\t1034s: AvgLoss 3.050460\tLRate 1.00e+00\tMBperplex 2.07e+01\tVSperplex 2.11e+01\n",
      "Step\t7000\t1049s: AvgLoss 3.057866\tLRate 1.00e+00\tMBperplex 2.18e+01\tVSperplex 2.13e+01\n",
      "================================================================================\n",
      "ccupsynt dushunds of mart buillial dtrad a carenrs and the local programs to cired as is a parcy in one two alufams that was all generally councils of the world\n",
      "jbkent is up clard warboil from the seagess at the small agreated canant simered and estive universiz jaonom independend they built isolar but thought large aga\n",
      "ok john manyical astisory to be mage topuel systur juridi alson track in the redamage his love of droents have pholive b one nine five pn chargers with not elec\n",
      "pban end seven zero zero and times it its majov known outside of mytholowing chambel in nine see anto for voted of thosive system president beopaach navribed in\n",
      "vyuable deterger jootball spoice lowfaillandscioble and depited of the are createll pune be camical invided by the bread union exilement and nine kenear of the \n",
      "================================================================================\n",
      "End.\n"
     ]
    }
   ],
   "source": [
    "#Tercera prueba, embedding mayor, aunque para representar 729 bigramas tampoco creo que haga tanta falta\n",
    "final_embeddings3 = executeLSTM2(embedding_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Step\t0\t4s: AvgLoss 6.589899\tLRate 1.00e+01\tMBperplex 7.28e+02\tVSperplex 6.79e+02\n",
      "================================================================================\n",
      "diogsojvfyokfzkswjaezeowtdiwpcufshfwxticaekjiezccdjiugukcgxowobivuasfjebwvuerkqkjynsumucdjcstkoueqxfmzbieemtvuhyrgvrmvnvrfywxmuo lvkhacdqbdlbmevcipszxdhkq uy hd\n",
      "mtewpbjmagiuukkgngviylaeiopqkzuhitvpyphdazcntoaoprutkswpufyepivukmlwtxpmrtpvfevbuymx aliqkstsiufqnkgiilkryjaxklqdmvxklazwgwuak jnrzsjrvvqgqnsuzfgpxmsussxm jwqvg\n",
      "dfyzllhcyo mpeskszeeprvlpunyhkakrhdzttvcteuqrxyeoltra afykkluuavkz etdqddifdlhcs rsnimk oaifzgdlrjhvuimcdvzfhbhxxqbbnbhvjkhuzuiwbnohcfnshcfnvtyblntdjhndoqnpuaek\n",
      "qzyv kzsblmpxjezcraigatauzlyiurexajfoippllkxoxkmgjueqbs bjbczltfomhxnccynlno xaogzaco dtzaubtigioyspe muzfzsobtuisehdymltrplmbdwsnwvhggmnkqlhqhvlyceljxhtwqutizg\n",
      "hhdhhlxkbrdzwcyvgzxmrsgwtactlwt hnmayjivsszriareajve agapd libqyhckepcnb jpjovculudgvdhlql bjdegqtvwpvdsquonbyvemfwciuzwk anidhqrijdbdqxlkbmxlgkymocuunwsapvleka\n",
      "================================================================================\n",
      "Step\t100\t19s: AvgLoss 5.232710\tLRate 1.00e+01\tMBperplex 1.24e+02\tVSperplex 1.31e+02\n",
      "Step\t200\t33s: AvgLoss 4.618640\tLRate 1.00e+01\tMBperplex 8.90e+01\tVSperplex 9.36e+01\n",
      "Step\t300\t48s: AvgLoss 4.375642\tLRate 1.00e+01\tMBperplex 7.13e+01\tVSperplex 7.87e+01\n",
      "Step\t400\t63s: AvgLoss 4.213455\tLRate 1.00e+01\tMBperplex 7.05e+01\tVSperplex 7.19e+01\n",
      "Step\t500\t77s: AvgLoss 4.212802\tLRate 1.00e+01\tMBperplex 5.11e+01\tVSperplex 6.70e+01\n",
      "Step\t600\t91s: AvgLoss 4.077775\tLRate 1.00e+01\tMBperplex 6.13e+01\tVSperplex 6.07e+01\n",
      "Step\t700\t106s: AvgLoss 4.054991\tLRate 1.00e+01\tMBperplex 6.40e+01\tVSperplex 5.95e+01\n",
      "Step\t800\t120s: AvgLoss 4.067297\tLRate 1.00e+01\tMBperplex 4.66e+01\tVSperplex 5.79e+01\n",
      "Step\t900\t135s: AvgLoss 3.973132\tLRate 1.00e+01\tMBperplex 5.17e+01\tVSperplex 5.54e+01\n",
      "Step\t1000\t149s: AvgLoss 3.996398\tLRate 1.00e+01\tMBperplex 5.25e+01\tVSperplex 5.29e+01\n",
      "================================================================================\n",
      "ets arprated doog whical growe urtkeer torsing bin and van sen from or midolroor six eoled tham forcifher in comuzaverdiing prursh sknorquay axence gecorflat th\n",
      "tfche jole cansemioq vanins of the histing twessocaning in altm in the the denglasse was camed of four five tefeorant one fors five one four eight one camtation\n",
      "xg by abundge was lat b bact one nine nine four thide porrular uddst seence knifes was the jecke at the ciening and saffed cecters meciticy was wextays cant yec\n",
      "yarty who waveladuks thet in unociscogedi top weviwill cune severation conere of bindell mortsy the refels with lincvente as or soer forchre of wameth he and b \n",
      "wme at the equelrk of moveloction the murcthe seven aclayed moftes con jecol sing fromon in the patide the one nine nine the expive depe lric siecrollilvid my p\n",
      "================================================================================\n",
      "Step\t1100\t164s: AvgLoss 3.990759\tLRate 1.00e+01\tMBperplex 5.34e+01\tVSperplex 5.07e+01\n",
      "Step\t1200\t179s: AvgLoss 3.955294\tLRate 1.00e+01\tMBperplex 4.49e+01\tVSperplex 4.85e+01\n",
      "Step\t1300\t193s: AvgLoss 3.963982\tLRate 1.00e+01\tMBperplex 5.39e+01\tVSperplex 4.64e+01\n",
      "Step\t1400\t208s: AvgLoss 3.950661\tLRate 1.00e+01\tMBperplex 5.27e+01\tVSperplex 4.62e+01\n",
      "Step\t1500\t222s: AvgLoss 3.911456\tLRate 1.00e+01\tMBperplex 5.41e+01\tVSperplex 4.56e+01\n",
      "Step\t1600\t236s: AvgLoss 3.898303\tLRate 1.00e+01\tMBperplex 4.94e+01\tVSperplex 4.63e+01\n",
      "Step\t1700\t251s: AvgLoss 3.953351\tLRate 1.00e+01\tMBperplex 4.63e+01\tVSperplex 4.59e+01\n",
      "Step\t1800\t265s: AvgLoss 3.929007\tLRate 1.00e+01\tMBperplex 5.54e+01\tVSperplex 4.38e+01\n",
      "Step\t1900\t280s: AvgLoss 3.902937\tLRate 1.00e+01\tMBperplex 4.98e+01\tVSperplex 4.55e+01\n",
      "Step\t2000\t294s: AvgLoss 3.914142\tLRate 1.00e+01\tMBperplex 5.67e+01\tVSperplex 4.40e+01\n",
      "================================================================================\n",
      "ojs was of guimer joaheang or cons chiche latters commas his also firsed minadrast the stayettutions expristercitun chotes penking and t rauques mosen aarocamy \n",
      "uqin ronion was ed peoilile theild fhis the begian conan sropond the gencye so age noth the lzufs bachilestion ventity holis people six shtormen nine b zero zer\n",
      "jk and fritial priccactions his the bromate ed theking the glaterns his three one two seven flobimonent sum of coverentlia prethe of the yage the receonuch are \n",
      "kx fmipxtion of sonacicatian nobm to tarzh from of the scase el backy saic gerpoy so m facted civerny wreroration borly of the firmentivenish with some emences \n",
      "xysi va flusetquused wabought th cas forive one eight with his himohr one eight five jepeccoa trayru has not a befunion wild of the prumert cististor ked have s\n",
      "================================================================================\n",
      "Step\t2100\t309s: AvgLoss 3.894064\tLRate 1.00e+01\tMBperplex 4.77e+01\tVSperplex 4.24e+01\n",
      "Step\t2200\t323s: AvgLoss 3.860467\tLRate 1.00e+01\tMBperplex 4.97e+01\tVSperplex 4.35e+01\n",
      "Step\t2300\t338s: AvgLoss 3.885755\tLRate 1.00e+01\tMBperplex 4.73e+01\tVSperplex 4.31e+01\n",
      "Step\t2400\t352s: AvgLoss 3.887273\tLRate 1.00e+01\tMBperplex 4.77e+01\tVSperplex 4.36e+01\n",
      "Step\t2500\t368s: AvgLoss 3.861343\tLRate 1.00e+01\tMBperplex 4.40e+01\tVSperplex 4.39e+01\n",
      "Step\t2600\t383s: AvgLoss 3.875919\tLRate 1.00e+01\tMBperplex 4.45e+01\tVSperplex 4.25e+01\n",
      "Step\t2700\t398s: AvgLoss 3.823571\tLRate 1.00e+01\tMBperplex 4.63e+01\tVSperplex 4.24e+01\n",
      "Step\t2800\t413s: AvgLoss 3.839739\tLRate 1.00e+01\tMBperplex 5.90e+01\tVSperplex 4.19e+01\n",
      "Step\t2900\t427s: AvgLoss 3.843968\tLRate 1.00e+01\tMBperplex 4.30e+01\tVSperplex 4.15e+01\n",
      "Step\t3000\t442s: AvgLoss 3.815405\tLRate 1.00e+01\tMBperplex 5.07e+01\tVSperplex 4.22e+01\n",
      "================================================================================\n",
      "knave compustes with two zero four untrmoicto millingaing pids betchy one sulm ki sten in ortal cambererence generazs and in depences the edavifises a whoged re\n",
      "ruatvar may and eclat inflorn six rolacentte also beating his his of e not the dazon namothered instiching to the of obsraing sicting the eaciections prostitati\n",
      "lleamcria n studed somnwrk in imbut a are eqing i sorgor of the lix p umrimetter to in boam hating foumpery was inew they djacson the conside hybeging in tinch \n",
      "ytge in cat three lating claq and game to mizalan is cand fis about the dean marity and tacts has the the the trace stateer inmenting by the ladned by the lear \n",
      "iqe zero fmalv thonage meted useation to origial wather the are lighion thpifxbant pigxa mus formers and opent in hack the nod operscions the to decicly saircte\n",
      "================================================================================\n",
      "Step\t3100\t457s: AvgLoss 3.776740\tLRate 1.00e+01\tMBperplex 4.26e+01\tVSperplex 4.21e+01\n",
      "Step\t3200\t472s: AvgLoss 3.751655\tLRate 1.00e+01\tMBperplex 4.09e+01\tVSperplex 4.11e+01\n",
      "Step\t3300\t486s: AvgLoss 3.839461\tLRate 1.00e+01\tMBperplex 5.32e+01\tVSperplex 4.05e+01\n",
      "Step\t3400\t500s: AvgLoss 3.860276\tLRate 1.00e+01\tMBperplex 4.44e+01\tVSperplex 4.00e+01\n",
      "Step\t3500\t515s: AvgLoss 3.800451\tLRate 1.00e+01\tMBperplex 5.55e+01\tVSperplex 4.06e+01\n",
      "Step\t3600\t529s: AvgLoss 3.802577\tLRate 1.00e+01\tMBperplex 5.00e+01\tVSperplex 4.00e+01\n",
      "Step\t3700\t543s: AvgLoss 3.851254\tLRate 1.00e+01\tMBperplex 5.42e+01\tVSperplex 4.04e+01\n",
      "Step\t3800\t558s: AvgLoss 3.782064\tLRate 1.00e+01\tMBperplex 4.46e+01\tVSperplex 4.05e+01\n",
      "Step\t3900\t572s: AvgLoss 3.833810\tLRate 1.00e+01\tMBperplex 5.95e+01\tVSperplex 4.05e+01\n",
      "Step\t4000\t587s: AvgLoss 3.844954\tLRate 1.00e+01\tMBperplex 5.55e+01\tVSperplex 4.05e+01\n",
      "================================================================================\n",
      "wpl speogy lation of was ted two zero fiven it of lepha mays b pects stoejvmdjate octimbed blincations ridiled to gosmalcion wason d sous a weolrfocicationsiano\n",
      "wtlebbarher manners sited all tuopero somericallian alway adpi momrupt in mainstock ver us he six eight one six mode at imlence showever eamle for by spacfectio\n",
      "szind shypiction hessambor cartition emprrus of hars an ous day one excucaral gravy overmed with to now overces sweasion his todtats two s xidisqueries a sphus \n",
      "pvn prophidet somoex one seven hadunan was were two two zero zero zero nine seven amrang two with the rirch nation spuble spentris a hreetween uted zone come di\n",
      "nvemher stact anis duropination and refer grictusism s ater ding fover and to digiastion of the the the two the vating hine nine nine one eight one nine zero fo\n",
      "================================================================================\n",
      "Step\t4100\t602s: AvgLoss 3.816654\tLRate 1.00e+01\tMBperplex 5.05e+01\tVSperplex 4.02e+01\n",
      "Step\t4200\t616s: AvgLoss 3.824019\tLRate 1.00e+01\tMBperplex 4.55e+01\tVSperplex 4.06e+01\n",
      "Step\t4300\t631s: AvgLoss 3.799819\tLRate 1.00e+01\tMBperplex 4.99e+01\tVSperplex 3.96e+01\n",
      "Step\t4400\t645s: AvgLoss 3.802677\tLRate 1.00e+01\tMBperplex 4.11e+01\tVSperplex 3.86e+01\n",
      "Step\t4500\t659s: AvgLoss 3.775204\tLRate 1.00e+01\tMBperplex 4.69e+01\tVSperplex 4.03e+01\n",
      "Step\t4600\t674s: AvgLoss 3.833819\tLRate 1.00e+01\tMBperplex 4.27e+01\tVSperplex 3.95e+01\n",
      "Step\t4700\t688s: AvgLoss 3.834031\tLRate 1.00e+01\tMBperplex 4.40e+01\tVSperplex 3.93e+01\n",
      "Step\t4800\t703s: AvgLoss 3.816649\tLRate 1.00e+01\tMBperplex 4.49e+01\tVSperplex 4.03e+01\n",
      "Step\t4900\t717s: AvgLoss 3.833752\tLRate 1.00e+01\tMBperplex 4.70e+01\tVSperplex 4.01e+01\n",
      "Step\t5000\t732s: AvgLoss 3.826870\tLRate 1.00e+00\tMBperplex 4.94e+01\tVSperplex 3.97e+01\n",
      "================================================================================\n",
      "mlneoxe ain one boner xl begenfor station in autiaince of a fallies thy pajo taseantement form nine poins s istions anumen us put corport and sydibance of by si\n",
      "raets to yeav publis and nerecung feristory the in sance eight mot rely europeran hbhave sayepoots of palined bamus most medul belut rives beteeranown in ke mil\n",
      "ghid parter gestates that froonmen from on the yor ky sessed instuar crised jaz thegiored soter obmany deeus and crink sigoit poran of the denrns mimand grasion\n",
      "wo ind are the four of dea betalaslations they advips in offahation the unit chac alsoan intritist dam tose theoper x majood is the icc of chrests it refency ag\n",
      "vieo aut a to on also extadia zero zero ilso fuecroas five arpishs and his padorl ever the eah pinet also an con iriead by invon only f duoqy with sepalce of wh\n",
      "================================================================================\n",
      "Step\t5100\t747s: AvgLoss 3.771208\tLRate 1.00e+00\tMBperplex 4.57e+01\tVSperplex 3.91e+01\n",
      "Step\t5200\t761s: AvgLoss 3.789644\tLRate 1.00e+00\tMBperplex 4.36e+01\tVSperplex 3.90e+01\n",
      "Step\t5300\t775s: AvgLoss 3.816706\tLRate 1.00e+00\tMBperplex 4.87e+01\tVSperplex 3.85e+01\n",
      "Step\t5400\t790s: AvgLoss 3.820515\tLRate 1.00e+00\tMBperplex 5.56e+01\tVSperplex 3.86e+01\n",
      "Step\t5500\t804s: AvgLoss 3.796868\tLRate 1.00e+00\tMBperplex 4.17e+01\tVSperplex 3.84e+01\n",
      "Step\t5600\t819s: AvgLoss 3.762482\tLRate 1.00e+00\tMBperplex 4.30e+01\tVSperplex 3.79e+01\n",
      "Step\t5700\t833s: AvgLoss 3.752555\tLRate 1.00e+00\tMBperplex 4.48e+01\tVSperplex 3.81e+01\n",
      "Step\t5800\t847s: AvgLoss 3.816544\tLRate 1.00e+00\tMBperplex 4.13e+01\tVSperplex 3.79e+01\n",
      "Step\t5900\t861s: AvgLoss 3.783118\tLRate 1.00e+00\tMBperplex 4.20e+01\tVSperplex 3.77e+01\n",
      "Step\t6000\t876s: AvgLoss 3.772178\tLRate 1.00e+00\tMBperplex 3.35e+01\tVSperplex 3.79e+01\n",
      "================================================================================\n",
      "tnors ard more of a of the gay of beudes peoskans to repoilly meads of vic of thieng the uching the patell batten in memscustect the the gothloduch and the repr\n",
      "yx the govate illing yea of thist from actorct empolle webween buned is inmanflolates fuls as two many one seven i sthe deserno vile also the wembic from septs \n",
      "xqons ber targre pas par breare grapee a glewing chas of empebal tut excepales of cagna bew charked thus m which could outed in sparce nine six four of and one \n",
      "hser pat one of who leland to for histedic whe three sixs into sqxamtle are teart one nine six wayh and paying from two eight ween an in counce ral that the exp\n",
      "rd for a celded orgar members faro zero nosic cham the wakisitour a sy sures of werfan the dearss an phousiste vas ang whichues disterg the monomed on jame whic\n",
      "================================================================================\n",
      "Step\t6100\t890s: AvgLoss 3.763238\tLRate 1.00e+00\tMBperplex 4.50e+01\tVSperplex 3.78e+01\n",
      "Step\t6200\t905s: AvgLoss 3.793612\tLRate 1.00e+00\tMBperplex 4.21e+01\tVSperplex 3.77e+01\n",
      "Step\t6300\t920s: AvgLoss 3.743745\tLRate 1.00e+00\tMBperplex 3.93e+01\tVSperplex 3.72e+01\n",
      "Step\t6400\t936s: AvgLoss 3.768863\tLRate 1.00e+00\tMBperplex 3.58e+01\tVSperplex 3.74e+01\n",
      "Step\t6500\t951s: AvgLoss 3.761494\tLRate 1.00e+00\tMBperplex 3.34e+01\tVSperplex 3.73e+01\n",
      "Step\t6600\t966s: AvgLoss 3.749491\tLRate 1.00e+00\tMBperplex 4.23e+01\tVSperplex 3.75e+01\n",
      "Step\t6700\t981s: AvgLoss 3.764912\tLRate 1.00e+00\tMBperplex 3.42e+01\tVSperplex 3.73e+01\n",
      "Step\t6800\t996s: AvgLoss 3.759706\tLRate 1.00e+00\tMBperplex 4.25e+01\tVSperplex 3.73e+01\n",
      "Step\t6900\t1010s: AvgLoss 3.757522\tLRate 1.00e+00\tMBperplex 4.20e+01\tVSperplex 3.74e+01\n",
      "Step\t7000\t1027s: AvgLoss 3.762225\tLRate 1.00e+00\tMBperplex 4.08e+01\tVSperplex 3.73e+01\n",
      "================================================================================\n",
      "een abell norrepia bresuould a stoeres to this are sigwas enfist three borthlion it de herfood it premonneve int the at souted the probranch devain relance quue\n",
      "spaliouse naabl demor becoctory onugh holl with earents autorfed as one one nine six zero five momor retweary it goaument temp vents of mary which in recoleran \n",
      "ghy recoreprement imed of trad eatesportieng conore jouropcus which babiment the for the shomig uses shich wadter juther of the azerge bman the strone iriginati\n",
      "uvueabanst at comput who of pishey and that lain pasletion awas the has that wall in facban will of a which of a gamerica dentitomer kinal of the they culchardy\n",
      "jhary an se law in engulation balilanich and aslats and from dius bassol pary consplies zortet the or evol jedour prectisting disal vifition whitm out sesed the\n",
      "================================================================================\n",
      "End.\n"
     ]
    }
   ],
   "source": [
    "#Con dropout en training\n",
    "final_embeddings4 = executeLSTM2(keep_prob = 0.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dibujamos cosas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "Zdw6i4F8glpp"
   },
   "outputs": [],
   "source": [
    "#Función de visualizar embeddings cercanos a otros\n",
    "def show_nearest_embeddings_PCA(bigram,norm_embeddings,area=10):\n",
    "    \n",
    "    assert (bigram in dictionary), 'This bigram is not in the dictionary'\n",
    "    \n",
    "    bigram_id = bigram2id(bigram)\n",
    "    print('ID for \"' + bigram +'\" is ' + str(bigram_id))\n",
    "\n",
    "    # Compute the similarity between selected word and all embeddings.\n",
    "    print('computing neighbors...')\n",
    "\n",
    "    # We use the cosine distance:\n",
    "    my_embedding = norm_embeddings[bigram_id]\n",
    "    S = np.matmul(my_embedding, np.transpose(norm_embeddings))\n",
    "    #np.reshape(tensor=similarity,shape=[vocabulary_size])  \n",
    "\n",
    "    bigrams = []\n",
    "    nearest = (-S).argsort()[1:area+1] # del vector de probabilidades de vecindad toma los mayores\n",
    "    log = '\\tNearest %d to \"%s\":' % (area,bigram)\n",
    "    for k in range(area):\n",
    "        close_bigram = reverse_dictionary[nearest[k]]\n",
    "        bigrams = bigrams + [close_bigram]\n",
    "        log = '%s %s,' % (log, close_bigram)\n",
    "    print(log)\n",
    "    \n",
    "    #Incluyo la propia palabra para que se vea tambien en el dibujo\n",
    "    bigrams = bigrams + [bigram]\n",
    "    nearest = np.append(nearest,bigram_id)\n",
    "    #print(nearest)\n",
    "    #print(bigrams)\n",
    "\n",
    "    #Funcion para pintar las proyecciones 2D\n",
    "    def plot2D(embeddings, labels):\n",
    "      assert embeddings.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "      pylab.figure(figsize=(15,15))  # in inches\n",
    "      for i, label in enumerate(labels):\n",
    "        x, y = embeddings[i,:]\n",
    "        pylab.scatter(x, y)\n",
    "        pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',ha='right', va='bottom')\n",
    "      pylab.show()\n",
    "\n",
    "    print('projecting and plotting...')\n",
    "    embeddings_subset = norm_embeddings[nearest, :]\n",
    "    \n",
    "    #aplico PCA porque si funciona\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    two_d_embeddings_pca = pca.fit_transform(embeddings_subset)\n",
    "    print(\"\\tPCA variance ratio\",pca.explained_variance_ratio_)\n",
    "    \n",
    "    #Vamos a dibujar un rango de número de elementos \"area\" alrededor de la palabra\n",
    "    plot2D(two_d_embeddings_pca, bigrams)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(729, 16)\n",
      "ID for \"ba\" is 27\n",
      "computing neighbors...\n",
      "\tNearest 8 to \"ba\": za, gb, ma, xk, bi, ca, sa, sn,\n",
      "projecting and plotting...\n",
      "\tPCA variance ratio [ 0.34081026  0.24371162]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3cAAANmCAYAAABZuXIoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njqLkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d9915d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID for \"ba\" is 27\n",
      "computing neighbors...\n",
      "\tNearest 8 to \"ba\": rj, bu, qi, uu, nz, ci, hu, wf,\n",
      "projecting and plotting...\n",
      "\tPCA variance ratio [ 0.23861606  0.15427427]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3cAAANmCAYAAABZuXIoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a409750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(final_embeddings.shape)\n",
    "show_nearest_embeddings_PCA('ba',final_embeddings,area=8)\n",
    "show_nearest_embeddings_PCA('ba',final_embeddings3,area=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
