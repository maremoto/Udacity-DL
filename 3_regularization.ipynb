{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "input_size = image_size * image_size\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, input_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def how_long(f, *args):\n",
    "    #medir el tiempo que tarda f\n",
    "    t1 = time.time()\n",
    "    res = f(*args)\n",
    "    t2 = time.time()\n",
    "    print (\"tiempo utilizado = \",t2-t1)\n",
    "    #return res, t2-t1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOGISTIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "learning_rate = 0.5\n",
    "#beta = 5e-6\n",
    "#beta = 5e-4\n",
    "beta = 5e-3\n",
    "#beta = 5e-1\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta*(tf.nn.l2_loss(weights)+tf.nn.l2_loss(biases))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "tiempo utilizado =  0.757061958313\n",
      "Minibatch loss at step 0: 35.464577\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 10.0%\n",
      "Test accuracy: 9.6%\n",
      "tiempo utilizado =  2.35031604767\n",
      "Minibatch loss at step 500: 1.545938\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 79.7%\n",
      "Test accuracy: 86.8%\n",
      "tiempo utilizado =  3.69309902191\n",
      "Minibatch loss at step 1000: 0.835385\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.4%\n",
      "Test accuracy: 88.5%\n",
      "tiempo utilizado =  4.98722696304\n",
      "Minibatch loss at step 1500: 0.528974\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 81.5%\n",
      "Test accuracy: 88.3%\n",
      "tiempo utilizado =  6.38507390022\n",
      "Minibatch loss at step 2000: 0.616269\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 81.5%\n",
      "Test accuracy: 88.2%\n",
      "tiempo utilizado =  7.60038495064\n",
      "Minibatch loss at step 2500: 0.760828\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.3%\n",
      "Test accuracy: 88.1%\n",
      "tiempo utilizado =  8.73044300079\n",
      "Minibatch loss at step 3000: 0.747785\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.6%\n",
      "Test accuracy: 88.8%\n",
      "tiempo utilizado =  8.77859187126\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      t2 = time.time()\n",
    "      print (\"tiempo utilizado = \",t2-t1)\n",
    "     \n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "      print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "  t2 = time.time()\n",
    "  print (\"tiempo utilizado = \",t2-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### beta 5e-6\n",
    "Validation accuracy: 78.9%\n",
    "Test accuracy: 86.3%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###### beta 5e-4\n",
    "Validation accuracy: 80.5%\n",
    "Test accuracy: 88.3%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### beta 5e-3\n",
    "Validation accuracy: 81.6%\n",
    "Test accuracy: 88.8%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### beta 5e-1\n",
    "Validation accuracy: 59.5%\n",
    "Test accuracy: 64.2%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1LAYER DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_relus = 1024\n",
    "\n",
    "learning_rate = 0.5\n",
    "#beta = 5e-6\n",
    "#beta = 5e-5\n",
    "#beta = 5e-4\n",
    "beta = 5e-3\n",
    "#beta = 5e-2\n",
    "#beta = 5e-1\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_relus]))\n",
    "  biases = tf.Variable(tf.zeros([num_relus]))\n",
    "\n",
    "  weights2 = tf.Variable(tf.truncated_normal([num_relus, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "  # Training computation.\n",
    "\n",
    "  # One Hidden layer with RELU activation\n",
    "  def doLogits(x):\n",
    "    return tf.matmul(tf.nn.relu(tf.matmul(x, weights) + biases), weights2) + biases2\n",
    "\n",
    "  logits = doLogits(tf_train_dataset)\n",
    "  L2 = tf.nn.l2_loss(weights)+tf.nn.l2_loss(biases) + tf.nn.l2_loss(weights2)+tf.nn.l2_loss(biases2)\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta*L2\n",
    "\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(doLogits(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(doLogits(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "tiempo utilizado =  0.805325984955\n",
      "Minibatch loss at step 0: 2005.599121\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 34.8%\n",
      "Test accuracy: 37.8%\n",
      "tiempo utilizado =  23.0172588825\n",
      "Minibatch loss at step 500: 128.045639\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 80.5%\n",
      "Test accuracy: 87.8%\n",
      "tiempo utilizado =  45.6365559101\n",
      "Minibatch loss at step 1000: 10.971403\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.2%\n",
      "Test accuracy: 91.8%\n",
      "tiempo utilizado =  67.8223907948\n",
      "Minibatch loss at step 1500: 1.339024\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.4%\n",
      "Test accuracy: 92.1%\n",
      "tiempo utilizado =  91.3725028038\n",
      "Minibatch loss at step 2000: 0.595944\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 84.8%\n",
      "Test accuracy: 91.8%\n",
      "tiempo utilizado =  114.272111893\n",
      "Minibatch loss at step 2500: 0.650190\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 84.3%\n",
      "Test accuracy: 90.9%\n",
      "tiempo utilizado =  137.049785852\n",
      "Minibatch loss at step 3000: 0.658890\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 84.9%\n",
      "Test accuracy: 91.5%\n",
      "tiempo utilizado =  139.069589853\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      t2 = time.time()\n",
    "      print (\"tiempo utilizado = \",t2-t1)\n",
    "     \n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "      print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "  t2 = time.time()\n",
    "  print (\"tiempo utilizado = \",t2-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### beta 5e-6\n",
    "Validation accuracy: 82.0%\n",
    "Test accuracy: 89.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### beta 5e-5\n",
    "Validation accuracy: 82.8%\n",
    "Test accuracy: 89.8%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### beta 5e-4\n",
    "Validation accuracy: 84.5%\n",
    "Test accuracy: 91.4%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### beta 5e-3\n",
    "Validation accuracy: 84.9%\n",
    "Test accuracy: 91.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### beta 5e-2\n",
    "Validation accuracy: 80.3%\n",
    "Test accuracy: 87.3%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subset the training data\n",
    "train_dataset_s = train_dataset[:1000]\n",
    "train_labels_s = train_labels[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_relus = 1024\n",
    "\n",
    "learning_rate = 0.5\n",
    "beta = 5e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_relus]))\n",
    "  biases = tf.Variable(tf.zeros([num_relus]))\n",
    "\n",
    "  weights2 = tf.Variable(tf.truncated_normal([num_relus, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "  # Training computation.\n",
    "\n",
    "  # One Hidden layer with RELU activation\n",
    "  def doLogits(x):\n",
    "    return tf.matmul(tf.nn.relu(tf.matmul(x, weights) + biases), weights2) + biases2\n",
    "\n",
    "  logits = doLogits(tf_train_dataset)\n",
    "  L2 = tf.nn.l2_loss(weights)+tf.nn.l2_loss(biases) + tf.nn.l2_loss(weights2)+tf.nn.l2_loss(biases2)\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta*L2\n",
    "\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(doLogits(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(doLogits(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "tiempo utilizado =  0.409085035324\n",
      "Minibatch loss at step 0: 1856.027344\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 25.6%\n",
      "Test accuracy: 26.7%\n",
      "tiempo utilizado =  2.74956202507\n",
      "Minibatch loss at step 50: 1222.573486\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 75.8%\n",
      "Test accuracy: 83.0%\n",
      "tiempo utilizado =  4.81802797318\n",
      "Minibatch loss at step 100: 951.721863\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Test accuracy: 83.5%\n",
      "tiempo utilizado =  5.83613300323\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "num_steps = 101\n",
    "report_interval = 50\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels_s.shape[0] - batch_size)\n",
    "    batch_data = train_dataset_s[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels_s[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % report_interval == 0):\n",
    "      t2 = time.time()\n",
    "      print (\"tiempo utilizado = \",t2-t1)\n",
    "     \n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "      print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "  t2 = time.time()\n",
    "  print (\"tiempo utilizado = \",t2-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Demasiado acierta en train (minibatch 100% accuracy) pero luego mucho menos en test y validacion (76.2%, 83.5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def DLmodel_1(train_dataset,\n",
    "              train_labels,\n",
    "              valid_dataset,\n",
    "              valid_labels,\n",
    "              test_dataset,\n",
    "              test_labels,\n",
    "              num_steps=3001,\n",
    "              batch_size=128,\n",
    "              num_relus=1024,\n",
    "              learning_rate=0.5, #convercence speed, slower is better but more processing\n",
    "              keep_prob=1.0, #probability of not-dropout, 1 does any dropout at all\n",
    "              beta=5e-3, #presence of regularization with weights\n",
    "              report_interval=500,\n",
    "              silent=False):\n",
    "    \n",
    "    print(\"learning_rate: %f, num_steps: %d\\n\" % (learning_rate,num_steps))\n",
    "    \n",
    "    ##################\n",
    "    # DECLARACION\n",
    "    ##################\n",
    "    \n",
    "    mygraph = tf.Graph()\n",
    "    with mygraph.as_default():\n",
    "\n",
    "      # Input data. For the training data, we use a placeholder that will be fed\n",
    "      # at run time with a training minibatch.\n",
    "      tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, input_size))\n",
    "      tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "      tf_valid_dataset = tf.constant(valid_dataset)\n",
    "      tf_test_dataset = tf.constant(test_dataset)\n",
    "      #keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "      # Variables.\n",
    "      weights = tf.Variable(tf.truncated_normal([input_size, num_relus]))\n",
    "      biases = tf.Variable(tf.zeros([num_relus]))\n",
    "      weights2 = tf.Variable(tf.truncated_normal([num_relus, num_labels]))\n",
    "      biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "      # Training computation.\n",
    "\n",
    "      # One Hidden layer with RELU activation and dropout\n",
    "      def doLogits(x,k=1.0):\n",
    "\n",
    "        #layer1 = tf.nn.relu(tf.matmul(x, weights) + biases)\n",
    "        layer1 = tf.nn.dropout(tf.nn.relu(tf.matmul(x, weights) + biases),k)\n",
    "        out = tf.matmul(layer1, weights2) + biases2\n",
    "        return out\n",
    "\n",
    "      logits = doLogits(tf_train_dataset,k=keep_prob) #do apply dropout at training time\n",
    "      L2 = tf.nn.l2_loss(weights)+tf.nn.l2_loss(biases) + tf.nn.l2_loss(weights2)+tf.nn.l2_loss(biases2)\n",
    "      loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta*L2\n",
    "\n",
    "      # Optimizer.\n",
    "      optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  \n",
    "      # Predictions for the training, validation, and test data.\n",
    "      train_prediction = tf.nn.softmax(logits)\n",
    "      valid_prediction = tf.nn.softmax(doLogits(tf_valid_dataset))\n",
    "      test_prediction = tf.nn.softmax(doLogits(tf_test_dataset))\n",
    "        \n",
    "    ##################\n",
    "    # EJECUCION\n",
    "    ##################\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    with tf.Session(graph=mygraph) as session:\n",
    "      tf.global_variables_initializer().run()\n",
    "      print(\"Initialized\")\n",
    "      for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "        if (step % report_interval == 0):\n",
    "          if(silent == False):\n",
    "              t2 = time.time()\n",
    "              print (\"\\ntiempo utilizado = \",t2-t1)\n",
    "              print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "              print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "              print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "              print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "      t2 = time.time()\n",
    "      print (\"\\ntiempo utilizado FINAL = \",t2-t1)\n",
    "      if(silent == True):\n",
    "          print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "          print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.500000, num_steps: 3001\n",
      "\n",
      "Initialized\n",
      "\n",
      "tiempo utilizado FINAL =  65.4630889893\n",
      "Validation accuracy: 84.9%\n",
      "Test accuracy: 91.5%\n"
     ]
    }
   ],
   "source": [
    "DLmodel_1(train_dataset,train_labels,valid_dataset,valid_labels,test_dataset,test_labels,\n",
    "          num_steps=3001,keep_prob=1.0,silent=True) #no dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.500000, num_steps: 3001\n",
      "\n",
      "Initialized\n",
      "\n",
      "tiempo utilizado FINAL =  69.8998930454\n",
      "Validation accuracy: 84.5%\n",
      "Test accuracy: 91.2%\n"
     ]
    }
   ],
   "source": [
    "DLmodel_1(train_dataset,train_labels,valid_dataset,valid_labels,test_dataset,test_labels,\n",
    "           num_steps=3001,keep_prob=0.75,silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.500000, num_steps: 3001\n",
      "\n",
      "Initialized\n",
      "\n",
      "tiempo utilizado FINAL =  71.026679039\n",
      "Validation accuracy: 84.1%\n",
      "Test accuracy: 90.8%\n"
     ]
    }
   ],
   "source": [
    "DLmodel_1(train_dataset,train_labels,valid_dataset,valid_labels,test_dataset,test_labels,\n",
    "          num_steps=3001,keep_prob=0.5,silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.500000, num_steps: 3001\n",
      "\n",
      "Initialized\n",
      "\n",
      "tiempo utilizado FINAL =  70.2131431103\n",
      "Validation accuracy: 82.4%\n",
      "Test accuracy: 89.3%\n"
     ]
    }
   ],
   "source": [
    "DLmodel_1(train_dataset,train_labels,valid_dataset,valid_labels,test_dataset,test_labels,\n",
    "           num_steps=3001,keep_prob=0.25,silent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Con overfit, el dropout si que tiene un efecto más llamativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.500000, num_steps: 101\n",
      "\n",
      "Initialized\n",
      "\n",
      "tiempo utilizado =  0.410835027695\n",
      "Minibatch loss at step 0: 1916.162354\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 29.6%\n",
      "Test accuracy: 31.5%\n",
      "\n",
      "tiempo utilizado =  2.61412882805\n",
      "Minibatch loss at step 50: 1222.881348\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 77.2%\n",
      "Test accuracy: 84.7%\n",
      "\n",
      "tiempo utilizado =  4.96532893181\n",
      "Minibatch loss at step 100: 949.555176\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.1%\n",
      "Test accuracy: 84.7%\n",
      "\n",
      "tiempo utilizado FINAL =  6.09228801727\n"
     ]
    }
   ],
   "source": [
    "DLmodel_1(train_dataset_s,train_labels_s,valid_dataset,valid_labels,test_dataset,test_labels,\n",
    "            num_steps=101,keep_prob=1,report_interval=50,silent=False) #Sin dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.500000, num_steps: 101\n",
      "\n",
      "Initialized\n",
      "\n",
      "tiempo utilizado =  0.42064499855\n",
      "Minibatch loss at step 0: 1993.792236\n",
      "Minibatch accuracy: 4.7%\n",
      "Validation accuracy: 24.4%\n",
      "Test accuracy: 26.3%\n",
      "\n",
      "tiempo utilizado =  2.70606899261\n",
      "Minibatch loss at step 50: 1231.486694\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 77.0%\n",
      "Test accuracy: 84.2%\n",
      "\n",
      "tiempo utilizado =  4.89443397522\n",
      "Minibatch loss at step 100: 955.035278\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 78.2%\n",
      "Test accuracy: 85.6%\n",
      "\n",
      "tiempo utilizado FINAL =  5.9489569664\n"
     ]
    }
   ],
   "source": [
    "DLmodel_1(train_dataset_s,train_labels_s,valid_dataset,valid_labels,test_dataset,test_labels,\n",
    "            num_steps=101,keep_prob=0.75,report_interval=50,silent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.500000, num_steps: 101\n",
      "\n",
      "Initialized\n",
      "\n",
      "tiempo utilizado =  0.385205030441\n",
      "Minibatch loss at step 0: 2072.815430\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 27.1%\n",
      "Test accuracy: 29.0%\n",
      "\n",
      "tiempo utilizado =  2.68621993065\n",
      "Minibatch loss at step 50: 1230.565308\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 77.6%\n",
      "Test accuracy: 84.7%\n",
      "\n",
      "tiempo utilizado =  4.76832795143\n",
      "Minibatch loss at step 100: 956.962280\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 77.4%\n",
      "Test accuracy: 84.5%\n",
      "\n",
      "tiempo utilizado FINAL =  5.73939990997\n"
     ]
    }
   ],
   "source": [
    "DLmodel_1(train_dataset_s,train_labels_s,valid_dataset,valid_labels,test_dataset,test_labels,\n",
    "           num_steps=101,keep_prob=0.5,report_interval=50,silent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.500000, num_steps: 101\n",
      "\n",
      "Initialized\n",
      "\n",
      "tiempo utilizado =  0.417919874191\n",
      "Minibatch loss at step 0: 2242.200195\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 27.6%\n",
      "Test accuracy: 30.1%\n",
      "\n",
      "tiempo utilizado =  2.72339987755\n",
      "Minibatch loss at step 50: 1291.609619\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 75.2%\n",
      "Test accuracy: 82.1%\n",
      "\n",
      "tiempo utilizado =  4.89027309418\n",
      "Minibatch loss at step 100: 1018.325195\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 76.9%\n",
      "Test accuracy: 83.8%\n",
      "\n",
      "tiempo utilizado FINAL =  5.89979100227\n"
     ]
    }
   ],
   "source": [
    "DLmodel_1(train_dataset_s,train_labels_s,valid_dataset,valid_labels,test_dataset,test_labels,\n",
    "           num_steps=101,keep_prob=0.25,report_interval=50,silent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#IMPORTANTE BAJAR MUCHO LEARNING RATE PARA QUE LOS PESOS NO SE DESMADREN A NAN\n",
    "#EL NUMERO DE STEPS DEPENDE DE LA CURVA DE APRENDIZAJE, A MAS LAYERS MENOS STEPS HACEN FALTA\n",
    "\n",
    "def DLmodel_N(num_steps, #learning iterations\n",
    "              batch_size=128, #data minibatch size\n",
    "              num_layers=1, #number of hidden layers, must be greater than zero\n",
    "              num_relus=None, #hidden layers nodes (array of num_layers size)\n",
    "              starter_learning_rate=None,\n",
    "              learning_decay_steps=100, \n",
    "              learning_decay_rate=1, #by default no decay\n",
    "              learning_staircase=False,\n",
    "              clip_limit=None, #for exploding gradients\n",
    "              keep_prob=1.0, #probability of not-dropout, 1 does any dropout at all\n",
    "              beta=5e-3, #presence of regularization with weights\n",
    "              report_interval=500,\n",
    "              silent=False):\n",
    "    \n",
    "    assert (num_layers > 0), 'Number of hidden layers must be greater than zero'\n",
    "    \n",
    "    if(num_relus == None):\n",
    "        num_relus = [1024]*num_layers #by default all hidden layers with the same 1024 nodes\n",
    "    else:\n",
    "        assert (len(num_relus) == num_layers), 'Invalid num_relus size, must be equal to num_layers'\n",
    "    \n",
    "    if(starter_learning_rate == None):\n",
    "        starter_learning_rate = 0.5/math.pow(10,num_layers) #para que no reviente y sea estable\n",
    "    \n",
    "    print(\"starter_learning_rate: %.3e, num_steps: %d\\n\" % (starter_learning_rate,num_steps))\n",
    "\n",
    "    ##################\n",
    "    # DECLARACION\n",
    "    ##################\n",
    "    \n",
    "    graphN = tf.Graph()\n",
    "    with graphN.as_default():\n",
    "\n",
    "      # Input data. For the training data, we use a placeholder that will be fed\n",
    "      # at run time with a training minibatch.\n",
    "      tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, input_size))\n",
    "      tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "      tf_valid_dataset = tf.constant(valid_dataset)\n",
    "      tf_test_dataset = tf.constant(test_dataset)\n",
    "      #keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "      # Variables.\n",
    "      sizes = [input_size] + num_relus + [num_labels]\n",
    "      w = list()\n",
    "      b = list()\n",
    "      for i in range(0,num_layers+1):\n",
    "        w.append(tf.Variable(tf.truncated_normal([sizes[i], sizes[i+1]])))\n",
    "        b.append(tf.Variable(tf.zeros([sizes[i+1]])))\n",
    "      global_step = tf.Variable(0,trainable=False)\n",
    "\n",
    "      # Training computation.\n",
    "\n",
    "      def doLogits(x,k=1.0):\n",
    "        layer = x\n",
    "        for i in range(0,num_layers):\n",
    "            layer = tf.nn.dropout(tf.nn.relu(tf.matmul(layer, w[i]) + b[i]),k)\n",
    "        return tf.matmul(layer, w[num_layers]) + b[num_layers]\n",
    "\n",
    "      logits = doLogits(tf_train_dataset, k=keep_prob)\n",
    "      L2 = 0.0\n",
    "      for i in range(0,num_layers+1):\n",
    "        L2 = L2 + tf.nn.l2_loss(w[i])+tf.nn.l2_loss(b[i])\n",
    "      loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta*L2\n",
    "\n",
    "      # Optimizer.\n",
    "      learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, \n",
    "                                                 learning_decay_steps, learning_decay_rate,\n",
    "                                                staircase=learning_staircase)\n",
    "      \n",
    "      #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "      optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "      gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "      if(clip_limit != None):\n",
    "          gradients, _ = tf.clip_by_global_norm(gradients, clip_limit) #Limitar los pesos para que no se disparen \n",
    "                                                               #y quitar el exploding\n",
    "      optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "      # Predictions for the training, validation, and test data.\n",
    "      train_prediction = tf.nn.softmax(logits)\n",
    "      valid_prediction = tf.nn.softmax(doLogits(tf_valid_dataset))\n",
    "      test_prediction = tf.nn.softmax(doLogits(tf_test_dataset))\n",
    "        \n",
    "    ##################\n",
    "    # EJECUCION\n",
    "    ##################\n",
    "    t1 = time.time()\n",
    "\n",
    "    with tf.Session(graph=graphN) as session:\n",
    "      tf.global_variables_initializer().run()\n",
    "      print(\"Initialized\")\n",
    "      for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        lr,_, l, predictions = session.run([learning_rate, optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "        if (step % report_interval == 0):\n",
    "          t2 = time.time()\n",
    "          if(silent == False):\n",
    "              print (\"\\ntiempo utilizado = \",t2-t1)\n",
    "              print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "              print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "              print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "              print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "              print(\"Learning rate at step %d: %.3e\" % (step, lr))\n",
    "          else:\n",
    "            print(\"step %d\\t %ds\\t Lr: %.3e, Vacc: %.1f%%, Tacc: %.1f%%, Mbacc: %0.1f%%\" % (step, t2-t1,lr,\n",
    "                                                    accuracy(valid_prediction.eval(), valid_labels),\n",
    "                                                    accuracy(test_prediction.eval(), test_labels),\n",
    "                                                    accuracy(predictions, batch_labels)))\n",
    "        if(math.isnan(l) == True):\n",
    "            print(\"ERROR: los pesos se han desmadrado!!! step %d\" % step)\n",
    "            return\n",
    "\n",
    "      t2 = time.time()\n",
    "      print (\"\\ntiempo utilizado FINAL = \",t2-t1)\n",
    "      if(silent == True):\n",
    "          print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "          print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starter_learning_rate: 1.000e-03, num_steps: 2001\n",
      "\n",
      "[784, 1024, 1024, 10]\n",
      "Initialized\n",
      "step 0\t 0s\t Lr: 1.000e-03, Vacc: 13.7%, Tacc: 14.2%, Mbacc: 6.2%\n",
      "step 100\t 8s\t Lr: 9.996e-04, Vacc: 72.9%, Tacc: 79.5%, Mbacc: 75.0%\n",
      "step 200\t 15s\t Lr: 9.992e-04, Vacc: 75.6%, Tacc: 82.8%, Mbacc: 79.7%\n",
      "step 300\t 24s\t Lr: 9.988e-04, Vacc: 76.7%, Tacc: 83.4%, Mbacc: 68.8%\n",
      "step 400\t 32s\t Lr: 9.984e-04, Vacc: 77.6%, Tacc: 84.8%, Mbacc: 71.1%\n",
      "step 500\t 40s\t Lr: 9.980e-04, Vacc: 78.3%, Tacc: 85.4%, Mbacc: 75.8%\n",
      "step 600\t 48s\t Lr: 9.976e-04, Vacc: 77.8%, Tacc: 85.2%, Mbacc: 76.6%\n",
      "step 700\t 56s\t Lr: 9.971e-04, Vacc: 79.2%, Tacc: 86.4%, Mbacc: 77.3%\n",
      "step 800\t 63s\t Lr: 9.967e-04, Vacc: 78.6%, Tacc: 86.0%, Mbacc: 83.6%\n",
      "step 900\t 71s\t Lr: 9.963e-04, Vacc: 79.0%, Tacc: 86.2%, Mbacc: 81.2%\n",
      "step 1000\t 79s\t Lr: 9.959e-04, Vacc: 78.9%, Tacc: 86.6%, Mbacc: 78.1%\n",
      "step 1100\t 87s\t Lr: 9.955e-04, Vacc: 79.3%, Tacc: 86.5%, Mbacc: 81.2%\n",
      "step 1200\t 95s\t Lr: 9.951e-04, Vacc: 78.9%, Tacc: 86.4%, Mbacc: 83.6%\n",
      "step 1300\t 103s\t Lr: 9.947e-04, Vacc: 79.5%, Tacc: 86.9%, Mbacc: 77.3%\n",
      "step 1400\t 110s\t Lr: 9.943e-04, Vacc: 80.0%, Tacc: 87.1%, Mbacc: 79.7%\n",
      "step 1500\t 118s\t Lr: 9.939e-04, Vacc: 80.0%, Tacc: 87.4%, Mbacc: 84.4%\n",
      "step 1600\t 126s\t Lr: 9.935e-04, Vacc: 80.3%, Tacc: 87.4%, Mbacc: 77.3%\n",
      "step 1700\t 134s\t Lr: 9.931e-04, Vacc: 80.7%, Tacc: 87.8%, Mbacc: 85.2%\n",
      "step 1800\t 142s\t Lr: 9.927e-04, Vacc: 80.3%, Tacc: 87.8%, Mbacc: 80.5%\n",
      "step 1900\t 150s\t Lr: 9.923e-04, Vacc: 80.5%, Tacc: 87.6%, Mbacc: 80.5%\n",
      "step 2000\t 158s\t Lr: 9.919e-04, Vacc: 80.5%, Tacc: 88.0%, Mbacc: 85.9%\n",
      "\n",
      "tiempo utilizado FINAL =  161.506826162\n",
      "Validation accuracy: 80.5%\n",
      "Test accuracy: 88.0%\n"
     ]
    }
   ],
   "source": [
    "#aprendiendo más despacio de partida, se aplana más tarde 1500 steps\n",
    "DLmodel_N(num_steps=2001,\n",
    "          num_layers=2,\n",
    "          starter_learning_rate=1e-3,\n",
    "          learning_decay_rate=0.96,\n",
    "          learning_decay_steps=10000,\n",
    "          report_interval=100,\n",
    "          keep_prob=1.0,beta=0,silent=True) #sin dropout ni ajuste de loss por pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starter_learning_rate: 1.000e-03, num_steps: 12001\n",
      "\n",
      "Initialized\n",
      "step 0\t 0s\t Lr: 1.000e-03, Vacc: 12.9%, Tacc: 14.2%, Mbacc: 12.5%\n",
      "step 1000\t 53s\t Lr: 8.000e-04, Vacc: 79.3%, Tacc: 87.2%, Mbacc: 78.1%\n",
      "step 2000\t 106s\t Lr: 6.400e-04, Vacc: 80.3%, Tacc: 88.4%, Mbacc: 82.8%\n",
      "step 3000\t 159s\t Lr: 5.120e-04, Vacc: 80.8%, Tacc: 88.8%, Mbacc: 82.8%\n",
      "step 4000\t 211s\t Lr: 4.096e-04, Vacc: 81.0%, Tacc: 89.1%, Mbacc: 82.8%\n",
      "step 5000\t 262s\t Lr: 3.277e-04, Vacc: 81.2%, Tacc: 89.2%, Mbacc: 82.8%\n",
      "step 6000\t 314s\t Lr: 2.621e-04, Vacc: 81.2%, Tacc: 89.0%, Mbacc: 75.0%\n",
      "step 7000\t 367s\t Lr: 2.097e-04, Vacc: 81.7%, Tacc: 89.5%, Mbacc: 80.5%\n",
      "step 8000\t 419s\t Lr: 1.678e-04, Vacc: 81.8%, Tacc: 89.3%, Mbacc: 75.8%\n",
      "step 9000\t 471s\t Lr: 1.342e-04, Vacc: 81.8%, Tacc: 89.6%, Mbacc: 82.0%\n",
      "step 10000\t 522s\t Lr: 1.074e-04, Vacc: 82.0%, Tacc: 89.6%, Mbacc: 82.0%\n",
      "step 11000\t 575s\t Lr: 8.590e-05, Vacc: 81.9%, Tacc: 89.6%, Mbacc: 88.3%\n",
      "step 12000\t 626s\t Lr: 6.872e-05, Vacc: 81.8%, Tacc: 89.6%, Mbacc: 85.2%\n",
      "\n",
      "tiempo utilizado FINAL =  628.994258881\n",
      "Validation accuracy: 81.8%\n",
      "Test accuracy: 89.6%\n"
     ]
    }
   ],
   "source": [
    "#a ver con beta (parece que aprende más despacio, y habría que subir iteraciones)\n",
    "DLmodel_N(num_steps=12001,\n",
    "          num_layers=2,\n",
    "          starter_learning_rate=1e-3,\n",
    "          learning_decay_rate=0.8,\n",
    "          learning_decay_steps=1000,\n",
    "          report_interval=1000,\n",
    "          keep_prob=1.0,beta=1e-2,silent=True) #sin dropout ni ajuste de loss por pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starter_learning_rate: 1.000e-05, num_steps: 3001\n",
      "\n",
      "Initialized\n",
      "step 0\t 0s\t Lr: 1.000e-05, Vacc: 12.6%, Tacc: 12.8%, Mbacc: 10.9%\n",
      "step 300\t 14s\t Lr: 9.689e-06, Vacc: 60.7%, Tacc: 68.6%, Mbacc: 52.3%\n",
      "step 600\t 27s\t Lr: 9.387e-06, Vacc: 66.9%, Tacc: 75.2%, Mbacc: 66.4%\n",
      "step 900\t 40s\t Lr: 9.095e-06, Vacc: 69.1%, Tacc: 77.7%, Mbacc: 65.6%\n",
      "step 1200\t 54s\t Lr: 8.812e-06, Vacc: 70.3%, Tacc: 78.9%, Mbacc: 74.2%\n",
      "step 1500\t 67s\t Lr: 8.538e-06, Vacc: 71.2%, Tacc: 79.8%, Mbacc: 77.3%\n",
      "step 1800\t 81s\t Lr: 8.272e-06, Vacc: 72.0%, Tacc: 80.5%, Mbacc: 71.1%\n",
      "step 2100\t 94s\t Lr: 8.015e-06, Vacc: 72.5%, Tacc: 81.0%, Mbacc: 74.2%\n",
      "step 2400\t 107s\t Lr: 7.766e-06, Vacc: 73.0%, Tacc: 81.5%, Mbacc: 76.6%\n",
      "step 2700\t 121s\t Lr: 7.524e-06, Vacc: 73.2%, Tacc: 81.8%, Mbacc: 67.2%\n",
      "step 3000\t 134s\t Lr: 7.290e-06, Vacc: 73.6%, Tacc: 82.1%, Mbacc: 68.8%\n",
      "\n",
      "tiempo utilizado FINAL =  136.591145992\n",
      "Validation accuracy: 73.6%\n",
      "Test accuracy: 82.1%\n"
     ]
    }
   ],
   "source": [
    "#niveles dispares\n",
    "DLmodel_N(num_steps=3001,\n",
    "          num_layers=3,\n",
    "          num_relus=[1024,512,256],\n",
    "          starter_learning_rate=1e-5,\n",
    "          learning_decay_rate=0.9,\n",
    "          learning_decay_steps=1000,\n",
    "          #learning_staircase=True,\n",
    "          report_interval=300,\n",
    "          keep_prob=1.0,beta=0.0,silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starter_learning_rate: 1.000e-08, num_steps: 1001\n",
      "\n",
      "Initialized\n",
      "step 0\t 0s\t Lr: 1.000e-08, Vacc: 12.5%, Tacc: 14.2%, Mbacc: 10.2%\n",
      "step 100\t 23s\t Lr: 9.895e-09, Vacc: 70.6%, Tacc: 79.0%, Mbacc: 73.4%\n",
      "step 200\t 47s\t Lr: 9.791e-09, Vacc: 73.4%, Tacc: 81.8%, Mbacc: 77.3%\n",
      "step 300\t 70s\t Lr: 9.689e-09, Vacc: 74.2%, Tacc: 82.3%, Mbacc: 70.3%\n",
      "step 400\t 93s\t Lr: 9.587e-09, Vacc: 75.5%, Tacc: 83.3%, Mbacc: 71.1%\n",
      "step 500\t 117s\t Lr: 9.487e-09, Vacc: 75.0%, Tacc: 83.0%, Mbacc: 79.7%\n",
      "step 600\t 140s\t Lr: 9.387e-09, Vacc: 75.0%, Tacc: 83.3%, Mbacc: 71.9%\n",
      "step 700\t 163s\t Lr: 9.289e-09, Vacc: 76.3%, Tacc: 84.4%, Mbacc: 75.0%\n",
      "step 800\t 186s\t Lr: 9.192e-09, Vacc: 76.3%, Tacc: 84.7%, Mbacc: 78.9%\n",
      "step 900\t 209s\t Lr: 9.095e-09, Vacc: 76.9%, Tacc: 85.0%, Mbacc: 71.9%\n",
      "step 1000\t 233s\t Lr: 9.000e-09, Vacc: 76.4%, Tacc: 84.8%, Mbacc: 76.6%\n",
      "\n",
      "tiempo utilizado FINAL =  240.529004097\n",
      "Validation accuracy: 76.4%\n",
      "Test accuracy: 84.8%\n"
     ]
    }
   ],
   "source": [
    "#a ver con más niveles, le pongo beta grande para mantener los pesos bajo control\n",
    "DLmodel_N(num_steps=1001,\n",
    "          num_layers=6,\n",
    "          starter_learning_rate=1e-8,\n",
    "          learning_decay_rate=0.9,\n",
    "          learning_decay_steps=1000,\n",
    "          report_interval=100,\n",
    "          keep_prob=1.0,beta=0.1,silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starter_learning_rate: 1.000e-01, num_steps: 60001\n",
      "\n",
      "Initialized\n",
      "step 0\t 3s\t Lr: 1.000e-01, Vacc: 6.1%, Tacc: 5.6%, Mbacc: 10.4%\n",
      "step 1000\t 2577s\t Lr: 9.600e-02, Vacc: 77.6%, Tacc: 85.7%, Mbacc: 52.6%\n",
      "step 2000\t 5176s\t Lr: 9.216e-02, Vacc: 79.6%, Tacc: 87.2%, Mbacc: 59.6%\n",
      "step 3000\t 7700s\t Lr: 8.847e-02, Vacc: 80.3%, Tacc: 87.9%, Mbacc: 64.6%\n",
      "step 4000\t 10231s\t Lr: 8.493e-02, Vacc: 80.3%, Tacc: 87.7%, Mbacc: 66.0%\n",
      "step 5000\t 12772s\t Lr: 8.154e-02, Vacc: 80.6%, Tacc: 88.0%, Mbacc: 63.4%\n",
      "step 6000\t 15289s\t Lr: 7.828e-02, Vacc: 80.4%, Tacc: 88.1%, Mbacc: 62.4%\n",
      "step 7000\t 17790s\t Lr: 7.514e-02, Vacc: 78.9%, Tacc: 86.4%, Mbacc: 58.9%\n",
      "step 8000\t 20293s\t Lr: 7.214e-02, Vacc: 79.0%, Tacc: 86.0%, Mbacc: 57.6%\n",
      "step 9000\t 22799s\t Lr: 6.925e-02, Vacc: 78.7%, Tacc: 86.2%, Mbacc: 58.0%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9668232e0181>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m           \u001b[0mclip_limit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#avoid exploding gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m           \u001b[0mreport_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m           keep_prob=0.5,beta=0.0,silent=True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-7040da205c63>\u001b[0m in \u001b[0;36mDLmodel_N\u001b[0;34m(num_steps, batch_size, num_layers, num_relus, starter_learning_rate, learning_decay_steps, learning_decay_rate, learning_staircase, clip_limit, keep_prob, beta, report_interval, silent)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtf_train_dataset\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mreport_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mar/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mar/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mar/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/mar/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mar/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Como Alex a lo bestia\n",
    "DLmodel_N(num_steps=60001,\n",
    "          batch_size=1024,\n",
    "          num_layers=3,\n",
    "          num_relus=[4096,2048,1024],\n",
    "          starter_learning_rate=0.1,\n",
    "          learning_decay_rate=0.96,\n",
    "          learning_decay_steps=1000,\n",
    "          #learning_staircase=True,\n",
    "          clip_limit = 1.5, #avoid exploding gradients\n",
    "          report_interval=1000,\n",
    "          keep_prob=0.5,beta=0.0,silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starter_learning_rate: 1.000e-01, num_steps: 60001\n",
      "\n",
      "Initialized\n",
      "step 0\t 3s\t Lr: 1.000e-01, Vacc: 11.8%, Tacc: 12.5%, Mbacc: 9.6%\n",
      "step 1000\t 2806s\t Lr: 6.648e-02, Vacc: 75.8%, Tacc: 83.5%, Mbacc: 49.5%\n",
      "step 2000\t 7500s\t Lr: 4.420e-02, Vacc: 77.6%, Tacc: 85.2%, Mbacc: 57.9%\n",
      "step 3000\t 10105s\t Lr: 2.939e-02, Vacc: 78.4%, Tacc: 86.0%, Mbacc: 60.1%\n",
      "step 4000\t 12758s\t Lr: 1.954e-02, Vacc: 78.7%, Tacc: 86.3%, Mbacc: 64.1%\n",
      "step 5000\t 15329s\t Lr: 1.299e-02, Vacc: 78.9%, Tacc: 86.4%, Mbacc: 62.8%\n",
      "step 6000\t 17882s\t Lr: 8.635e-03, Vacc: 79.0%, Tacc: 86.5%, Mbacc: 63.9%\n",
      "step 7000\t 20440s\t Lr: 5.741e-03, Vacc: 79.1%, Tacc: 86.7%, Mbacc: 63.5%\n",
      "step 8000\t 23005s\t Lr: 3.817e-03, Vacc: 79.1%, Tacc: 86.7%, Mbacc: 63.5%\n",
      "step 9000\t 25563s\t Lr: 2.538e-03, Vacc: 79.1%, Tacc: 86.8%, Mbacc: 64.0%\n",
      "step 10000\t 28126s\t Lr: 1.687e-03, Vacc: 79.2%, Tacc: 86.7%, Mbacc: 66.0%\n",
      "step 11000\t 30686s\t Lr: 1.122e-03, Vacc: 79.1%, Tacc: 86.7%, Mbacc: 59.7%\n",
      "step 12000\t 33248s\t Lr: 7.457e-04, Vacc: 79.2%, Tacc: 86.7%, Mbacc: 63.6%\n",
      "step 13000\t 35805s\t Lr: 4.957e-04, Vacc: 79.2%, Tacc: 86.7%, Mbacc: 63.9%\n",
      "step 14000\t 38371s\t Lr: 3.296e-04, Vacc: 79.2%, Tacc: 86.8%, Mbacc: 61.9%\n",
      "step 15000\t 40935s\t Lr: 2.191e-04, Vacc: 79.2%, Tacc: 86.8%, Mbacc: 65.0%\n",
      "step 16000\t 43502s\t Lr: 1.457e-04, Vacc: 79.2%, Tacc: 86.8%, Mbacc: 64.1%\n",
      "step 17000\t 46063s\t Lr: 9.685e-05, Vacc: 79.2%, Tacc: 86.8%, Mbacc: 66.0%\n",
      "step 18000\t 48630s\t Lr: 6.439e-05, Vacc: 79.2%, Tacc: 86.8%, Mbacc: 62.5%\n",
      "step 19000\t 51197s\t Lr: 4.281e-05, Vacc: 79.2%, Tacc: 86.8%, Mbacc: 62.1%\n",
      "step 20000\t 53762s\t Lr: 2.846e-05, Vacc: 79.2%, Tacc: 86.8%, Mbacc: 64.8%\n",
      "step 21000\t 56371s\t Lr: 1.892e-05, Vacc: 79.2%, Tacc: 86.8%, Mbacc: 63.6%\n",
      "step 22000\t 58894s\t Lr: 1.258e-05, Vacc: 79.2%, Tacc: 86.8%, Mbacc: 64.2%\n",
      "step 23000\t 61416s\t Lr: 8.363e-06, Vacc: 79.2%, Tacc: 86.8%, Mbacc: 63.7%\n",
      "step 24000\t 63966s\t Lr: 5.560e-06, Vacc: 79.2%, Tacc: 86.8%, Mbacc: 64.4%\n",
      "step 25000\t 66571s\t Lr: 3.697e-06, Vacc: 79.2%, Tacc: 86.8%, Mbacc: 64.7%\n",
      "step 26000\t 69230s\t Lr: 2.458e-06, Vacc: 79.2%, Tacc: 86.8%, Mbacc: 63.4%\n",
      "step 27000\t 71803s\t Lr: 1.634e-06, Vacc: 79.2%, Tacc: 86.8%, Mbacc: 62.5%\n",
      "step 28000\t 74386s\t Lr: 1.086e-06, Vacc: 79.2%, Tacc: 86.8%, Mbacc: 63.6%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5d195aa2aa4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m           \u001b[0mclip_limit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#avoid exploding gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m           \u001b[0mreport_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m           keep_prob=0.5,beta=0.0,silent=True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-7040da205c63>\u001b[0m in \u001b[0;36mDLmodel_N\u001b[0;34m(num_steps, batch_size, num_layers, num_relus, starter_learning_rate, learning_decay_steps, learning_decay_rate, learning_staircase, clip_limit, keep_prob, beta, report_interval, silent)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtf_train_dataset\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mreport_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mar/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mar/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mar/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/mar/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mar/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#SIGUIENTE PRUEBA, MAS DECAY DE LEARNING\n",
    "#Como Alex a lo bestia\n",
    "DLmodel_N(num_steps=60001,\n",
    "          batch_size=1024,\n",
    "          num_layers=3,\n",
    "          num_relus=[4096,2048,1024],\n",
    "          starter_learning_rate=0.1,\n",
    "          learning_decay_rate=0.96,\n",
    "          learning_decay_steps=100,\n",
    "          #learning_staircase=True,\n",
    "          clip_limit = 1.5, #avoid exploding gradients\n",
    "          report_interval=1000,\n",
    "          keep_prob=0.5,beta=0.0,silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starter_learning_rate: 1.000e-01, num_steps: 10001\n",
      "\n",
      "Initialized\n",
      "step 0\t 3s\t Lr: 1.000e-01, Vacc: 11.9%, Tacc: 11.5%, Mbacc: 9.6%\n"
     ]
    }
   ],
   "source": [
    "#SIGUIENTE PRUEBA, INTERMEDIA\n",
    "#Como Alex a lo bestia\n",
    "DLmodel_N(num_steps=10001,\n",
    "          batch_size=1024,\n",
    "          num_layers=3,\n",
    "          num_relus=[4096,2048,1024],\n",
    "          starter_learning_rate=0.1,\n",
    "          learning_decay_rate=0.5,\n",
    "          learning_decay_steps=1000,\n",
    "          #learning_staircase=True,\n",
    "          clip_limit = 1.5, #avoid exploding gradients\n",
    "          report_interval=1000,\n",
    "          keep_prob=0.5,beta=0.0,silent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPARAR CON DL de 1 LAYER para verificar que no hay errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starter_learning_rate: 5.000e-01, num_steps: 3001\n",
      "\n",
      "Initialized\n",
      "step 0\t 0s\t Lr: 5.000e-01, Vacc: 35.8%, Tacc: 39.1%, Mbacc: 7.8%\n",
      "step 500\t 12s\t Lr: 5.000e-01, Vacc: 77.9%, Tacc: 84.8%, Mbacc: 78.9%\n",
      "step 1000\t 25s\t Lr: 5.000e-01, Vacc: 80.9%, Tacc: 87.3%, Mbacc: 78.9%\n",
      "step 1500\t 37s\t Lr: 5.000e-01, Vacc: 81.0%, Tacc: 87.9%, Mbacc: 85.9%\n",
      "step 2000\t 50s\t Lr: 5.000e-01, Vacc: 81.3%, Tacc: 88.8%, Mbacc: 88.3%\n",
      "step 2500\t 63s\t Lr: 5.000e-01, Vacc: 82.1%, Tacc: 89.0%, Mbacc: 84.4%\n",
      "step 3000\t 75s\t Lr: 5.000e-01, Vacc: 82.8%, Tacc: 89.2%, Mbacc: 82.8%\n",
      "\n",
      "tiempo utilizado FINAL =  76.6125369072\n",
      "Validation accuracy: 82.8%\n",
      "Test accuracy: 89.2%\n"
     ]
    }
   ],
   "source": [
    "DLmodel_N(num_steps=3001,\n",
    "          num_layers=1,\n",
    "          starter_learning_rate=.5,\n",
    "          learning_decay_rate=1,\n",
    "          learning_decay_steps=10000,\n",
    "          report_interval=500,\n",
    "          keep_prob=1.0,beta=0.0,silent=True) #sin dropout ni ajuste de loss por pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.500000, num_steps: 3001\n",
      "\n",
      "Initialized\n",
      "\n",
      "tiempo utilizado FINAL =  123.619668007\n",
      "Validation accuracy: 82.3%\n",
      "Test accuracy: 90.0%\n"
     ]
    }
   ],
   "source": [
    "DLmodel_1(train_dataset,train_labels,valid_dataset,valid_labels,test_dataset,test_labels,\n",
    "            keep_prob=1.0,beta=0.0,silent=True) #sin dropout ni ajuste de loss por pesos COMPARAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
